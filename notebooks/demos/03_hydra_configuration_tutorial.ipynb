{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hydra-tutorial-header",
   "metadata": {
    "tags": [
     "tutorial",
     "hydra",
     "configuration"
    ]
   },
   "source": [
    "# Hydra Configuration Tutorial: Advanced Configuration Management for Odor Plume Navigation\n",
    "\n",
    "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)](https://python.org)\n",
    "[![Hydra](https://img.shields.io/badge/hydra-1.3.2%2B-green)](https://hydra.cc)\n",
    "[![Pydantic](https://img.shields.io/badge/pydantic-2.5.0%2B-red)](https://pydantic.dev)\n",
    "\n",
    "## 📋 Tutorial Overview\n",
    "\n",
    "This comprehensive tutorial demonstrates advanced Hydra configuration management integrated with the odor plume navigation library. You'll learn sophisticated configuration composition patterns, dynamic parameter exploration, and enterprise-grade configuration validation techniques essential for reproducible research workflows.\n",
    "\n",
    "### 🎯 Learning Objectives\n",
    "\n",
    "By completing this tutorial, you will master:\n",
    "\n",
    "1. **Hierarchical Configuration Composition** - Building sophisticated configuration systems with inheritance\n",
    "2. **Interactive Parameter Exploration** - Using Hydra's Compose API for dynamic configuration assembly\n",
    "3. **Configuration Validation** - Implementing robust validation with Pydantic schema integration\n",
    "4. **Environment Variable Integration** - Secure credential management and deployment flexibility\n",
    "5. **Multi-run Parameter Sweeps** - Automated experiment orchestration and batch processing\n",
    "6. **CLI Integration Patterns** - Command-line parameter override and validation workflows\n",
    "\n",
    "### 🏗️ Technical Architecture\n",
    "\n",
    "This tutorial implements the configuration-driven interaction model specified in Section 7.4.5.1 of the technical specification, demonstrating:\n",
    "\n",
    "- **Three-Layer Configuration Hierarchy**: `base.yaml` → `config.yaml` → `local overrides`\n",
    "- **Hydra Compose API Integration**: Dynamic configuration assembly within notebook environments\n",
    "- **Pydantic Schema Validation**: Type-safe configuration with comprehensive error reporting\n",
    "- **Environment Variable Interpolation**: Secure deployment patterns with `${oc.env:}` syntax\n",
    "- **Configuration Group Management**: Modular experiment design and parameter organization\n",
    "\n",
    "### 📚 Prerequisites\n",
    "\n",
    "- Python 3.9+ with Jupyter environment\n",
    "- Basic understanding of YAML configuration formats\n",
    "- Familiarity with Python data structures and type annotations\n",
    "- Introduction to the odor plume navigation library concepts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 🔧 1. Environment Setup and Dependencies\n",
    "\n",
    "First, let's establish our development environment and verify all dependencies are properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependency-check",
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "# Core dependency verification and import setup\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional, Union\n",
    "import warnings\n",
    "\n",
    "# Hydra and configuration management\n",
    "try:\n",
    "    import hydra\n",
    "    from hydra import compose, initialize, initialize_config_dir\n",
    "    from hydra.core.global_hydra import GlobalHydra\n",
    "    from hydra.core.config_store import ConfigStore\n",
    "    from omegaconf import DictConfig, OmegaConf\n",
    "    print(f\"✅ Hydra {hydra.__version__} successfully imported\")\nexcept ImportError as e:\n",
    "    print(f\"❌ Hydra import failed: {e}\")\n",
    "    print(\"Install with: pip install hydra-core>=1.3.2\")\n",
    "\n",
    "# Pydantic for configuration validation\n",
    "try:\n",
    "    from pydantic import BaseModel, ValidationError, Field\n",
    "    import pydantic\n",
    "    print(f\"✅ Pydantic {pydantic.__version__} successfully imported\")\nexcept ImportError as e:\n",
    "    print(f\"❌ Pydantic import failed: {e}\")\n",
    "    print(\"Install with: pip install pydantic>=2.5.0\")\n",
    "\n",
    "# Scientific computing foundations\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"✅ NumPy {np.__version__} successfully imported\")\nexcept ImportError as e:\n",
    "    print(f\"❌ NumPy import failed: {e}\")\n",
    "\n",
    "# Visualization and display utilities\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    print(f\"✅ Matplotlib {matplotlib.__version__} successfully imported\")\nexcept ImportError as e:\n",
    "    print(f\"❌ Matplotlib import failed: {e}\")\n",
    "\n",
    "# Environment variable management\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    print(\"✅ python-dotenv successfully imported\")\nexcept ImportError as e:\n",
    "    print(f\"❌ python-dotenv import failed: {e}\")\n",
    "    print(\"Install with: pip install python-dotenv>=1.1.0\")\n",
    "\n",
    "# Project-specific imports\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "try:\n",
    "    # Import configuration schemas for validation examples\n",
    "    from {{cookiecutter.project_slug}}.config import schemas\n",
    "    print(\"✅ Project configuration schemas imported successfully\")\nexcept ImportError as e:\n",
    "    print(f\"⚠️  Project schemas not yet available: {e}\")\n",
    "    print(\"This is expected during initial development - we'll use mock schemas\")\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "print(\"\\n🎯 Environment setup complete! Ready to explore Hydra configuration patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "project-structure",
   "metadata": {},
   "source": [
    "### 📁 Project Structure Overview\n",
    "\n",
    "Let's examine the configuration directory structure that supports our Hydra integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structure-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the configuration directory structure\n",
    "config_dir = project_root / \"conf\"\n",
    "\n",
    "def display_tree(directory: Path, prefix: str = \"\", max_depth: int = 3, current_depth: int = 0):\n",
    "    \"\"\"Display directory tree structure with file details.\"\"\"\n    if current_depth > max_depth:\n        return\n    \n    if not directory.exists():\n        print(f\"{prefix}📁 {directory.name}/ (pending creation)\")\n        return\n    \n    items = sorted(directory.iterdir(), key=lambda x: (x.is_file(), x.name))\n    \n    for i, item in enumerate(items):\n        is_last = i == len(items) - 1\n        current_prefix = \"└── \" if is_last else \"├── \"\n        next_prefix = prefix + (\"    \" if is_last else \"│   \")\n        \n        if item.is_dir():\n            print(f\"{prefix}{current_prefix}📁 {item.name}/\")\n            display_tree(item, next_prefix, max_depth, current_depth + 1)\n        else:\n            # Show file size for configuration files\n            size = item.stat().st_size if item.exists() else 0\n            size_str = f\" ({size:,} bytes)\" if size > 0 else \" (empty)\"\n            \n            if item.suffix == '.yaml':\n                icon = \"📄\"\n            elif item.suffix == '.py':\n                icon = \"🐍\"\n            else:\n                icon = \"📝\"\n            \n            print(f\"{prefix}{current_prefix}{icon} {item.name}{size_str}\")\n\nprint(\"🏗️ Configuration Directory Structure:\")\nprint(\"\")\ndisplay_tree(config_dir)\n\nprint(\"\\n📋 Configuration Layer Hierarchy:\")\nprint(\"1. conf/base.yaml        - Foundational system defaults\")\nprint(\"2. conf/config.yaml      - User customizations and overrides\")\nprint(\"3. conf/local/*.yaml     - Environment-specific secrets (git-ignored)\")\nprint(\"4. CLI overrides         - Dynamic runtime parameter modifications\")\nprint(\"5. Environment variables - Secure deployment configurations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-composition",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏗️ 2. Hierarchical Configuration Composition\n",
    "\n",
    "This section demonstrates the sophisticated three-layer configuration hierarchy that forms the foundation of our configuration-driven interaction model (Section 7.4.5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "base-config-exploration",
   "metadata": {},
   "source": [
    "### 2.1 Foundation Layer: base.yaml\n",
    "\n",
    "The `base.yaml` file provides immutable foundational defaults ensuring system stability across all environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "base-config-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the base configuration\n",
    "base_config_path = config_dir / \"base.yaml\"\n",
    "\n",
    "if base_config_path.exists():\n",
    "    # Read the base configuration\n",
    "    with open(base_config_path, 'r') as f:\n",
    "        base_content = f.read()\n    \n",
    "    # Parse with OmegaConf for analysis\n",
    "    base_config = OmegaConf.load(base_config_path)\n    \n",
    "    print(\"📄 Base Configuration Structure Analysis:\")\n    print(\"=\"*50)\n    \n",
    "    # Analyze top-level configuration groups\n",
    "    def analyze_config_section(config: DictConfig, section_name: str, level: int = 0):\n",
    "        \"\"\"Recursively analyze configuration structure.\"\"\"\n        indent = \"  \" * level\n        \n        if section_name in config:\n            section = config[section_name]\n            print(f\"{indent}🔧 {section_name}:\")\n            \n            if isinstance(section, DictConfig):\n                for key, value in section.items():\n                    if isinstance(value, DictConfig) and len(value) > 0:\n                        print(f\"{indent}  ├── {key}/ (nested configuration)\")\n                        if level < 2:  # Limit depth for readability\n                            analyze_config_section(section, key, level + 2)\n                    else:\n                        value_preview = str(value)[:50] + \"...\" if len(str(value)) > 50 else str(value)\n                        print(f\"{indent}  ├── {key}: {value_preview}\")\n            print()\n    \n    # Analyze major configuration sections\n    major_sections = ['environment', 'navigator', 'video_plume', 'simulation', 'visualization', 'logging']\n    \n    for section in major_sections:\n        analyze_config_section(base_config, section)\n    \n    # Show environment variable interpolation examples\n    print(\"🌍 Environment Variable Interpolation Examples:\")\n    print(\"=\" * 50)\n    \n    def find_env_interpolations(config: DictConfig, path: str = \"\"):\n        \"\"\"Find and display environment variable interpolations.\"\"\"\n        interpolations = []\n        \n        def search_recursive(obj, current_path):\n            if isinstance(obj, DictConfig):\n                for key, value in obj.items():\n                    new_path = f\"{current_path}.{key}\" if current_path else key\n                    search_recursive(value, new_path)\n            elif isinstance(obj, str) and \"${oc.env:\" in obj:\n                interpolations.append((current_path, obj))\n        \n        search_recursive(config, path)\n        return interpolations\n    \n    env_interpolations = find_env_interpolations(base_config)\n    \n    for path, value in env_interpolations[:10]:  # Show first 10 examples\n        print(f\"  📍 {path}: {value}\")\n    \n    if len(env_interpolations) > 10:\n        print(f\"  ... and {len(env_interpolations) - 10} more environment interpolations\")\n    \n    print(f\"\\n📊 Configuration Statistics:\")\n    print(f\"  • Total configuration keys: {len(OmegaConf.to_yaml(base_config).split('\\n'))}\")\n    print(f\"  • Environment interpolations: {len(env_interpolations)}\")\n    print(f\"  • File size: {base_config_path.stat().st_size:,} bytes\")\n    \nelse:\n    print(\"⚠️  Base configuration file not found. Creating example structure...\")\n    \n    # Create a minimal example structure for demonstration\n    example_base = {\n        'environment': {\n            'type': 'development',\n            'debug_mode': True,\n            'paths': {\n                'data_dir': '${oc.env:DATA_DIR,data}',\n                'output_dir': '${oc.env:OUTPUT_DIR,outputs}'\n            }\n        },\n        'navigator': {\n            'position': None,\n            'orientation': 0.0,\n            'speed': 0.0,\n            'max_speed': '${oc.env:NAVIGATOR_MAX_SPEED,1.0}'\n        }\n    }\n    \n    print(\"📄 Example Base Configuration Structure:\")\n    print(OmegaConf.to_yaml(OmegaConf.create(example_base)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "user-config-exploration",
   "metadata": {},
   "source": [
    "### 2.2 Customization Layer: config.yaml\n",
    "\n",
    "The `config.yaml` file demonstrates user-driven customizations and parameter override patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "user-config-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze the user configuration\n",
    "user_config_path = config_dir / \"config.yaml\"\n",
    "\n",
    "if user_config_path.exists():\n",
    "    # Load user configuration\n",
    "    user_config = OmegaConf.load(user_config_path)\n    \n",
    "    print(\"🎛️ User Configuration Customization Analysis:\")\n    print(\"=\" * 50)\n    \n",
    "    # Analyze the defaults section\n",
    "    if 'defaults' in user_config:\n",
    "        print(\"📋 Configuration Inheritance Chain:\")\n",
    "        for i, default in enumerate(user_config.defaults):\n",
    "            print(f\"  {i+1}. {default}\")\n        print()\n    \n",
    "    # Show override examples\n",
    "    print(\"🔄 Parameter Override Examples:\")\n    \n",
    "    def show_overrides(config: DictConfig, base_config: DictConfig = None, path: str = \"\"):\n",
    "        \"\"\"Compare user config with base config to show overrides.\"\"\"\n        overrides = []\n        \n",
    "        def compare_recursive(user_obj, base_obj, current_path):\n",
    "            if isinstance(user_obj, DictConfig):\n",
    "                for key, value in user_obj.items():\n",
    "                    new_path = f\"{current_path}.{key}\" if current_path else key\n",
    "                    \n",
    "                    if base_obj and key in base_obj:\n",
    "                        if isinstance(value, DictConfig) and isinstance(base_obj[key], DictConfig):\n",
    "                            compare_recursive(value, base_obj[key], new_path)\n",
    "                        elif value != base_obj[key]:\n",
    "                            overrides.append((new_path, base_obj[key], value))\n",
    "                    else:\n",
    "                        overrides.append((new_path, \"<not set>\", value))\n        \n",
    "        if base_config_path.exists():\n",
    "            base_cfg = OmegaConf.load(base_config_path)\n",
    "            compare_recursive(config, base_cfg, path)\n        \n        return overrides\n    \n    if base_config_path.exists():\n        overrides = show_overrides(user_config, OmegaConf.load(base_config_path))\n        \n        for path, base_val, user_val in overrides[:15]:  # Show first 15 overrides\n            print(f\"  📍 {path}:\")\n            print(f\"      Base:  {base_val}\")\n            print(f\"      User:  {user_val}\")\n            print()\n        \n        if len(overrides) > 15:\n            print(f\"  ... and {len(overrides) - 15} more parameter overrides\")\n    \n    # Show environment variable usage in user config\n    user_env_vars = find_env_interpolations(user_config)\n    if user_env_vars:\n        print(\"🌍 Environment Variables in User Config:\")\n        for path, value in user_env_vars:\n            print(f\"  📍 {path}: {value}\")\n    \nelse:\n    print(\"⚠️  User configuration file not found. Creating demonstration example...\")\n    \n    # Create example user configuration\n    example_user = {\n        'defaults': ['base', '_self_'],\n        'navigator': {\n            'orientation': 90.0,  # Override from base (0.0)\n            'speed': 0.5,         # Override from base (0.0)\n            'max_speed': '${oc.env:NAVIGATOR_MAX_SPEED,1.5}'  # Custom env var\n        },\n        'simulation': {\n            'max_duration': 180.0,  # Override from base (300.0)\n            'fps': 60               # Override from base (30)\n        }\n    }\n    \n    print(\"📄 Example User Configuration Overrides:\")\n    print(OmegaConf.to_yaml(OmegaConf.create(example_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-composition-demo",
   "metadata": {},
   "source": [
    "### 2.3 Configuration Composition in Action\n",
    "\n",
    "Now let's demonstrate how Hydra composes the hierarchical configuration layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composition-demonstration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any existing Hydra global state\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "# Initialize Hydra with the project configuration directory\n",
    "try:\n",
    "    with initialize(config_path=\"../../conf\", version_base=None):\n",
    "        # Compose the complete configuration\n",
    "        cfg = compose(config_name=\"config\")\n",
    "        \n",
    "        print(\"🔄 Hierarchical Configuration Composition Results:\")\n        print(\"=\" * 55)\n        \n",
    "        print(\"\\n📊 Configuration Summary:\")\n        print(f\"  • Total configuration keys: {len(OmegaConf.to_yaml(cfg).split())}\")\n        print(f\"  • Configuration depth: {_get_config_depth(cfg)} levels\")\n        print(f\"  • Memory usage: ~{len(OmegaConf.to_yaml(cfg))} characters\")\n        \n        # Show the composition hierarchy\n        if 'defaults' in cfg:\n            print(\"\\n📋 Effective Composition Chain:\")\n            for i, default in enumerate(cfg.defaults):\n                print(f\"  {i+1}. {default}\")\n        \n        # Demonstrate specific parameter resolution\n        print(\"\\n🎯 Parameter Resolution Examples:\")\n        \n        resolution_examples = [\n            ('navigator.orientation', 'Agent initial orientation'),\n            ('navigator.max_speed', 'Maximum navigation speed'),\n            ('simulation.fps', 'Simulation frame rate'),\n            ('environment.type', 'Environment deployment type'),\n            ('logging.level', 'System logging verbosity')\n        ]\n        \n        for param_path, description in resolution_examples:\n            try:\n                value = OmegaConf.select(cfg, param_path)\n                print(f\"  📍 {param_path}:\")\n                print(f\"      Value: {value}\")\n                print(f\"      Description: {description}\")\n                print()\n            except Exception as e:\n                print(f\"  ❌ {param_path}: {e}\")\n        \n        # Show environment variable placeholders\n        print(\"🌍 Environment Variable Placeholders:\")\n        env_placeholders = _find_env_placeholders(cfg)\n        \n        for placeholder, paths in env_placeholders.items():\n            print(f\"  🔑 {placeholder}:\")\n            for path in paths[:3]:  # Show first 3 paths\n                print(f\"      Used in: {path}\")\n            if len(paths) > 3:\n                print(f\"      ... and {len(paths) - 3} more locations\")\n            print()\n        \n        print(\"\\n✅ Configuration composition completed successfully!\")\n        \nexcept Exception as e:\n    print(f\"❌ Configuration composition failed: {e}\")\n    print(\"\\nCreating mock configuration for demonstration...\")\n    \n    # Create a mock composed configuration\n    mock_cfg = OmegaConf.create({\n        'navigator': {\n            'orientation': 90.0,  # From user override\n            'speed': 0.5,         # From user override\n            'max_speed': 1.5      # From environment variable default\n        },\n        'simulation': {\n            'max_duration': 180.0,  # From user override\n            'fps': 60               # From user override\n        },\n        'environment': {\n            'type': 'development',   # From base configuration\n            'debug_mode': True       # From base configuration\n        }\n    })\n    \n    print(\"📄 Mock Composed Configuration:\")\n    print(OmegaConf.to_yaml(mock_cfg))\n    cfg = mock_cfg\n\n# Helper functions\ndef _get_config_depth(config: DictConfig, current_depth: int = 0) -> int:\n    \"\"\"Calculate maximum configuration nesting depth.\"\"\"\n    max_depth = current_depth\n    \n    if isinstance(config, DictConfig):\n        for value in config.values():\n            if isinstance(value, DictConfig):\n                depth = _get_config_depth(value, current_depth + 1)\n                max_depth = max(max_depth, depth)\n    \n    return max_depth\n\ndef _find_env_placeholders(config: DictConfig) -> Dict[str, List[str]]:\n    \"\"\"Find all environment variable placeholders in configuration.\"\"\"\n    placeholders = {}\n    \n    def search_recursive(obj, path: str = \"\"):\n        if isinstance(obj, DictConfig):\n            for key, value in obj.items():\n                new_path = f\"{path}.{key}\" if path else key\n                search_recursive(value, new_path)\n        elif isinstance(obj, str) and \"${oc.env:\" in obj:\n            # Extract environment variable name\n            import re\n            match = re.search(r'\\$\\{oc\\.env:([^,}]+)', obj)\n            if match:\n                env_var = match.group(1)\n                if env_var not in placeholders:\n                    placeholders[env_var] = []\n                placeholders[env_var].append(path)\n    \n    search_recursive(config)\n    return placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-exploration",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔬 3. Interactive Parameter Exploration with Compose API\n",
    "\n",
    "This section demonstrates Hydra's Compose API for interactive parameter exploration within notebook environments, as specified in Section 7.4.3.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compose-api-basics",
   "metadata": {},
   "source": [
    "### 3.1 Basic Compose API Usage\n",
    "\n",
    "The Compose API enables dynamic configuration assembly and parameter exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compose-api-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous Hydra state\n",
    "GlobalHydra.instance().clear()\n",
    "\n",
    "print(\"🔬 Interactive Parameter Exploration with Hydra Compose API\")\nprint(\"=\" * 60)\n\n# Example 1: Basic configuration composition\nprint(\"\\n1️⃣ Basic Configuration Loading:\")\nprint(\"-\" * 35)\n\ntry:\n    with initialize(config_path=\"../../conf\", version_base=None):\n        # Load base configuration\n        base_cfg = compose(config_name=\"config\")\n        \n        print(\"📄 Base Configuration Loaded:\")\n        \n        # Show key navigation parameters\n        nav_params = [\n            'navigator.orientation',\n            'navigator.speed', \n            'navigator.max_speed'\n        ]\n        \n        for param in nav_params:\n            value = OmegaConf.select(base_cfg, param)\n            print(f\"  • {param}: {value}\")\n        \nexcept Exception as e:\n    print(f\"⚠️  Using mock configuration: {e}\")\n    base_cfg = OmegaConf.create({\n        'navigator': {'orientation': 0.0, 'speed': 0.0, 'max_speed': 1.0}\n    })\n\n# Example 2: Dynamic parameter overrides\nprint(\"\\n2️⃣ Dynamic Parameter Overrides:\")\nprint(\"-\" * 38)\n\n# Create different configuration variants\noverride_scenarios = [\n    {\n        'name': 'High-Speed Navigation',\n        'overrides': ['navigator.max_speed=3.0', 'navigator.speed=1.5']\n    },\n    {\n        'name': 'Precision Navigation', \n        'overrides': ['navigator.max_speed=0.8', 'navigator.speed=0.2']\n    },\n    {\n        'name': 'Directional Navigation',\n        'overrides': ['navigator.orientation=180.0', 'navigator.speed=1.0']\n    }\n]\n\nconfigurations = {}\n\nfor scenario in override_scenarios:\n    try:\n        with initialize(config_path=\"../../conf\", version_base=None):\n            cfg = compose(config_name=\"config\", overrides=scenario['overrides'])\n            configurations[scenario['name']] = cfg\n            \n            print(f\"\\n🎯 {scenario['name']}:\")\n            for override in scenario['overrides']:\n                param, value = override.split('=')\n                actual_value = OmegaConf.select(cfg, param)\n                print(f\"  • {param}: {actual_value}\")\n                \n    except Exception as e:\n        print(f\"⚠️  Scenario '{scenario['name']}' failed: {e}\")\n        # Create mock configuration\n        mock_values = {}\n        for override in scenario['overrides']:\n            param, value = override.split('=')\n            try:\n                mock_values[param] = float(value)\n            except ValueError:\n                mock_values[param] = value\n        configurations[scenario['name']] = OmegaConf.create({'navigator': mock_values})\n\nprint(f\"\\n✅ Generated {len(configurations)} configuration variants for exploration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parameter-exploration",
   "metadata": {},
   "source": [
    "### 3.2 Advanced Parameter Exploration Patterns\n",
    "\n",
    "Let's explore sophisticated configuration manipulation and analysis techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-exploration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced parameter exploration and configuration analysis\n",
    "print(\"🔬 Advanced Parameter Exploration Patterns\")\nprint(\"=\" * 45)\n\n# Example 1: Configuration parameter space exploration\nprint(\"\\n1️⃣ Parameter Space Exploration:\")\nprint(\"-\" * 36)\n\n# Define parameter ranges for exploration\nparameter_space = {\n    'navigator.max_speed': [0.5, 1.0, 1.5, 2.0, 3.0],\n    'navigator.orientation': [0.0, 45.0, 90.0, 135.0, 180.0],\n    'simulation.fps': [30, 45, 60, 90, 120]\n}\n\n# Generate configurations for parameter space exploration\nexploration_configs = []\n\nfor max_speed in parameter_space['navigator.max_speed'][:3]:  # Limit for demo\n    for orientation in parameter_space['navigator.orientation'][:3]:\n        config_name = f\"speed_{max_speed}_orient_{orientation}\"\n        overrides = [\n            f'navigator.max_speed={max_speed}',\n            f'navigator.orientation={orientation}'\n        ]\n        \n        try:\n            with initialize(config_path=\"../../conf\", version_base=None):\n                cfg = compose(config_name=\"config\", overrides=overrides)\n                exploration_configs.append((config_name, cfg))\n                \n        except Exception:\n            # Create mock configuration for demonstration\n            mock_cfg = OmegaConf.create({\n                'navigator': {\n                    'max_speed': max_speed,\n                    'orientation': orientation\n                }\n            })\n            exploration_configs.append((config_name, mock_cfg))\n\nprint(f\"Generated {len(exploration_configs)} parameter combinations:\")\nfor i, (name, cfg) in enumerate(exploration_configs[:6]):  # Show first 6\n    speed = OmegaConf.select(cfg, 'navigator.max_speed')\n    orient = OmegaConf.select(cfg, 'navigator.orientation')\n    print(f\"  {i+1}. {name}: speed={speed}, orientation={orient}°\")\n\nif len(exploration_configs) > 6:\n    print(f\"  ... and {len(exploration_configs) - 6} more combinations\")\n\n# Example 2: Configuration validation and analysis\nprint(\"\\n2️⃣ Configuration Analysis and Validation:\")\nprint(\"-\" * 44)\n\ndef analyze_configuration(cfg: DictConfig, name: str) -> Dict[str, Any]:\n    \"\"\"Analyze configuration properties and characteristics.\"\"\"\n    analysis = {\n        'name': name,\n        'parameter_count': 0,\n        'environment_variables': 0,\n        'nested_levels': 0,\n        'configuration_size': len(OmegaConf.to_yaml(cfg)),\n        'validation_status': 'unknown'\n    }\n    \n    def count_parameters(obj, level=0):\n        count = 0\n        max_level = level\n        \n        if isinstance(obj, DictConfig):\n            for key, value in obj.items():\n                if isinstance(value, DictConfig):\n                    sub_count, sub_level = count_parameters(value, level + 1)\n                    count += sub_count\n                    max_level = max(max_level, sub_level)\n                else:\n                    count += 1\n                    if isinstance(value, str) and \"${oc.env:\" in value:\n                        analysis['environment_variables'] += 1\n        \n        return count, max_level\n    \n    analysis['parameter_count'], analysis['nested_levels'] = count_parameters(cfg)\n    \n    # Basic validation check\n    try:\n        # Check for required navigation parameters\n        required_params = ['navigator.max_speed', 'navigator.orientation']\n        for param in required_params:\n            value = OmegaConf.select(cfg, param)\n            if value is None:\n                analysis['validation_status'] = 'missing_required'\n                break\n        else:\n            analysis['validation_status'] = 'valid'\n    except Exception:\n        analysis['validation_status'] = 'error'\n    \n    return analysis\n\n# Analyze our exploration configurations\nanalyses = []\nfor name, cfg in exploration_configs[:5]:  # Analyze first 5\n    analysis = analyze_configuration(cfg, name)\n    analyses.append(analysis)\n\nprint(\"📊 Configuration Analysis Results:\")\nprint()\nfor analysis in analyses:\n    print(f\"🔍 {analysis['name']}:\")\n    print(f\"  • Parameters: {analysis['parameter_count']}\")\n    print(f\"  • Nesting depth: {analysis['nested_levels']}\")\n    print(f\"  • Environment vars: {analysis['environment_variables']}\")\n    print(f\"  • Validation: {analysis['validation_status']}\")\n    print(f\"  • Size: {analysis['configuration_size']} chars\")\n    print()\n\n# Example 3: Interactive configuration comparison\nprint(\"3️⃣ Interactive Configuration Comparison:\")\nprint(\"-\" * 43)\n\ndef compare_configurations(cfg1: DictConfig, cfg2: DictConfig, \n                         name1: str, name2: str) -> List[Dict[str, Any]]:\n    \"\"\"Compare two configurations and identify differences.\"\"\"\n    differences = []\n    \n    def compare_recursive(obj1, obj2, path=\"\"):\n        if isinstance(obj1, DictConfig) and isinstance(obj2, DictConfig):\n            all_keys = set(obj1.keys()) | set(obj2.keys())\n            for key in all_keys:\n                new_path = f\"{path}.{key}\" if path else key\n                \n                val1 = obj1.get(key) if key in obj1 else \"<missing>\"\n                val2 = obj2.get(key) if key in obj2 else \"<missing>\"\n                \n                if isinstance(val1, DictConfig) or isinstance(val2, DictConfig):\n                    compare_recursive(val1, val2, new_path)\n                elif val1 != val2:\n                    differences.append({\n                        'parameter': new_path,\n                        'value1': val1,\n                        'value2': val2\n                    })\n    \n    compare_recursive(cfg1, cfg2)\n    return differences\n\n# Compare first two configurations\nif len(exploration_configs) >= 2:\n    name1, cfg1 = exploration_configs[0]\n    name2, cfg2 = exploration_configs[1]\n    \n    differences = compare_configurations(cfg1, cfg2, name1, name2)\n    \n    print(f\"🔄 Comparing '{name1}' vs '{name2}':\")\n    print()\n    \n    if differences:\n        for diff in differences:\n            print(f\"  📍 {diff['parameter']}:\")\n            print(f\"      {name1}: {diff['value1']}\")\n            print(f\"      {name2}: {diff['value2']}\")\n            print()\n    else:\n        print(\"  ✅ Configurations are identical\")\nelse:\n    print(\"  ⚠️  Need at least 2 configurations for comparison\")\n\nprint(\"\\n✅ Advanced parameter exploration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-examples",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🛡️ 4. Configuration Validation with Pydantic Integration\n",
    "\n",
    "This section demonstrates robust configuration validation using Pydantic schemas integrated with Hydra, supporting Features F-006 and F-007 from the technical specification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "schema-definition",
   "metadata": {},
   "source": [
    "### 4.1 Pydantic Schema Integration\n",
    "\n",
    "Let's examine how Pydantic schemas provide type-safe configuration validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "schema-validation-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration validation with Pydantic schemas\n",
    "print(\"🛡️ Configuration Validation with Pydantic Integration\")\nprint(\"=\" * 55)\n\n# Example 1: Basic schema definition and validation\nprint(\"\\n1️⃣ Basic Schema Definition and Validation:\")\nprint(\"-\" * 46)\n\n# Define example navigation configuration schema\nfrom typing import Tuple, List, Optional, Literal\nfrom pydantic import BaseModel, Field, field_validator, model_validator\n\nclass NavigatorConfigSchema(BaseModel):\n    \"\"\"Example navigator configuration schema with comprehensive validation.\"\"\"\n    \n    # Basic navigation parameters\n    position: Optional[Tuple[float, float]] = Field(\n        default=None,\n        description=\"Initial [x, y] position coordinates\"\n    )\n    orientation: float = Field(\n        default=0.0,\n        ge=0.0,\n        le=360.0,\n        description=\"Initial orientation in degrees (0=right, 90=up)\"\n    )\n    speed: float = Field(\n        default=0.0,\n        ge=0.0,\n        description=\"Initial speed in units per timestep\"\n    )\n    max_speed: float = Field(\n        default=1.0,\n        gt=0.0,\n        description=\"Maximum allowed speed constraint\"\n    )\n    angular_velocity: float = Field(\n        default=0.0,\n        description=\"Angular velocity in degrees per timestep\"\n    )\n    \n    @field_validator('speed', 'max_speed')\n    @classmethod\n    def validate_speed_constraints(cls, v, info):\n        \"\"\"Validate speed parameters are non-negative.\"\"\"\n        if v < 0:\n            raise ValueError(f\"{info.field_name} must be non-negative\")\n        return v\n    \n    @model_validator(mode=\"after\")\n    def validate_speed_relationship(self):\n        \"\"\"Ensure speed does not exceed max_speed.\"\"\"\n        if self.speed > self.max_speed:\n            raise ValueError(f\"Initial speed ({self.speed}) cannot exceed max_speed ({self.max_speed})\")\n        return self\n    \n    class Config:\n        extra = \"allow\"\n        validate_assignment = True\n        json_schema_extra = {\n            \"examples\": [\n                {\n                    \"position\": [25.0, 50.0],\n                    \"orientation\": 90.0,\n                    \"speed\": 0.5,\n                    \"max_speed\": 2.0,\n                    \"angular_velocity\": 0.1\n                }\n            ]\n        }\n\nclass SimulationConfigSchema(BaseModel):\n    \"\"\"Example simulation configuration schema.\"\"\"\n    \n    max_duration: float = Field(\n        default=300.0,\n        gt=0.0,\n        description=\"Maximum simulation duration in seconds\"\n    )\n    fps: int = Field(\n        default=30,\n        gt=0,\n        le=120,\n        description=\"Simulation frame rate\"\n    )\n    real_time: bool = Field(\n        default=False,\n        description=\"Enable real-time simulation constraint\"\n    )\n    \n    @field_validator('fps')\n    @classmethod\n    def validate_fps_range(cls, v):\n        \"\"\"Validate FPS is within reasonable range.\"\"\"\n        if v > 120:\n            raise ValueError(\"FPS should not exceed 120 for practical simulations\")\n        return v\n\nclass RootConfigSchema(BaseModel):\n    \"\"\"Root configuration schema combining all components.\"\"\"\n    \n    navigator: NavigatorConfigSchema = Field(default_factory=NavigatorConfigSchema)\n    simulation: SimulationConfigSchema = Field(default_factory=SimulationConfigSchema)\n    \n    class Config:\n        extra = \"allow\"\n\nprint(\"📄 Schema Definition Complete:\")\nprint(f\"  • NavigatorConfigSchema: {len(NavigatorConfigSchema.__fields__)} fields\")\nprint(f\"  • SimulationConfigSchema: {len(SimulationConfigSchema.__fields__)} fields\")\nprint(f\"  • Root schema with nested validation\")\n\n# Example 2: Validation success cases\nprint(\"\\n2️⃣ Validation Success Examples:\")\nprint(\"-\" * 35)\n\nvalid_configs = [\n    {\n        'name': 'Basic Valid Configuration',\n        'config': {\n            'navigator': {\n                'orientation': 45.0,\n                'speed': 0.8,\n                'max_speed': 2.0\n            },\n            'simulation': {\n                'max_duration': 180.0,\n                'fps': 60\n            }\n        }\n    },\n    {\n        'name': 'Minimal Configuration',\n        'config': {\n            'navigator': {},  # All defaults\n            'simulation': {'fps': 30}\n        }\n    },\n    {\n        'name': 'High-Performance Setup',\n        'config': {\n            'navigator': {\n                'position': [100.0, 200.0],\n                'orientation': 180.0,\n                'speed': 1.5,\n                'max_speed': 3.0,\n                'angular_velocity': 0.2\n            },\n            'simulation': {\n                'max_duration': 600.0,\n                'fps': 90,\n                'real_time': False\n            }\n        }\n    }\n]\n\nfor example in valid_configs:\n    try:\n        # Validate configuration\n        validated_config = RootConfigSchema(**example['config'])\n        \n        print(f\"✅ {example['name']}:\")\n        print(f\"  • Navigator speed: {validated_config.navigator.speed}\")\n        print(f\"  • Navigator max_speed: {validated_config.navigator.max_speed}\")\n        print(f\"  • Simulation fps: {validated_config.simulation.fps}\")\n        print(f\"  • Validation: SUCCESS\")\n        print()\n        \n    except ValidationError as e:\n        print(f\"❌ {example['name']}: Unexpected validation error\")\n        print(f\"  Error: {e}\")\n        print()\n\nprint(\"🎯 All valid configurations passed validation successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-error-handling",
   "metadata": {},
   "source": [
    "### 4.2 Validation Error Handling and Diagnostics\n",
    "\n",
    "Let's explore comprehensive error handling and diagnostic capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-errors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration validation error handling and diagnostics\n",
    "print(\"🚨 Configuration Validation Error Handling\")\nprint(\"=\" * 45)\n\n# Example validation error cases\nprint(\"\\n1️⃣ Comprehensive Error Case Analysis:\")\nprint(\"-\" * 41)\n\ninvalid_configs = [\n    {\n        'name': 'Speed Constraint Violation',\n        'config': {\n            'navigator': {\n                'speed': 2.5,      # Exceeds max_speed\n                'max_speed': 2.0\n            }\n        },\n        'expected_error': 'Speed exceeds maximum'\n    },\n    {\n        'name': 'Negative Speed Value',\n        'config': {\n            'navigator': {\n                'speed': -1.0,     # Negative speed\n                'max_speed': 2.0\n            }\n        },\n        'expected_error': 'Speed must be non-negative'\n    },\n    {\n        'name': 'Invalid Orientation Range',\n        'config': {\n            'navigator': {\n                'orientation': 400.0  # Exceeds 360 degrees\n            }\n        },\n        'expected_error': 'Orientation out of range'\n    },\n    {\n        'name': 'Invalid FPS Value',\n        'config': {\n            'simulation': {\n                'fps': 0  # FPS must be positive\n            }\n        },\n        'expected_error': 'FPS must be positive'\n    },\n    {\n        'name': 'Type Mismatch Error',\n        'config': {\n            'navigator': {\n                'speed': \"fast\",   # String instead of number\n                'max_speed': 2.0\n            }\n        },\n        'expected_error': 'Type validation error'\n    }\n]\n\nfor i, example in enumerate(invalid_configs, 1):\n    print(f\"🔍 Test Case {i}: {example['name']}\")\n    print(f\"   Expected: {example['expected_error']}\")\n    \n    try:\n        # Attempt validation (should fail)\n        validated_config = RootConfigSchema(**example['config'])\n        print(f\"   Result: ❌ UNEXPECTED SUCCESS (validation should have failed)\")\n        \n    except ValidationError as e:\n        print(f\"   Result: ✅ VALIDATION FAILED AS EXPECTED\")\n        \n        # Parse and display detailed error information\n        errors = e.errors()\n        print(f\"   Error count: {len(errors)}\")\n        \n        for error in errors[:2]:  # Show first 2 errors\n            location = \" → \".join(str(loc) for loc in error['loc'])\n            print(f\"   📍 Location: {location}\")\n            print(f\"      Type: {error['type']}\")\n            print(f\"      Message: {error['msg']}\")\n            if 'input' in error:\n                print(f\"      Input: {error['input']}\")\n        \n        if len(errors) > 2:\n            print(f\"   ... and {len(errors) - 2} more validation errors\")\n    \n    except Exception as e:\n        print(f\"   Result: ❌ UNEXPECTED ERROR: {e}\")\n    \n    print()\n\n# Example 2: Custom validation with detailed diagnostics\nprint(\"2️⃣ Advanced Validation Diagnostics:\")\nprint(\"-\" * 37)\n\ndef validate_configuration_with_diagnostics(config_dict: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Validate configuration with comprehensive diagnostic reporting.\"\"\"\n    diagnostics = {\n        'validation_status': 'unknown',\n        'error_count': 0,\n        'warning_count': 0,\n        'errors': [],\n        'warnings': [],\n        'suggestions': []\n    }\n    \n    try:\n        # Attempt validation\n        validated_config = RootConfigSchema(**config_dict)\n        diagnostics['validation_status'] = 'success'\n        \n        # Check for potential performance warnings\n        nav_config = validated_config.navigator\n        sim_config = validated_config.simulation\n        \n        # Performance warnings\n        if sim_config.fps > 60:\n            diagnostics['warnings'].append({\n                'type': 'performance',\n                'message': f'High FPS ({sim_config.fps}) may impact performance',\n                'location': 'simulation.fps'\n            })\n            diagnostics['warning_count'] += 1\n        \n        if nav_config.max_speed > 5.0:\n            diagnostics['warnings'].append({\n                'type': 'physics',\n                'message': f'Very high max_speed ({nav_config.max_speed}) may cause instability',\n                'location': 'navigator.max_speed'\n            })\n            diagnostics['warning_count'] += 1\n        \n        # Suggestions for optimization\n        if nav_config.speed == 0.0 and nav_config.angular_velocity == 0.0:\n            diagnostics['suggestions'].append({\n                'type': 'optimization',\n                'message': 'Consider setting initial speed or angular velocity for dynamic start',\n                'location': 'navigator'\n            })\n        \n    except ValidationError as e:\n        diagnostics['validation_status'] = 'failed'\n        diagnostics['error_count'] = len(e.errors())\n        \n        for error in e.errors():\n            diagnostics['errors'].append({\n                'location': \" → \".join(str(loc) for loc in error['loc']),\n                'type': error['type'],\n                'message': error['msg'],\n                'input': error.get('input', 'N/A')\n            })\n    \n    except Exception as e:\n        diagnostics['validation_status'] = 'error'\n        diagnostics['error_count'] = 1\n        diagnostics['errors'].append({\n            'location': 'root',\n            'type': 'system_error',\n            'message': str(e),\n            'input': 'N/A'\n        })\n    \n    return diagnostics\n\n# Test diagnostic system with various configurations\ntest_configs = [\n    {\n        'name': 'High-Performance Config',\n        'config': {\n            'navigator': {'max_speed': 6.0, 'speed': 2.0},\n            'simulation': {'fps': 120}\n        }\n    },\n    {\n        'name': 'Static Agent Config',\n        'config': {\n            'navigator': {'speed': 0.0, 'angular_velocity': 0.0}\n        }\n    },\n    {\n        'name': 'Invalid Config',\n        'config': {\n            'navigator': {'speed': 3.0, 'max_speed': 2.0}\n        }\n    }\n]\n\nfor test_config in test_configs:\n    print(f\"🔍 Analyzing: {test_config['name']}\")\n    diagnostics = validate_configuration_with_diagnostics(test_config['config'])\n    \n    print(f\"   Status: {diagnostics['validation_status']}\")\n    print(f\"   Errors: {diagnostics['error_count']}, Warnings: {diagnostics['warning_count']}\")\n    \n    # Show errors\n    for error in diagnostics['errors']:\n        print(f\"   ❌ {error['location']}: {error['message']}\")\n    \n    # Show warnings\n    for warning in diagnostics['warnings']:\n        print(f\"   ⚠️  {warning['location']}: {warning['message']}\")\n    \n    # Show suggestions\n    for suggestion in diagnostics['suggestions']:\n        print(f\"   💡 {suggestion['location']}: {suggestion['message']}\")\n    \n    print()\n\nprint(\"✅ Validation error handling and diagnostics complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environment-integration",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🌍 5. Environment Variable Integration and Runtime Overrides\n",
    "\n",
    "This section demonstrates secure environment variable integration and runtime parameter override capabilities as specified in Section 7.4.5.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "env-var-setup",
   "metadata": {},
   "source": [
    "### 5.1 Environment Variable Interpolation Patterns\n",
    "\n",
    "Let's explore Hydra's powerful environment variable interpolation capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-variables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment variable integration and runtime parameter overrides\n",
    "print(\"🌍 Environment Variable Integration and Runtime Overrides\")\nprint(\"=\" * 58)\n\n# Example 1: Setting up environment variables for demonstration\nprint(\"\\n1️⃣ Environment Variable Setup and Testing:\")\nprint(\"-\" * 44)\n\n# Set demonstration environment variables\ndemo_env_vars = {\n    'NAVIGATOR_MAX_SPEED': '2.5',\n    'SIMULATION_FPS': '45',\n    'VIDEO_PATH': '/data/experiments/plume_video.mp4',\n    'DEBUG_MODE': 'true',\n    'LOG_LEVEL': 'INFO',\n    'EXPERIMENT_NAME': 'hydra_tutorial_demo',\n    'RANDOM_SEED': '42'\n}\n\n# Apply environment variables\nfor key, value in demo_env_vars.items():\n    os.environ[key] = value\n    print(f\"  🔑 {key} = {value}\")\n\nprint(f\"\\n✅ Set {len(demo_env_vars)} environment variables for demonstration\")\n\n# Example 2: Environment variable interpolation syntax\nprint(\"\\n2️⃣ Environment Variable Interpolation Syntax:\")\nprint(\"-\" * 47)\n\n# Create configuration with environment interpolation\nenv_config_example = {\n    'navigator': {\n        # Basic interpolation with default\n        'max_speed': '${oc.env:NAVIGATOR_MAX_SPEED,1.0}',\n        \n        # Environment variable without default (will use system value)\n        'orientation': '${oc.env:NAVIGATOR_ORIENTATION,0.0}',\n        \n        # Complex interpolation with type conversion\n        'speed': '${oc.env:NAVIGATOR_SPEED,0.5}'\n    },\n    'simulation': {\n        # Integer environment variable\n        'fps': '${oc.env:SIMULATION_FPS,30}',\n        \n        # Boolean environment variable\n        'real_time': '${oc.env:REAL_TIME_MODE,false}',\n        \n        # String environment variable with path\n        'output_dir': '${oc.env:OUTPUT_DIR,./outputs}'\n    },\n    'environment': {\n        # Boolean flag from environment\n        'debug_mode': '${oc.env:DEBUG_MODE,false}',\n        \n        # String with complex default\n        'experiment_name': '${oc.env:EXPERIMENT_NAME,default_experiment}',\n        \n        # Numeric seed for reproducibility\n        'random_seed': '${oc.env:RANDOM_SEED,null}'\n    },\n    'video_plume': {\n        # File path with secure default\n        'video_path': '${oc.env:VIDEO_PATH,data/videos/default.mp4}'\n    },\n    'logging': {\n        # Log level configuration\n        'level': '${oc.env:LOG_LEVEL,INFO}'\n    }\n}\n\nprint(\"📄 Environment Interpolation Examples:\")\nfor section, params in env_config_example.items():\n    print(f\"\\n🔧 {section}:\")\n    for param, value in params.items():\n        print(f\"  • {param}: {value}\")\n\n# Example 3: Runtime resolution demonstration\nprint(\"\\n3️⃣ Runtime Environment Variable Resolution:\")\nprint(\"-\" * 46)\n\n# Create OmegaConf configuration with environment interpolation\nenv_omega_config = OmegaConf.create(env_config_example)\n\n# Resolve environment variables\nprint(\"🔄 Environment Variable Resolution Results:\")\nprint()\n\nresolution_results = []\n\ndef resolve_and_display(config: DictConfig, path: str = \"\"):\n    \"\"\"Recursively resolve and display environment variables.\"\"\"\n    if isinstance(config, DictConfig):\n        for key, value in config.items():\n            current_path = f\"{path}.{key}\" if path else key\n            \n            if isinstance(value, DictConfig):\n                resolve_and_display(value, current_path)\n            elif isinstance(value, str) and \"${oc.env:\" in value:\n                try:\n                    # Extract environment variable info\n                    import re\n                    match = re.search(r'\\$\\{oc\\.env:([^,}]+)(?:,([^}]+))?\\}', value)\n                    if match:\n                        env_var = match.group(1)\n                        default_val = match.group(2) if match.group(2) else \"None\"\n                        \n                        # Get resolved value\n                        resolved_value = OmegaConf.select(config, key.split('.')[-1])\n                        env_value = os.environ.get(env_var, \"<not set>\")\n                        \n                        print(f\"  📍 {current_path}:\")\n                        print(f\"      Pattern: {value}\")\n                        print(f\"      Env var: {env_var} = {env_value}\")\n                        print(f\"      Default: {default_val}\")\n                        print(f\"      Resolved: {resolved_value}\")\n                        print()\n                        \n                        resolution_results.append({\n                            'path': current_path,\n                            'env_var': env_var,\n                            'env_value': env_value,\n                            'default': default_val,\n                            'resolved': resolved_value\n                        })\n                        \n                except Exception as e:\n                    print(f\"  ❌ Error resolving {current_path}: {e}\")\n\nresolve_and_display(env_omega_config)\n\nprint(f\"📊 Environment Resolution Summary:\")\nprint(f\"  • Total interpolations: {len(resolution_results)}\")\nprint(f\"  • Environment variables used: {len(set(r['env_var'] for r in resolution_results))}\")\nprint(f\"  • Successfully resolved: {len([r for r in resolution_results if r['env_value'] != '<not set>'])}\")\n\n# Example 4: Type conversion and validation\nprint(\"\\n4️⃣ Environment Variable Type Conversion:\")\nprint(\"-\" * 42)\n\ndef demonstrate_type_conversion():\n    \"\"\"Demonstrate automatic type conversion for environment variables.\"\"\"\n    \n    # Set typed environment variables\n    type_test_vars = {\n        'TEST_FLOAT': '3.14159',\n        'TEST_INT': '42',\n        'TEST_BOOL_TRUE': 'true',\n        'TEST_BOOL_FALSE': 'false',\n        'TEST_LIST': '[1, 2, 3]',\n        'TEST_DICT': '{\"key\": \"value\"}'\n    }\n    \n    for key, value in type_test_vars.items():\n        os.environ[key] = value\n    \n    # Create configuration with type expectations\n    type_config = {\n        'numeric_values': {\n            'float_param': '${oc.env:TEST_FLOAT,1.0}',\n            'int_param': '${oc.env:TEST_INT,10}'\n        },\n        'boolean_values': {\n            'bool_true': '${oc.env:TEST_BOOL_TRUE,false}',\n            'bool_false': '${oc.env:TEST_BOOL_FALSE,true}'\n        },\n        'complex_values': {\n            'list_param': '${oc.env:TEST_LIST,[]}',\n            'dict_param': '${oc.env:TEST_DICT,{}}'\n        }\n    }\n    \n    omega_type_config = OmegaConf.create(type_config)\n    \n    print(\"🔄 Type Conversion Results:\")\n    print()\n    \n    # Test numeric conversions\n    float_val = OmegaConf.select(omega_type_config, 'numeric_values.float_param')\n    int_val = OmegaConf.select(omega_type_config, 'numeric_values.int_param')\n    \n    print(f\"  📊 Numeric Conversions:\")\n    print(f\"      float_param: {float_val} (type: {type(float_val).__name__})\")\n    print(f\"      int_param: {int_val} (type: {type(int_val).__name__})\")\n    \n    # Test boolean conversions\n    bool_true = OmegaConf.select(omega_type_config, 'boolean_values.bool_true')\n    bool_false = OmegaConf.select(omega_type_config, 'boolean_values.bool_false')\n    \n    print(f\"\\n  🔘 Boolean Conversions:\")\n    print(f\"      bool_true: {bool_true} (type: {type(bool_true).__name__})\")\n    print(f\"      bool_false: {bool_false} (type: {type(bool_false).__name__})\")\n    \n    # Clean up test variables\n    for key in type_test_vars:\n        os.environ.pop(key, None)\n\ndemonstrate_type_conversion()\n\nprint(\"\\n✅ Environment variable integration demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "runtime-overrides",
   "metadata": {},
   "source": [
    "### 5.2 Runtime Parameter Override Patterns\n",
    "\n",
    "Let's explore sophisticated runtime parameter override techniques for experiment control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "runtime-overrides-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime parameter override patterns and techniques\n",
    "print(\"⚡ Runtime Parameter Override Patterns and Techniques\")\nprint(\"=\" * 54)\n\n# Example 1: CLI-style parameter overrides\nprint(\"\\n1️⃣ CLI-Style Parameter Override Simulation:\")\nprint(\"-\" * 44)\n\n# Simulate CLI parameter overrides\ncli_override_examples = [\n    {\n        'name': 'Navigation Speed Adjustment',\n        'overrides': [\n            'navigator.max_speed=3.5',\n            'navigator.speed=1.2',\n            'navigator.orientation=45.0'\n        ],\n        'description': 'High-speed navigation with diagonal orientation'\n    },\n    {\n        'name': 'Simulation Performance Tuning',\n        'overrides': [\n            'simulation.fps=120',\n            'simulation.max_duration=600',\n            'simulation.real_time=false'\n        ],\n        'description': 'High-performance simulation with extended duration'\n    },\n    {\n        'name': 'Multi-Agent Configuration',\n        'overrides': [\n            'navigator.positions=[[0,0],[10,10],[20,20]]',\n            'navigator.speeds=[0.5,1.0,1.5]',\n            'navigator.num_agents=3'\n        ],\n        'description': 'Three-agent formation with varied speeds'\n    },\n    {\n        'name': 'Debug and Logging Setup',\n        'overrides': [\n            'environment.debug_mode=true',\n            'logging.level=DEBUG',\n            'logging.performance.enabled=true'\n        ],\n        'description': 'Development debugging configuration'\n    }\n]\n\n# Clear Hydra state for clean override demonstrations\nGlobalHydra.instance().clear()\n\nfor i, example in enumerate(cli_override_examples, 1):\n    print(f\"🎯 Example {i}: {example['name']}\")\n    print(f\"   Description: {example['description']}\")\n    print(f\"   Overrides: {len(example['overrides'])} parameters\")\n    \n    try:\n        with initialize(config_path=\"../../conf\", version_base=None):\n            # Apply CLI-style overrides\n            cfg = compose(config_name=\"config\", overrides=example['overrides'])\n            \n            print(f\"   Status: ✅ SUCCESS\")\n            \n            # Show resolved values\n            print(f\"   Resolved values:\")\n            for override in example['overrides'][:3]:  # Show first 3\n                param_path, expected_value = override.split('=', 1)\n                try:\n                    actual_value = OmegaConf.select(cfg, param_path)\n                    print(f\"     • {param_path}: {actual_value}\")\n                except Exception as e:\n                    print(f\"     • {param_path}: Error resolving - {e}\")\n            \n            if len(example['overrides']) > 3:\n                print(f\"     ... and {len(example['overrides']) - 3} more parameters\")\n            \n    except Exception as e:\n        print(f\"   Status: ❌ FAILED - {e}\")\n    \n    print()\n\n# Example 2: Conditional parameter overrides\nprint(\"2️⃣ Conditional Parameter Override Patterns:\")\nprint(\"-\" * 42)\n\ndef apply_conditional_overrides(base_config: DictConfig, \n                              conditions: Dict[str, Any]) -> DictConfig:\n    \"\"\"Apply parameter overrides based on runtime conditions.\"\"\"\n    \n    # Make a copy of the configuration\n    config = OmegaConf.create(OmegaConf.to_yaml(base_config))\n    \n    # Apply conditional logic\n    if conditions.get('performance_mode') == 'high':\n        OmegaConf.set(config, 'simulation.fps', 90)\n        OmegaConf.set(config, 'navigator.max_speed', 3.0)\n        print(\"   📈 Applied high-performance optimizations\")\n    \n    if conditions.get('debug_mode', False):\n        OmegaConf.set(config, 'logging.level', 'DEBUG')\n        OmegaConf.set(config, 'environment.debug_mode', True)\n        print(\"   🐛 Enabled debug mode configurations\")\n    \n    if conditions.get('agent_count', 1) > 1:\n        # Configure for multi-agent mode\n        positions = [[i*10, i*10] for i in range(conditions['agent_count'])]\n        OmegaConf.set(config, 'navigator.positions', positions)\n        OmegaConf.set(config, 'navigator.num_agents', conditions['agent_count'])\n        print(f\"   👥 Configured for {conditions['agent_count']}-agent simulation\")\n    \n    if conditions.get('video_quality') == 'high':\n        OmegaConf.set(config, 'video_plume.kernel_size', 7)\n        OmegaConf.set(config, 'video_plume.kernel_sigma', 2.0)\n        print(\"   🎥 Applied high-quality video processing\")\n    \n    return config\n\n# Test conditional override scenarios\nconditional_scenarios = [\n    {\n        'name': 'High-Performance Multi-Agent',\n        'conditions': {\n            'performance_mode': 'high',\n            'agent_count': 5,\n            'debug_mode': False\n        }\n    },\n    {\n        'name': 'Debug Single-Agent',\n        'conditions': {\n            'performance_mode': 'standard',\n            'agent_count': 1,\n            'debug_mode': True\n        }\n    },\n    {\n        'name': 'Production High-Quality',\n        'conditions': {\n            'performance_mode': 'high',\n            'video_quality': 'high',\n            'agent_count': 3,\n            'debug_mode': False\n        }\n    }\n]\n\n# Create base configuration for testing\nbase_test_config = OmegaConf.create({\n    'navigator': {\n        'max_speed': 1.0,\n        'orientation': 0.0,\n        'num_agents': 1\n    },\n    'simulation': {\n        'fps': 30,\n        'max_duration': 300\n    },\n    'logging': {\n        'level': 'INFO'\n    },\n    'environment': {\n        'debug_mode': False\n    },\n    'video_plume': {\n        'kernel_size': 3,\n        'kernel_sigma': 1.0\n    }\n})\n\nfor scenario in conditional_scenarios:\n    print(f\"🔄 Testing: {scenario['name']}\")\n    print(f\"   Conditions: {scenario['conditions']}\")\n    \n    # Apply conditional overrides\n    modified_config = apply_conditional_overrides(base_test_config, scenario['conditions'])\n    \n    # Show key changes\n    print(f\"   Results:\")\n    key_params = [\n        'navigator.max_speed',\n        'simulation.fps', \n        'logging.level',\n        'navigator.num_agents'\n    ]\n    \n    for param in key_params:\n        value = OmegaConf.select(modified_config, param)\n        if value is not None:\n            print(f\"     • {param}: {value}\")\n    \n    print()\n\n# Example 3: Parameter override validation\nprint(\"3️⃣ Parameter Override Validation:\")\nprint(\"-\" * 35)\n\ndef validate_override_safety(overrides: List[str]) -> Dict[str, Any]:\n    \"\"\"Validate that parameter overrides are safe and valid.\"\"\"\n    \n    validation_result = {\n        'status': 'unknown',\n        'safe_overrides': [],\n        'unsafe_overrides': [],\n        'warnings': [],\n        'errors': []\n    }\n    \n    # Define parameter safety rules\n    safety_rules = {\n        'navigator.max_speed': {'min': 0.1, 'max': 10.0, 'type': float},\n        'navigator.speed': {'min': 0.0, 'max': 5.0, 'type': float},\n        'navigator.orientation': {'min': 0.0, 'max': 360.0, 'type': float},\n        'simulation.fps': {'min': 1, 'max': 120, 'type': int},\n        'simulation.max_duration': {'min': 1.0, 'max': 3600.0, 'type': float}\n    }\n    \n    for override in overrides:\n        try:\n            param_path, value_str = override.split('=', 1)\n            \n            if param_path in safety_rules:\n                rule = safety_rules[param_path]\n                \n                # Type conversion and validation\n                try:\n                    if rule['type'] == float:\n                        value = float(value_str)\n                    elif rule['type'] == int:\n                        value = int(value_str)\n                    else:\n                        value = value_str\n                    \n                    # Range validation\n                    if 'min' in rule and value < rule['min']:\n                        validation_result['unsafe_overrides'].append({\n                            'override': override,\n                            'reason': f'Value {value} below minimum {rule[\"min\"]}'\n                        })\n                    elif 'max' in rule and value > rule['max']:\n                        validation_result['unsafe_overrides'].append({\n                            'override': override,\n                            'reason': f'Value {value} exceeds maximum {rule[\"max\"]}'\n                        })\n                    else:\n                        validation_result['safe_overrides'].append(override)\n                        \n                        # Performance warnings\n                        if param_path == 'simulation.fps' and value > 60:\n                            validation_result['warnings'].append(\n                                f'High FPS ({value}) may impact performance'\n                            )\n                \n                except (ValueError, TypeError) as e:\n                    validation_result['errors'].append(\n                        f'Type conversion error for {override}: {e}'\n                    )\n            else:\n                # Unknown parameter - add to safe list with warning\n                validation_result['safe_overrides'].append(override)\n                validation_result['warnings'].append(\n                    f'Parameter {param_path} not in safety rules'\n                )\n        \n        except ValueError as e:\n            validation_result['errors'].append(\n                f'Invalid override format {override}: {e}'\n            )\n    \n    # Set overall status\n    if validation_result['errors']:\n        validation_result['status'] = 'failed'\n    elif validation_result['unsafe_overrides']:\n        validation_result['status'] = 'unsafe'\n    else:\n        validation_result['status'] = 'safe'\n    \n    return validation_result\n\n# Test override validation\ntest_overrides = [\n    'navigator.max_speed=2.5',     # Safe\n    'navigator.speed=15.0',        # Unsafe - too high\n    'navigator.orientation=450.0', # Unsafe - out of range\n    'simulation.fps=90',           # Safe but with warning\n    'simulation.fps=-10',          # Unsafe - negative\n    'custom.parameter=value',      # Unknown parameter\n    'invalid_format'               # Invalid format\n]\n\nprint(\"🔍 Validating Parameter Overrides:\")\nvalidation = validate_override_safety(test_overrides)\n\nprint(f\"\\n📊 Validation Results: {validation['status'].upper()}\")\nprint(f\"   • Safe overrides: {len(validation['safe_overrides'])}\")\nprint(f\"   • Unsafe overrides: {len(validation['unsafe_overrides'])}\")\nprint(f\"   • Warnings: {len(validation['warnings'])}\")\nprint(f\"   • Errors: {len(validation['errors'])}\")\n\nif validation['unsafe_overrides']:\n    print(\"\\n❌ Unsafe Overrides:\")\n    for unsafe in validation['unsafe_overrides']:\n        print(f\"   • {unsafe['override']}: {unsafe['reason']}\")\n\nif validation['warnings']:\n    print(\"\\n⚠️  Warnings:\")\n    for warning in validation['warnings']:\n        print(f\"   • {warning}\")\n\nif validation['errors']:\n    print(\"\\n🚨 Errors:\")\n    for error in validation['errors']:\n        print(f\"   • {error}\")\n\n# Clean up demonstration environment variables\nfor key in demo_env_vars:\n    os.environ.pop(key, None)\n\nprint(\"\\n✅ Runtime parameter override demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multirun-sweeps",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🔄 6. Multi-run Parameter Sweeps and Experiment Orchestration\n",
    "\n",
    "This section demonstrates advanced multi-run parameter sweep capabilities for automated experiment orchestration, as specified in Section 7.4.4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multirun-basics",
   "metadata": {},
   "source": [
    "### 6.1 Basic Multi-run Configuration Patterns\n",
    "\n",
    "Let's explore Hydra's multi-run capabilities for systematic parameter exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multirun-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-run parameter sweeps and experiment orchestration\n",
    "print(\"🔄 Multi-run Parameter Sweeps and Experiment Orchestration\")\nprint(\"=\" * 59)\n\n# Example 1: Basic parameter sweep simulation\nprint(\"\\n1️⃣ Basic Parameter Sweep Configuration:\")\nprint(\"-\" * 41)\n\n# Define parameter sweep configurations\nsweep_configurations = [\n    {\n        'name': 'Navigation Speed Analysis',\n        'description': 'Systematic exploration of navigation speed parameters',\n        'parameter': 'navigator.max_speed',\n        'values': [0.5, 1.0, 1.5, 2.0, 2.5, 3.0],\n        'fixed_params': {\n            'navigator.orientation': 90.0,\n            'simulation.max_duration': 180.0\n        }\n    },\n    {\n        'name': 'Simulation Frame Rate Study',\n        'description': 'Performance analysis across different frame rates',\n        'parameter': 'simulation.fps',\n        'values': [15, 30, 45, 60, 90, 120],\n        'fixed_params': {\n            'navigator.max_speed': 2.0,\n            'simulation.real_time': False\n        }\n    },\n    {\n        'name': 'Orientation Sweep',\n        'description': 'Directional navigation behavior analysis',\n        'parameter': 'navigator.orientation',\n        'values': [0, 45, 90, 135, 180, 225, 270, 315],\n        'fixed_params': {\n            'navigator.speed': 1.0,\n            'navigator.max_speed': 2.0\n        }\n    }\n]\n\n# Simulate multi-run execution for each sweep configuration\nfor i, sweep_config in enumerate(sweep_configurations, 1):\n    print(f\"🎯 Sweep {i}: {sweep_config['name']}\")\n    print(f\"   Description: {sweep_config['description']}\")\n    print(f\"   Parameter: {sweep_config['parameter']}\")\n    print(f\"   Values: {sweep_config['values']}\")\n    print(f\"   Total runs: {len(sweep_config['values'])}\")\n    \n    # Simulate generating configurations for each value\n    print(f\"   Generated configurations:\")\n    \n    for j, value in enumerate(sweep_config['values'][:4]):  # Show first 4\n        # Create override list\n        overrides = [f\"{sweep_config['parameter']}={value}\"]\n        \n        # Add fixed parameters\n        for fixed_param, fixed_value in sweep_config['fixed_params'].items():\n            overrides.append(f\"{fixed_param}={fixed_value}\")\n        \n        print(f\"     Run {j+1}: {sweep_config['parameter']}={value}\")\n        print(f\"            Overrides: {len(overrides)} parameters\")\n        \n        # Simulate configuration composition\n        try:\n            GlobalHydra.instance().clear()\n            with initialize(config_path=\"../../conf\", version_base=None):\n                cfg = compose(config_name=\"config\", overrides=overrides)\n                \n                # Extract key resolved values\n                resolved_value = OmegaConf.select(cfg, sweep_config['parameter'])\n                print(f\"            Resolved: {sweep_config['parameter']}={resolved_value}\")\n                \n        except Exception as e:\n            print(f\"            Status: ⚠️  Configuration error - using mock values\")\n    \n    if len(sweep_config['values']) > 4:\n        print(f\"     ... and {len(sweep_config['values']) - 4} more runs\")\n    \n    print()\n\n# Example 2: Multi-dimensional parameter sweeps\nprint(\"2️⃣ Multi-Dimensional Parameter Sweep Design:\")\nprint(\"-\" * 44)\n\n# Define multi-dimensional sweep\nmulti_dim_sweep = {\n    'name': 'Navigation Performance Matrix',\n    'description': 'Combined speed and orientation analysis',\n    'parameters': {\n        'navigator.max_speed': [1.0, 2.0, 3.0],\n        'navigator.orientation': [0, 90, 180, 270],\n        'simulation.fps': [30, 60]\n    }\n}\n\n# Calculate total combinations\ntotal_combinations = 1\nfor param, values in multi_dim_sweep['parameters'].items():\n    total_combinations *= len(values)\n\nprint(f\"🧮 {multi_dim_sweep['name']}:\")\nprint(f\"   Description: {multi_dim_sweep['description']}\")\nprint(f\"   Parameters: {len(multi_dim_sweep['parameters'])}\")\nprint(f\"   Total combinations: {total_combinations}\")\nprint()\n\n# Generate sample combinations\nprint(\"📊 Sample Parameter Combinations:\")\n\nimport itertools\n\n# Generate all combinations\nparam_names = list(multi_dim_sweep['parameters'].keys())\nparam_values = list(multi_dim_sweep['parameters'].values())\ncombinations = list(itertools.product(*param_values))\n\n# Show first 8 combinations\nfor i, combination in enumerate(combinations[:8]):\n    print(f\"  Run {i+1:2d}: \", end=\"\")\n    for param_name, param_value in zip(param_names, combination):\n        print(f\"{param_name.split('.')[-1]}={param_value} \", end=\"\")\n    print()\n\nif len(combinations) > 8:\n    print(f\"  ... and {len(combinations) - 8} more combinations\")\n\nprint(f\"\\n📈 Sweep Efficiency Analysis:\")\nprint(f\"   • Parameters: {len(param_names)}\")\nprint(f\"   • Average values per parameter: {sum(len(v) for v in param_values) / len(param_values):.1f}\")\nprint(f\"   • Total combinations: {len(combinations)}\")\nprint(f\"   • Estimated execution time (30s/run): {len(combinations) * 30 / 60:.1f} minutes\")\n\n# Example 3: Conditional sweep patterns\nprint(\"\\n3️⃣ Conditional and Adaptive Sweep Patterns:\")\nprint(\"-\" * 45)\n\ndef generate_adaptive_sweep(base_params: Dict[str, Any], \n                          optimization_target: str) -> List[Dict[str, Any]]:\n    \"\"\"Generate adaptive parameter sweep based on optimization target.\"\"\"\n    \n    sweep_runs = []\n    \n    if optimization_target == 'performance':\n        # Focus on high-performance configurations\n        speed_values = [2.0, 2.5, 3.0, 3.5]  # Higher speeds\n        fps_values = [60, 90, 120]            # Higher frame rates\n        \n        for speed in speed_values:\n            for fps in fps_values:\n                run_config = base_params.copy()\n                run_config.update({\n                    'navigator.max_speed': speed,\n                    'simulation.fps': fps,\n                    'simulation.real_time': False  # Disable for performance\n                })\n                sweep_runs.append(run_config)\n    \n    elif optimization_target == 'stability':\n        # Focus on stable, conservative configurations\n        speed_values = [0.5, 1.0, 1.5]       # Lower speeds\n        orientation_values = [0, 90, 180]     # Cardinal directions\n        \n        for speed in speed_values:\n            for orientation in orientation_values:\n                run_config = base_params.copy()\n                run_config.update({\n                    'navigator.max_speed': speed,\n                    'navigator.orientation': orientation,\n                    'navigator.speed': speed * 0.5  # Conservative initial speed\n                })\n                sweep_runs.append(run_config)\n    \n    elif optimization_target == 'exploration':\n        # Focus on comprehensive parameter space exploration\n        import random\n        random.seed(42)  # Reproducible random sampling\n        \n        for _ in range(12):  # Generate 12 exploration runs\n            run_config = base_params.copy()\n            run_config.update({\n                'navigator.max_speed': random.uniform(0.5, 3.0),\n                'navigator.orientation': random.uniform(0, 360),\n                'navigator.speed': random.uniform(0.0, 1.5),\n                'simulation.fps': random.choice([30, 45, 60, 90])\n            })\n            sweep_runs.append(run_config)\n    \n    return sweep_runs\n\n# Generate adaptive sweeps\nadaptive_targets = ['performance', 'stability', 'exploration']\nbase_configuration = {\n    'simulation.max_duration': 300.0,\n    'environment.debug_mode': False,\n    'logging.level': 'INFO'\n}\n\nfor target in adaptive_targets:\n    print(f\"🎯 {target.title()} Optimization Sweep:\")\n    \n    adaptive_runs = generate_adaptive_sweep(base_configuration, target)\n    \n    print(f\"   Generated runs: {len(adaptive_runs)}\")\n    print(f\"   Configuration focus: {target}\")\n    \n    # Show sample configurations\n    print(f\"   Sample configurations:\")\n    for i, run_config in enumerate(adaptive_runs[:3]):\n        print(f\"     Run {i+1}: \", end=\"\")\n        key_params = ['navigator.max_speed', 'navigator.orientation', 'simulation.fps']\n        for param in key_params:\n            if param in run_config:\n                value = run_config[param]\n                if isinstance(value, float):\n                    print(f\"{param.split('.')[-1]}={value:.2f} \", end=\"\")\n                else:\n                    print(f\"{param.split('.')[-1]}={value} \", end=\"\")\n        print()\n    \n    if len(adaptive_runs) > 3:\n        print(f\"     ... and {len(adaptive_runs) - 3} more runs\")\n    \n    print()\n\nprint(\"✅ Multi-run parameter sweep demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-orchestration",
   "metadata": {},
   "source": [
    "### 6.2 Advanced Experiment Orchestration Patterns\n",
    "\n",
    "Let's explore sophisticated experiment orchestration and result aggregation techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-orchestration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced experiment orchestration and result management\n",
    "print(\"🔬 Advanced Experiment Orchestration and Result Management\")\nprint(\"=\" * 58)\n\n# Example 1: Experiment metadata and tracking\nprint(\"\\n1️⃣ Experiment Metadata and Tracking System:\")\nprint(\"-\" * 45)\n\nfrom datetime import datetime, timezone\nimport hashlib\nimport json\n\nclass ExperimentTracker:\n    \"\"\"Comprehensive experiment tracking and metadata management.\"\"\"\n    \n    def __init__(self, experiment_name: str):\n        self.experiment_name = experiment_name\n        self.experiments = []\n        self.start_time = datetime.now(timezone.utc)\n        \n    def create_experiment_run(self, run_id: int, configuration: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Create metadata for a single experiment run.\"\"\"\n        \n        # Generate configuration hash for deduplication\n        config_str = json.dumps(configuration, sort_keys=True)\n        config_hash = hashlib.md5(config_str.encode()).hexdigest()[:8]\n        \n        experiment_run = {\n            'experiment_name': self.experiment_name,\n            'run_id': run_id,\n            'config_hash': config_hash,\n            'configuration': configuration,\n            'metadata': {\n                'created_at': datetime.now(timezone.utc).isoformat(),\n                'hydra_version': getattr(hydra, '__version__', 'unknown'),\n                'python_version': f\"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\",\n                'platform': sys.platform\n            },\n            'status': 'pending',\n            'results': None,\n            'execution_time': None,\n            'error_message': None\n        }\n        \n        return experiment_run\n    \n    def simulate_experiment_execution(self, experiment_run: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Simulate experiment execution with realistic results.\"\"\"\n        \n        import random\n        import time\n        \n        start_time = time.time()\n        \n        try:\n            # Simulate experiment execution\n            config = experiment_run['configuration']\n            \n            # Extract key parameters for simulation\n            max_speed = config.get('navigator.max_speed', 1.0)\n            fps = config.get('simulation.fps', 30)\n            duration = config.get('simulation.max_duration', 300.0)\n            \n            # Simulate realistic execution time (scaled down for demo)\n            execution_time = random.uniform(0.1, 0.5)  # 0.1-0.5 seconds for demo\n            time.sleep(execution_time)\n            \n            # Generate simulated results\n            results = {\n                'trajectory_length': random.uniform(50, 200) * max_speed,\n                'average_speed': random.uniform(0.3, 0.9) * max_speed,\n                'navigation_efficiency': random.uniform(0.6, 0.95),\n                'computational_performance': {\n                    'fps_achieved': fps * random.uniform(0.85, 1.0),\n                    'memory_usage_mb': random.uniform(10, 50),\n                    'cpu_utilization': random.uniform(0.2, 0.8)\n                },\n                'completion_status': 'success',\n                'frames_processed': int(fps * duration)\n            }\n            \n            # Update experiment run\n            experiment_run.update({\n                'status': 'completed',\n                'results': results,\n                'execution_time': execution_time,\n                'error_message': None\n            })\n            \n        except Exception as e:\n            # Handle execution failure\n            experiment_run.update({\n                'status': 'failed',\n                'results': None,\n                'execution_time': time.time() - start_time,\n                'error_message': str(e)\n            })\n        \n        return experiment_run\n    \n    def add_experiment_run(self, configuration: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Add and execute a single experiment run.\"\"\"\n        run_id = len(self.experiments) + 1\n        experiment_run = self.create_experiment_run(run_id, configuration)\n        \n        # Execute the experiment\n        completed_run = self.simulate_experiment_execution(experiment_run)\n        self.experiments.append(completed_run)\n        \n        return completed_run\n    \n    def get_experiment_summary(self) -> Dict[str, Any]:\n        \"\"\"Generate comprehensive experiment summary.\"\"\"\n        \n        if not self.experiments:\n            return {'status': 'no_experiments'}\n        \n        successful_runs = [exp for exp in self.experiments if exp['status'] == 'completed']\n        failed_runs = [exp for exp in self.experiments if exp['status'] == 'failed']\n        \n        summary = {\n            'experiment_name': self.experiment_name,\n            'total_runs': len(self.experiments),\n            'successful_runs': len(successful_runs),\n            'failed_runs': len(failed_runs),\n            'success_rate': len(successful_runs) / len(self.experiments) if self.experiments else 0,\n            'total_execution_time': sum(exp.get('execution_time', 0) for exp in self.experiments),\n            'average_execution_time': sum(exp.get('execution_time', 0) for exp in self.experiments) / len(self.experiments) if self.experiments else 0\n        }\n        \n        # Performance statistics\n        if successful_runs:\n            efficiencies = [exp['results']['navigation_efficiency'] for exp in successful_runs]\n            trajectory_lengths = [exp['results']['trajectory_length'] for exp in successful_runs]\n            \n            summary['performance_stats'] = {\n                'avg_navigation_efficiency': sum(efficiencies) / len(efficiencies),\n                'max_navigation_efficiency': max(efficiencies),\n                'min_navigation_efficiency': min(efficiencies),\n                'avg_trajectory_length': sum(trajectory_lengths) / len(trajectory_lengths),\n                'max_trajectory_length': max(trajectory_lengths),\n                'min_trajectory_length': min(trajectory_lengths)\n            }\n        \n        return summary\n\n# Demonstrate experiment tracking\nprint(\"🔬 Experiment Tracking Demonstration:\")\n\n# Create experiment tracker\ntracker = ExperimentTracker(\"Hydra_Tutorial_Demo\")\n\n# Define experimental configurations\nexperimental_configs = [\n    {'navigator.max_speed': 1.0, 'simulation.fps': 30, 'simulation.max_duration': 180},\n    {'navigator.max_speed': 2.0, 'simulation.fps': 60, 'simulation.max_duration': 180},\n    {'navigator.max_speed': 3.0, 'simulation.fps': 90, 'simulation.max_duration': 180},\n    {'navigator.max_speed': 1.5, 'simulation.fps': 45, 'simulation.max_duration': 300},\n    {'navigator.max_speed': 2.5, 'simulation.fps': 75, 'simulation.max_duration': 240}\n]\n\nprint(f\"Executing {len(experimental_configs)} experimental runs...\")\nprint()\n\n# Execute experiments\nfor i, config in enumerate(experimental_configs, 1):\n    print(f\"  🏃 Run {i}: max_speed={config['navigator.max_speed']}, fps={config['simulation.fps']}\")\n    \n    experiment_run = tracker.add_experiment_run(config)\n    \n    print(f\"      Status: {experiment_run['status']}\")\n    print(f\"      Execution time: {experiment_run['execution_time']:.3f}s\")\n    \n    if experiment_run['status'] == 'completed':\n        results = experiment_run['results']\n        print(f\"      Navigation efficiency: {results['navigation_efficiency']:.2f}\")\n        print(f\"      Trajectory length: {results['trajectory_length']:.1f}\")\n    else:\n        print(f\"      Error: {experiment_run['error_message']}\")\n    \n    print()\n\n# Generate experiment summary\nsummary = tracker.get_experiment_summary()\nprint(\"📊 Experiment Summary:\")\nprint(f\"   • Total runs: {summary['total_runs']}\")\nprint(f\"   • Success rate: {summary['success_rate']:.1%}\")\nprint(f\"   • Total execution time: {summary['total_execution_time']:.2f}s\")\nprint(f\"   • Average execution time: {summary['average_execution_time']:.3f}s\")\n\nif 'performance_stats' in summary:\n    stats = summary['performance_stats']\n    print(f\"   • Average navigation efficiency: {stats['avg_navigation_efficiency']:.3f}\")\n    print(f\"   • Average trajectory length: {stats['avg_trajectory_length']:.1f}\")\n\n# Example 2: Result aggregation and analysis\nprint(\"\\n2️⃣ Result Aggregation and Analysis:\")\nprint(\"-\" * 37)\n\ndef analyze_experimental_results(experiments: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Comprehensive analysis of experimental results.\"\"\"\n    \n    successful_experiments = [exp for exp in experiments if exp['status'] == 'completed']\n    \n    if not successful_experiments:\n        return {'status': 'no_successful_experiments'}\n    \n    # Parameter correlation analysis\n    parameter_correlations = {}\n    \n    # Extract parameter-result relationships\n    for param_name in ['navigator.max_speed', 'simulation.fps']:\n        param_values = []\n        efficiency_values = []\n        trajectory_values = []\n        \n        for exp in successful_experiments:\n            if param_name in exp['configuration']:\n                param_values.append(exp['configuration'][param_name])\n                efficiency_values.append(exp['results']['navigation_efficiency'])\n                trajectory_values.append(exp['results']['trajectory_length'])\n        \n        if param_values:\n            # Simple correlation calculation\n            n = len(param_values)\n            if n > 1:\n                # Calculate correlation with efficiency\n                param_mean = sum(param_values) / n\n                eff_mean = sum(efficiency_values) / n\n                \n                numerator = sum((p - param_mean) * (e - eff_mean) for p, e in zip(param_values, efficiency_values))\n                param_var = sum((p - param_mean) ** 2 for p in param_values)\n                eff_var = sum((e - eff_mean) ** 2 for e in efficiency_values)\n                \n                if param_var > 0 and eff_var > 0:\n                    correlation = numerator / (param_var * eff_var) ** 0.5\n                    parameter_correlations[param_name] = {\n                        'efficiency_correlation': correlation,\n                        'parameter_range': (min(param_values), max(param_values)),\n                        'efficiency_range': (min(efficiency_values), max(efficiency_values))\n                    }\n    \n    # Performance trend analysis\n    performance_trends = {\n        'best_efficiency_config': None,\n        'best_trajectory_config': None,\n        'most_stable_config': None\n    }\n    \n    # Find best performing configurations\n    best_efficiency_exp = max(successful_experiments, key=lambda x: x['results']['navigation_efficiency'])\n    best_trajectory_exp = max(successful_experiments, key=lambda x: x['results']['trajectory_length'])\n    \n    performance_trends['best_efficiency_config'] = {\n        'configuration': best_efficiency_exp['configuration'],\n        'efficiency': best_efficiency_exp['results']['navigation_efficiency'],\n        'run_id': best_efficiency_exp['run_id']\n    }\n    \n    performance_trends['best_trajectory_config'] = {\n        'configuration': best_trajectory_exp['configuration'],\n        'trajectory_length': best_trajectory_exp['results']['trajectory_length'],\n        'run_id': best_trajectory_exp['run_id']\n    }\n    \n    return {\n        'status': 'success',\n        'experiment_count': len(successful_experiments),\n        'parameter_correlations': parameter_correlations,\n        'performance_trends': performance_trends,\n        'analysis_timestamp': datetime.now(timezone.utc).isoformat()\n    }\n\n# Perform result analysis\nanalysis_results = analyze_experimental_results(tracker.experiments)\n\nif analysis_results['status'] == 'success':\n    print(\"📈 Experimental Analysis Results:\")\n    print(f\"   • Analyzed {analysis_results['experiment_count']} successful experiments\")\n    \n    # Parameter correlations\n    if analysis_results['parameter_correlations']:\n        print(\"\\n   🔗 Parameter Correlations with Navigation Efficiency:\")\n        for param, corr_data in analysis_results['parameter_correlations'].items():\n            correlation = corr_data['efficiency_correlation']\n            param_range = corr_data['parameter_range']\n            print(f\"      • {param}: {correlation:+.3f} (range: {param_range[0]}-{param_range[1]})\")\n    \n    # Performance trends\n    trends = analysis_results['performance_trends']\n    print(\"\\n   🏆 Best Performing Configurations:\")\n    \n    if trends['best_efficiency_config']:\n        best_eff = trends['best_efficiency_config']\n        print(f\"      • Highest efficiency: {best_eff['efficiency']:.3f} (Run {best_eff['run_id']})\")\n        config_str = \", \".join(f\"{k.split('.')[-1]}={v}\" for k, v in best_eff['configuration'].items())\n        print(f\"        Config: {config_str}\")\n    \n    if trends['best_trajectory_config']:\n        best_traj = trends['best_trajectory_config']\n        print(f\"      • Longest trajectory: {best_traj['trajectory_length']:.1f} (Run {best_traj['run_id']})\")\n        config_str = \", \".join(f\"{k.split('.')[-1]}={v}\" for k, v in best_traj['configuration'].items())\n        print(f\"        Config: {config_str}\")\n\nelse:\n    print(f\"❌ Analysis failed: {analysis_results.get('status', 'unknown_error')}\")\n\nprint(\"\\n✅ Advanced experiment orchestration demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli-integration",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 💻 7. CLI Integration and Command-Line Workflows\n",
    "\n",
    "This section demonstrates CLI integration patterns and command-line workflow examples for production deployment and automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cli-patterns",
   "metadata": {},
   "source": [
    "### 7.1 CLI Command Pattern Examples\n",
    "\n",
    "Let's explore practical CLI integration patterns for Hydra configuration management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cli-integration-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLI integration and command-line workflow examples\n",
    "print(\"💻 CLI Integration and Command-Line Workflow Examples\")\nprint(\"=\" * 55)\n\n# Example 1: CLI command pattern demonstrations\nprint(\"\\n1️⃣ CLI Command Pattern Demonstrations:\")\nprint(\"-\" * 40)\n\n# Define comprehensive CLI command examples\ncli_command_examples = [\n    {\n        'category': 'Basic Execution',\n        'commands': [\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main',\n                'description': 'Run simulation with default configuration',\n                'use_case': 'Basic simulation execution with base.yaml defaults'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main --config-name=config',\n                'description': 'Run with specific configuration file',\n                'use_case': 'Use custom configuration file instead of defaults'\n            }\n        ]\n    },\n    {\n        'category': 'Parameter Overrides',\n        'commands': [\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.max_speed=2.5',\n                'description': 'Override single navigation parameter',\n                'use_case': 'Quick parameter adjustment for single runs'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.max_speed=3.0 simulation.fps=60',\n                'description': 'Override multiple parameters',\n                'use_case': 'Adjust multiple parameters for custom experiment'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.orientation=90 navigator.speed=1.5 simulation.max_duration=600',\n                'description': 'Complex parameter combination',\n                'use_case': 'Comprehensive configuration customization'\n            }\n        ]\n    },\n    {\n        'category': 'Multi-Agent Configuration',\n        'commands': [\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.positions=\"[[0,0],[10,10],[20,20]]\"',\n                'description': 'Configure multi-agent positions',\n                'use_case': 'Set up three-agent formation simulation'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.num_agents=5 navigator.speeds=\"[1.0,1.2,0.8,1.5,1.1]\"',\n                'description': 'Multi-agent with varied speeds',\n                'use_case': 'Heterogeneous multi-agent simulation'\n            }\n        ]\n    },\n    {\n        'category': 'Environment and Debug',\n        'commands': [\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main environment.debug_mode=true logging.level=DEBUG',\n                'description': 'Enable debug mode with verbose logging',\n                'use_case': 'Development and troubleshooting workflows'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main video_plume.video_path=\"/data/custom_video.mp4\"',\n                'description': 'Specify custom video source',\n                'use_case': 'Use specific experimental video data'\n            }\n        ]\n    },\n    {\n        'category': 'Multi-run Parameter Sweeps',\n        'commands': [\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main --multirun navigator.max_speed=1.0,2.0,3.0',\n                'description': 'Speed parameter sweep',\n                'use_case': 'Systematic navigation speed analysis'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main --multirun navigator.orientation=0,90,180,270 simulation.fps=30,60',\n                'description': 'Multi-dimensional parameter sweep',\n                'use_case': 'Combined orientation and performance analysis'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main --multirun \"navigator.max_speed=range(0.5,3.0,0.5)\"',\n                'description': 'Range-based parameter sweep',\n                'use_case': 'Continuous parameter space exploration'\n            }\n        ]\n    },\n    {\n        'category': 'Production and Deployment',\n        'commands': [\n            {\n                'command': 'ENVIRONMENT_TYPE=production python -m {{cookiecutter.project_slug}}.cli.main',\n                'description': 'Production environment execution',\n                'use_case': 'Deploy with production environment variables'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main hydra.run.dir=\"/outputs/experiment_$(date +%Y%m%d)\"',\n                'description': 'Custom output directory with timestamp',\n                'use_case': 'Organized output management for batch processing'\n            },\n            {\n                'command': 'python -m {{cookiecutter.project_slug}}.cli.main --config-path=/etc/configs --config-name=production',\n                'description': 'External configuration directory',\n                'use_case': 'System-wide configuration management'\n            }\n        ]\n    }\n]\n\n# Display CLI command examples\nfor category_info in cli_command_examples:\n    print(f\"🎯 {category_info['category']}:\")\n    print()\n    \n    for i, cmd_info in enumerate(category_info['commands'], 1):\n        print(f\"  {i}. {cmd_info['description']}\")\n        print(f\"     Command: {cmd_info['command']}\")\n        print(f\"     Use case: {cmd_info['use_case']}\")\n        print()\n    \n    print()\n\n# Example 2: CLI parameter validation and help systems\nprint(\"2️⃣ CLI Parameter Validation and Help Systems:\")\nprint(\"-\" * 45)\n\n# Simulate CLI help and validation patterns\ncli_help_examples = [\n    {\n        'command': 'python -m {{cookiecutter.project_slug}}.cli.main --help',\n        'output_type': 'General help information',\n        'content': [\n            'Usage: python -m {{cookiecutter.project_slug}}.cli.main [OPTIONS]',\n            '',\n            'Options:',\n            '  --config-path PATH    Configuration directory path',\n            '  --config-name NAME    Configuration file name (without .yaml)',\n            '  --multirun           Enable parameter sweep mode',\n            '  --help               Show this help message and exit',\n            '',\n            'Parameter Overrides:',\n            '  Use KEY=VALUE syntax to override configuration parameters',\n            '  Example: navigator.max_speed=2.0 simulation.fps=60',\n            '',\n            'Multi-run Examples:',\n            '  --multirun navigator.max_speed=1.0,2.0,3.0',\n            '  --multirun \"navigator.orientation=range(0,360,45)\"'\n        ]\n    },\n    {\n        'command': 'python -m {{cookiecutter.project_slug}}.cli.main --cfg job',\n        'output_type': 'Configuration structure help',\n        'content': [\n            'Job Configuration Structure:',\n            '',\n            'navigator:',\n            '  position: null           # [x, y] coordinates',\n            '  orientation: 0.0         # degrees (0-360)',\n            '  speed: 0.0              # initial speed',\n            '  max_speed: 1.0          # maximum speed constraint',\n            '',\n            'simulation:',\n            '  max_duration: 300.0     # seconds',\n            '  fps: 30                 # frames per second',\n            '  real_time: false        # real-time constraint',\n            '',\n            'environment:',\n            '  type: development       # development|testing|production',\n            '  debug_mode: true        # enable debug features'\n        ]\n    }\n]\n\nfor help_example in cli_help_examples:\n    print(f\"📚 {help_example['output_type']}:\")\n    print(f\"   Command: {help_example['command']}\")\n    print()\n    print(\"   Sample Output:\")\n    for line in help_example['content']:\n        print(f\"   {line}\")\n    print()\n\n# Example 3: Error handling and validation feedback\nprint(\"3️⃣ CLI Error Handling and Validation Feedback:\")\nprint(\"-\" * 46)\n\n# Simulate CLI error scenarios and feedback\ncli_error_examples = [\n    {\n        'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.max_speed=-1.0',\n        'error_type': 'Parameter Validation Error',\n        'feedback': [\n            'ERROR: Configuration validation failed',\n            '',\n            'ValidationError: navigator.max_speed',\n            '  Value: -1.0',\n            '  Error: ensure this value is greater than 0',\n            '',\n            'Suggestion: Use a positive value for max_speed',\n            'Example: navigator.max_speed=2.0'\n        ]\n    },\n    {\n        'command': 'python -m {{cookiecutter.project_slug}}.cli.main navigator.orientation=400',\n        'error_type': 'Range Validation Error',\n        'feedback': [\n            'ERROR: Configuration validation failed',\n            '',\n            'ValidationError: navigator.orientation',\n            '  Value: 400.0',\n            '  Error: ensure this value is less than or equal to 360',\n            '',\n            'Suggestion: Orientation must be between 0 and 360 degrees',\n            'Example: navigator.orientation=270'\n        ]\n    },\n    {\n        'command': 'python -m {{cookiecutter.project_slug}}.cli.main --config-name=nonexistent',\n        'error_type': 'Configuration File Error',\n        'feedback': [\n            'ERROR: Configuration file not found',\n            '',\n            'ConfigurationError: Could not find config file',\n            '  Requested: nonexistent.yaml',\n            '  Search paths: [\"conf/\", \"/etc/configs/\"]',\n            '',\n            'Available configurations:',\n            '  • base.yaml      - Foundation defaults',\n            '  • config.yaml    - User customizations',\n            '',\n            'Suggestion: Use --config-name=config or create the missing file'\n        ]\n    }\n]\n\nfor error_example in cli_error_examples:\n    print(f\"❌ {error_example['error_type']}:\")\n    print(f\"   Command: {error_example['command']}\")\n    print()\n    print(\"   Error Feedback:\")\n    for line in error_example['feedback']:\n        print(f\"   {line}\")\n    print()\n\n# Example 4: Shell integration and automation patterns\nprint(\"4️⃣ Shell Integration and Automation Patterns:\")\nprint(\"-\" * 44)\n\n# Shell script examples for automation\nshell_examples = [\n    {\n        'name': 'Batch Parameter Sweep Script',\n        'description': 'Automated parameter exploration with result collection',\n        'script_type': 'Bash',\n        'content': [\n            '#!/bin/bash',\n            '# Automated navigation parameter sweep',\n            '',\n            'EXPERIMENT_NAME=\"navigation_speed_analysis\"',\n            'OUTPUT_BASE=\"/outputs/experiments\"',\n            'DATE_STAMP=$(date +\"%Y%m%d_%H%M%S\")',\n            '',\n            'echo \"Starting experiment: $EXPERIMENT_NAME\"',\n            '',\n            '# Create experiment directory',\n            'mkdir -p \"$OUTPUT_BASE/$DATE_STAMP\"',\n            '',\n            '# Run parameter sweep',\n            'python -m {{cookiecutter.project_slug}}.cli.main \\\\',\n            '  --multirun \\\\',\n            '  navigator.max_speed=0.5,1.0,1.5,2.0,2.5,3.0 \\\\',\n            '  hydra.run.dir=\"$OUTPUT_BASE/$DATE_STAMP/speed_\\${navigator.max_speed}\" \\\\',\n            '  experiment_name=\"$EXPERIMENT_NAME\"',\n            '',\n            'echo \"Experiment completed. Results in: $OUTPUT_BASE/$DATE_STAMP\"'\n        ]\n    },\n    {\n        'name': 'Environment-Specific Deployment',\n        'description': 'Production deployment with environment validation',\n        'script_type': 'Bash',\n        'content': [\n            '#!/bin/bash',\n            '# Production deployment script',\n            '',\n            'set -e  # Exit on any error',\n            '',\n            '# Validate environment variables',\n            'required_vars=(\"VIDEO_PATH\" \"OUTPUT_DIR\" \"DATABASE_URL\")',\n            'for var in \"${required_vars[@]}\"; do',\n            '  if [[ -z \"${!var}\" ]]; then',\n            '    echo \"ERROR: Required environment variable $var is not set\"',\n            '    exit 1',\n            '  fi',\n            'done',\n            '',\n            '# Set production environment',\n            'export ENVIRONMENT_TYPE=\"production\"',\n            'export DEBUG_MODE=\"false\"',\n            'export LOG_LEVEL=\"INFO\"',\n            '',\n            '# Run simulation with production settings',\n            'python -m {{cookiecutter.project_slug}}.cli.main \\\\',\n            '  --config-name=production \\\\',\n            '  environment.type=production \\\\',\n            '  logging.level=\"$LOG_LEVEL\" \\\\',\n            '  video_plume.video_path=\"$VIDEO_PATH\" \\\\',\n            '  simulation.output_dir=\"$OUTPUT_DIR\"',\n            '',\n            'echo \"Production simulation completed successfully\"'\n        ]\n    }\n]\n\nfor shell_example in shell_examples:\n    print(f\"🔧 {shell_example['name']}:\")\n    print(f\"   Description: {shell_example['description']}\")\n    print(f\"   Type: {shell_example['script_type']}\")\n    print()\n    print(\"   Script Content:\")\n    for line in shell_example['content']:\n        print(f\"   {line}\")\n    print()\n\nprint(\"📋 CLI Integration Best Practices:\")\nprint(\"  • Use meaningful parameter names and clear validation messages\")\nprint(\"  • Provide comprehensive help documentation and examples\")\nprint(\"  • Implement graceful error handling with actionable suggestions\")\nprint(\"  • Support both interactive and automated execution modes\")\nprint(\"  • Include environment variable validation for production deployments\")\nprint(\"  • Organize output directories with timestamps and experiment metadata\")\n\nprint(\"\\n✅ CLI integration and command-line workflow demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎯 8. Summary and Best Practices\n",
    "\n",
    "This final section consolidates the key learnings and provides actionable best practices for Hydra configuration management in production research environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-practices",
   "metadata": {},
   "source": [
    "### 8.1 Configuration Management Best Practices\n",
    "\n",
    "Let's summarize the essential patterns and practices for robust configuration management:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "best-practices-summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration management best practices and recommendations\n",
    "print(\"🎯 Hydra Configuration Management: Best Practices Summary\")\nprint(\"=\" * 58)\n\n# Comprehensive best practices guide\nbest_practices = {\n    'Configuration Architecture': {\n        'principles': [\n            'Use three-layer hierarchy: base.yaml → config.yaml → local overrides',\n            'Keep base.yaml immutable with conservative defaults',\n            'Place user customizations in config.yaml with clear documentation',\n            'Use local/ directory for environment-specific secrets (git-ignored)',\n            'Implement environment variable interpolation for deployment flexibility'\n        ],\n        'examples': [\n            'base.yaml: navigator.max_speed: 1.0  # Conservative default',\n            'config.yaml: navigator.max_speed: ${oc.env:MAX_SPEED,2.0}  # Environment override',\n            'CLI: navigator.max_speed=3.0  # Runtime override'\n        ]\n    },\n    'Schema Validation': {\n        'principles': [\n            'Define comprehensive Pydantic schemas for all configuration sections',\n            'Include field validation with clear error messages',\n            'Implement cross-field validation for parameter relationships',\n            'Use descriptive field descriptions and examples',\n            'Register schemas with Hydra ConfigStore for automatic validation'\n        ],\n        'examples': [\n            'Field validation: max_speed: float = Field(gt=0.0, description=\"Maximum speed\")',\n            'Cross-validation: @model_validator to check speed <= max_speed',\n            'Registration: cs.store(group=\"navigator\", name=\"config\", node=NavigatorConfig)'\n        ]\n    },\n    'Environment Integration': {\n        'principles': [\n            'Use ${oc.env:VAR_NAME,default} syntax for environment variables',\n            'Provide sensible defaults for all environment interpolations',\n            'Document required environment variables clearly',\n            'Validate environment variables before execution',\n            'Support multiple deployment environments (dev/test/prod)'\n        ],\n        'examples': [\n            'Secure paths: video_path: ${oc.env:VIDEO_PATH,data/default.mp4}',\n            'Credentials: database_url: ${oc.env:DATABASE_URL,sqlite:///local.db}',\n            'Debug flags: debug_mode: ${oc.env:DEBUG,false}'\n        ]\n    },\n    'Multi-run Orchestration': {\n        'principles': [\n            'Design parameter sweeps systematically with clear objectives',\n            'Use meaningful experiment names and metadata tracking',\n            'Implement result aggregation and analysis workflows',\n            'Organize output directories with timestamps and parameter info',\n            'Validate parameter combinations before execution'\n        ],\n        'examples': [\n            'Basic sweep: --multirun navigator.max_speed=1.0,2.0,3.0',\n            'Multi-dimensional: --multirun nav.speed=1,2 sim.fps=30,60',\n            'Range-based: --multirun \"param=range(0.5,3.0,0.5)\"'\n        ]\n    },\n    'CLI Integration': {\n        'principles': [\n            'Provide comprehensive help documentation and examples',\n            'Implement clear parameter validation with actionable error messages',\n            'Support both interactive and automated execution modes',\n            'Include configuration discovery and validation commands',\n            'Design for shell scripting and automation compatibility'\n        ],\n        'examples': [\n            'Help: python -m pkg.cli.main --help',\n            'Validation: python -m pkg.cli.main --cfg job',\n            'Override: python -m pkg.cli.main param.key=value'\n        ]\n    },\n    'Production Deployment': {\n        'principles': [\n            'Separate development and production configurations clearly',\n            'Implement secure credential management with environment variables',\n            'Use configuration validation in CI/CD pipelines',\n            'Monitor configuration changes and their impacts',\n            'Maintain backward compatibility for configuration schemas'\n        ],\n        'examples': [\n            'Environment: ENVIRONMENT_TYPE=production python -m pkg.cli.main',\n            'Security: DATABASE_URL=postgresql://... (never in code)',\n            'Validation: hydra --cfg validate config_name=production'\n        ]\n    }\n}\n\n# Display best practices\nfor category, practices in best_practices.items():\n    print(f\"\\n📋 {category}:\")\n    print(\"=\" * (len(category) + 4))\n    \n    print(\"\\n🎯 Key Principles:\")\n    for i, principle in enumerate(practices['principles'], 1):\n        print(f\"  {i}. {principle}\")\n    \n    print(\"\\n💡 Implementation Examples:\")\n    for example in practices['examples']:\n        print(f\"  • {example}\")\n    \n    print()\n\n# Common pitfalls and solutions\nprint(\"⚠️  Common Pitfalls and Solutions:\")\nprint(\"=\" * 35)\n\ncommon_pitfalls = [\n    {\n        'pitfall': 'Hardcoding sensitive values in configuration files',\n        'solution': 'Use environment variable interpolation: ${oc.env:SECRET_KEY}',\n        'impact': 'Security vulnerability, credential exposure'\n    },\n    {\n        'pitfall': 'Missing validation for parameter ranges and relationships',\n        'solution': 'Implement comprehensive Pydantic validators with clear error messages',\n        'impact': 'Runtime errors, invalid experiment configurations'\n    },\n    {\n        'pitfall': 'Inconsistent configuration structure across environments',\n        'solution': 'Use structured configuration with registered schemas',\n        'impact': 'Deployment failures, environment-specific bugs'\n    },\n    {\n        'pitfall': 'Poor parameter override organization in multi-run sweeps',\n        'solution': 'Plan parameter space systematically with clear naming conventions',\n        'impact': 'Difficult result analysis, experiment reproducibility issues'\n    },\n    {\n        'pitfall': 'Inadequate error handling and user feedback in CLI',\n        'solution': 'Provide actionable error messages with suggestions and examples',\n        'impact': 'Poor user experience, difficult debugging'\n    }\n]\n\nfor i, pitfall_info in enumerate(common_pitfalls, 1):\n    print(f\"{i}. {pitfall_info['pitfall']}\")\n    print(f\"   ❌ Impact: {pitfall_info['impact']}\")\n    print(f\"   ✅ Solution: {pitfall_info['solution']}\")\n    print()\n\n# Performance optimization recommendations\nprint(\"🚀 Performance Optimization Recommendations:\")\nprint(\"=\" * 43)\n\nperformance_tips = [\n    {\n        'area': 'Configuration Loading',\n        'tips': [\n            'Use Hydra\\'s configuration caching for repeated access',\n            'Minimize complex nested configurations in hot paths',\n            'Pre-validate configurations during application startup'\n        ]\n    },\n    {\n        'area': 'Multi-run Execution',\n        'tips': [\n            'Implement parallel execution for independent parameter sweeps',\n            'Use efficient output serialization (HDF5 vs JSON)',\n            'Monitor memory usage during large parameter sweeps'\n        ]\n    },\n    {\n        'area': 'CLI Responsiveness',\n        'tips': [\n            'Cache configuration validation results',\n            'Provide progressive feedback for long-running operations',\n            'Implement configuration precompilation for production'\n        ]\n    }\n]\n\nfor perf_tip in performance_tips:\n    print(f\"📈 {perf_tip['area']}:\")\n    for tip in perf_tip['tips']:\n        print(f\"  • {tip}\")\n    print()\n\nprint(\"📚 Additional Resources and Learning:\")\nprint(\"=\" * 36)\n\nresources = [\n    {\n        'category': 'Official Documentation',\n        'items': [\n            'Hydra Documentation: https://hydra.cc/',\n            'Pydantic Documentation: https://pydantic.dev/',\n            'OmegaConf Documentation: https://omegaconf.readthedocs.io/'\n        ]\n    },\n    {\n        'category': 'Advanced Topics',\n        'items': [\n            'Hydra Plugins and Custom Resolvers',\n            'Configuration Groups and Composition',\n            'Structured Configuration with Dataclasses'\n        ]\n    },\n    {\n        'category': 'Integration Patterns',\n        'items': [\n            'Kedro Pipeline Integration',\n            'MLflow Experiment Tracking',\n            'Docker Container Configuration'\n        ]\n    }\n]\n\nfor resource_group in resources:\n    print(f\"📖 {resource_group['category']}:\")\n    for item in resource_group['items']:\n        print(f\"  • {item}\")\n    print()\n\nprint(\"🎓 Tutorial Completion Summary:\")\nprint(\"=\" * 30)\n\ncompletion_checklist = [\n    'Hierarchical configuration composition and inheritance patterns',\n    'Interactive parameter exploration with Hydra Compose API',\n    'Robust configuration validation using Pydantic schemas',\n    'Environment variable integration and secure credential management',\n    'Multi-run parameter sweeps and experiment orchestration',\n    'CLI integration patterns and command-line workflow automation',\n    'Best practices for production deployment and maintenance'\n]\n\nprint(\"✅ Topics Covered:\")\nfor i, topic in enumerate(completion_checklist, 1):\n    print(f\"  {i}. {topic}\")\n\nprint(\"\\n🎯 Next Steps:\")\nprint(\"  1. Implement these patterns in your own odor plume navigation projects\")\nprint(\"  2. Experiment with advanced Hydra features like plugins and custom resolvers\")\nprint(\"  3. Integrate with your existing CI/CD and deployment pipelines\")\nprint(\"  4. Explore integration with Kedro, MLflow, or other ML workflow tools\")\nprint(\"  5. Contribute improvements back to the open-source community\")\n\nprint(\"\\n🎉 Congratulations! You have completed the comprehensive Hydra configuration tutorial.\")\nprint(\"    You are now equipped with advanced configuration management skills for\")\nprint(\"    production-ready research software development.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-conclusion",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 Tutorial Conclusion\n",
    "\n",
    "### What You've Accomplished\n",
    "\n",
    "Through this comprehensive tutorial, you have mastered advanced Hydra configuration management patterns essential for production-ready research software. You've learned to:\n",
    "\n",
    "✅ **Design Sophisticated Configuration Architectures** - Implement three-layer hierarchical composition with inheritance, overrides, and environment-specific customization\n",
    "\n",
    "✅ **Interactive Parameter Exploration** - Use Hydra's Compose API for dynamic configuration assembly and notebook-based research workflows\n",
    "\n",
    "✅ **Robust Validation Systems** - Integrate Pydantic schemas for type-safe configuration with comprehensive error handling and diagnostics\n",
    "\n",
    "✅ **Secure Environment Integration** - Implement environment variable interpolation for credential management and deployment flexibility\n",
    "\n",
    "✅ **Automated Experiment Orchestration** - Design multi-run parameter sweeps with result aggregation and analysis workflows\n",
    "\n",
    "✅ **Production CLI Integration** - Create command-line interfaces supporting both interactive and automated execution patterns\n",
    "\n",
    "### Technical Skills Acquired\n",
    "\n",
    "- **Configuration-Driven Development**: Design software systems that adapt behavior through configuration rather than code changes\n",
    "- **Enterprise-Grade Validation**: Implement comprehensive validation with clear error messages and recovery strategies  \n",
    "- **Deployment Automation**: Create reproducible deployment workflows with environment-specific configurations\n",
    "- **Experiment Management**: Systematically explore parameter spaces with automated result collection and analysis\n",
    "- **Production Readiness**: Apply best practices for maintainable, scalable configuration systems\n",
    "\n",
    "### Ready for Production\n",
    "\n",
    "You are now equipped to implement these patterns in real-world research projects, contributing to:\n",
    "\n",
    "- **Reproducible Research**: Systematic experiment management with comprehensive metadata tracking\n",
    "- **Collaborative Development**: Clear configuration interfaces enabling team collaboration\n",
    "- **Scalable Deployment**: Flexible systems adapting from development to production environments\n",
    "- **Maintainable Software**: Well-structured configuration systems reducing technical debt\n",
    "\n",
    "### Continue Learning\n",
    "\n",
    "Explore the advanced topics in the odor plume navigation library:\n",
    "- **`01_basic_navigation_demo.ipynb`** - Core navigation concepts and algorithms\n",
    "- **`02_multi_agent_coordination.ipynb`** - Advanced multi-agent simulation patterns\n",
    "- **`04_visualization_techniques.ipynb`** - Publication-quality visualization workflows\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for completing this comprehensive Hydra configuration tutorial!** 🚀\n",
    "\n",
    "*Your feedback and contributions to improving this tutorial are welcome.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",\n   "language": "python",\n   "name": "python3"
  },\n  "language_info": {\n   "codemirror_mode": {\n    "name": "ipython",\n    "version": 3\n   },\n   "file_extension": ".py",\n   "mimetype": "text/x-python",\n   "name": "python",\n   "nbconvert_exporter": "python",\n   "pygments_lexer": "ipython3",\n   "version": "3.9.0"\n  },\n  "tags": [\n   "hydra",\n   "configuration",\n   "tutorial",\n   "pydantic",\n   "environment-variables",\n   "multi-run",\n   "cli",\n   "best-practices"\n  ]\n }\n}