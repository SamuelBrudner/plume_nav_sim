{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Temporal-Gradient Agent Demo (Oriented Control)\n",
    "\n",
    "This notebook demonstrates a simple oriented agent that:\n",
    "- Keeps a one-back odor history to estimate a temporal derivative (dC/dt)\n",
    "- Surges FORWARD on non-decreasing odor, casts by TURNing when odor decreases\n",
    "- Uses a Gaussian plume environment with a step-penalty reward (sparse goal bonus + small per-step penalty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.pop(\"MPLBACKEND\", None)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import plume_nav_sim\n",
    "\n",
    "# Oriented action ids\n",
    "FORWARD, TURN_LEFT, TURN_RIGHT = 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TemporalGradientAgent:\n",
    "    cast_right_first: bool = True\n",
    "    eps: float = 1e-6  # avoid noise-driven flips\n",
    "\n",
    "    prev_moving: float | None = None  # last concentration after FORWARD\n",
    "    cast_right: bool = True\n",
    "    last_action: int | None = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.cast_right = self.cast_right_first\n",
    "\n",
    "    def act(self, obs: np.ndarray) -> int:\n",
    "        \"\"\"Oriented surge/cast using temporal gradient.\n",
    "        Forces a FORWARD probe after turns because rotations don't change concentration.\n",
    "        \"\"\"\n",
    "        c = float(obs[0])\n",
    "\n",
    "        if self.prev_moving is None:\n",
    "            self.prev_moving = c\n",
    "            self.last_action = FORWARD\n",
    "            return FORWARD\n",
    "\n",
    "        if self.last_action in (TURN_LEFT, TURN_RIGHT):\n",
    "            self.last_action = FORWARD\n",
    "            return FORWARD\n",
    "\n",
    "        dc = c - self.prev_moving\n",
    "        if dc >= self.eps:\n",
    "            self.prev_moving = c\n",
    "            self.last_action = FORWARD\n",
    "            return FORWARD\n",
    "\n",
    "        # Negative trend: alternate cast direction\n",
    "        self.cast_right = not self.cast_right\n",
    "        action = TURN_RIGHT if self.cast_right else TURN_LEFT\n",
    "        self.last_action = action\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "grid_size = (64, 64)\n",
    "source_location = (48, 48)\n",
    "start_location = (16, 16)\n",
    "goal_radius = 1.0\n",
    "max_steps = 500\n",
    "seed = 123\n",
    "\n",
    "env = plume_nav_sim.make_env(\n",
    "    grid_size=grid_size,\n",
    "    source_location=source_location,\n",
    "    start_location=start_location,\n",
    "    goal_radius=goal_radius,\n",
    "    max_steps=max_steps,\n",
    "    plume_sigma=20.0,\n",
    "    action_type=\"oriented\",\n",
    "    observation_type=\"concentration\",\n",
    "    reward_type=\"step_penalty\",\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "# Peek the underlying plume field for visualization\n",
    "core_env = getattr(env, \"_core_env\", None)\n",
    "field_array = getattr(\n",
    "    getattr(core_env, \"_concentration_field\", None), \"field_array\", None\n",
    ")\n",
    "assert field_array is not None, \"Could not access plume field for visualization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one episode\n",
    "obs, info = env.reset(seed=seed)\n",
    "agent = TemporalGradientAgent()\n",
    "\n",
    "positions = []\n",
    "concentrations = []\n",
    "rewards = []\n",
    "totals = []\n",
    "distances = []\n",
    "actions = []\n",
    "\n",
    "# Record initial\n",
    "positions.append(tuple(info.get(\"agent_position\") or info.get(\"agent_xy\")))\n",
    "concentrations.append(float(obs[0]))\n",
    "totals.append(float(info.get(\"total_reward\", 0.0)))\n",
    "distances.append(float(info.get(\"distance_to_goal\", np.nan)))\n",
    "\n",
    "terminated = truncated = False\n",
    "for t in range(max_steps):\n",
    "    action = agent.act(obs)\n",
    "    actions.append(action)\n",
    "    obs, reward, terminated, truncated, step_info = env.step(action)\n",
    "    rewards.append(float(reward))\n",
    "    positions.append(tuple(step_info.get(\"agent_position\")))\n",
    "    concentrations.append(float(obs[0]))\n",
    "    totals.append(float(step_info.get(\"total_reward\", totals[-1])))\n",
    "    distances.append(float(step_info.get(\"distance_to_goal\", np.nan)))\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"Finished: steps={len(rewards)}, terminated={terminated}, truncated={truncated}, total_reward={totals[-1]:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot plume field with path overlay\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "# Field as grayscale\n",
    "ax[0].imshow(field_array, cmap=\"gray\", origin=\"lower\")\n",
    "# Path overlay (x along width, y along height)\n",
    "xs = [p[0] for p in positions]\n",
    "ys = [p[1] for p in positions]\n",
    "ax[0].plot(xs, ys, color=\"cyan\", linewidth=2, label=\"path\")\n",
    "ax[0].scatter([source_location[0]], [source_location[1]], c=\"red\", s=40, label=\"source\")\n",
    "ax[0].set_title(\"Plume Field + Agent Path\")\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlim(0, grid_size[0] - 1)\n",
    "ax[0].set_ylim(0, grid_size[1] - 1)\n",
    "\n",
    "# Time series: concentration and rewards\n",
    "ts = np.arange(len(concentrations))\n",
    "dc = np.diff(concentrations, prepend=concentrations[0])\n",
    "ax[1].plot(ts, concentrations, label=\"concentration\")\n",
    "ax[1].plot(ts, dc, label=\"dC (temporal)\")\n",
    "ax[1].step(np.arange(len(rewards)) + 1, rewards, where=\"post\", label=\"step reward\")\n",
    "ax[1].plot(ts, totals, label=\"total reward\")\n",
    "ax[1].set_title(\"Signals over time\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plume-nav-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
