{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Temporal-Gradient Policy Demo (Deterministic vs Stochastic)\n",
    "\n",
    "This notebook demonstrates using Policy abstractions for an oriented agent that follows a temporal gradient of odor concentration. Choose between:\n",
    "- TemporalDerivativeDeterministicPolicy\n",
    "- TemporalDerivativePolicy (epsilon-greedy stochastic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ.pop(\"MPLBACKEND\", None)\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import plume_nav_sim\n",
    "from plume_nav_sim.policies import (\n",
    "    TemporalDerivativeDeterministicPolicy,\n",
    "    TemporalDerivativePolicy,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment\n",
    "grid_size = (64, 64)\n",
    "source_location = (48, 48)\n",
    "start_location = (16, 16)\n",
    "goal_radius = 1.0\n",
    "max_steps = 500\n",
    "seed = 123\n",
    "\n",
    "env = plume_nav_sim.make_env(\n",
    "    grid_size=grid_size,\n",
    "    source_location=source_location,\n",
    "    start_location=start_location,\n",
    "    goal_radius=goal_radius,\n",
    "    max_steps=max_steps,\n",
    "    plume_sigma=20.0,\n",
    "    action_type=\"oriented\",\n",
    "    observation_type=\"concentration\",\n",
    "    reward_type=\"step_penalty\",\n",
    "    render_mode=None,\n",
    ")\n",
    "\n",
    "core_env = getattr(env, \"_core_env\", None)\n",
    "field_array = getattr(\n",
    "    getattr(core_env, \"_concentration_field\", None), \"field_array\", None\n",
    ")\n",
    "assert field_array is not None, \"Could not access plume field for visualization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a policy\n",
    "policy_kind = \"stochastic\"  # 'deterministic' or 'stochastic'\n",
    "eps = 0.05\n",
    "eps_after_turn = 0.05\n",
    "forward_bias = 0.05\n",
    "\n",
    "if policy_kind == \"deterministic\":\n",
    "    policy = TemporalDerivativeDeterministicPolicy()\n",
    "else:\n",
    "    policy = TemporalDerivativePolicy(\n",
    "        eps=eps, eps_after_turn=eps_after_turn, eps_greedy_forward_bias=forward_bias\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run episode with selected policy\n",
    "obs, info = env.reset(seed=seed)\n",
    "policy.reset(seed=seed)\n",
    "\n",
    "rewards = []\n",
    "actions = []\n",
    "\n",
    "positions = [tuple((info.get(\"agent_position\") or info.get(\"agent_xy\")))]\n",
    "concentrations = [float(obs[0])]\n",
    "totals = [float(info.get(\"total_reward\", 0.0))]\n",
    "distances = [float(info.get(\"distance_to_goal\", np.nan))]\n",
    "terminated = truncated = False\n",
    "for _ in range(max_steps):\n",
    "    action = policy.select_action(obs, explore=True)\n",
    "    actions.append(action)\n",
    "    obs, reward, terminated, truncated, step_info = env.step(action)\n",
    "    rewards.append(float(reward))\n",
    "    positions.append(tuple(step_info.get(\"agent_position\")))\n",
    "    concentrations.append(float(obs[0]))\n",
    "    totals.append(float(step_info.get(\"total_reward\", totals[-1])))\n",
    "    distances.append(float(step_info.get(\"distance_to_goal\", np.nan)))\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\n",
    "    f\"Finished: steps={len(rewards)}, terminated={terminated}, truncated={truncated}, total_reward={totals[-1]:.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot plume field with path overlay and time series\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].imshow(field_array, cmap=\"gray\", origin=\"lower\")\n",
    "xs = [p[0] for p in positions]\n",
    "ys = [p[1] for p in positions]\n",
    "ax[0].plot(xs, ys, color=\"cyan\", linewidth=2, label=\"path\")\n",
    "ax[0].scatter([source_location[0]], [source_location[1]], c=\"red\", s=40, label=\"source\")\n",
    "ax[0].set_title(\"Plume Field + Agent Path\")\n",
    "ax[0].legend(loc=\"upper right\")\n",
    "ax[0].set_xlim(0, grid_size[0] - 1)\n",
    "ax[0].set_ylim(0, grid_size[1] - 1)\n",
    "\n",
    "ts = np.arange(len(concentrations))\n",
    "dc = np.diff(concentrations, prepend=concentrations[0])\n",
    "ax[1].plot(ts, concentrations, label=\"concentration\")\n",
    "ax[1].plot(ts, dc, label=\"dC (temporal)\")\n",
    "ax[1].step(np.arange(len(rewards)) + 1, rewards, where=\"post\", label=\"step reward\")\n",
    "ax[1].plot(ts, totals, label=\"total reward\")\n",
    "ax[1].set_title(\"Signals over time\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plume-nav-sim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
