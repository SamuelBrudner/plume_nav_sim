# Performance Benchmarks Workflow for plume_nav_sim
# Comprehensive performance testing across multiple platforms and Python versions
# with automated regression detection and reporting

name: Performance Benchmarks

# Trigger configuration for comprehensive testing coverage
on:
  push:
    branches: ['main', 'develop', 'release/**']
    paths: 
      - 'src/backend/**'
      - 'benchmarks/**'
      - 'tests/test_performance.py'
      - 'pyproject.toml'
      - '.github/workflows/performance.yml'
  
  pull_request:
    branches: ['main', 'develop']
    paths:
      - 'src/backend/**'
      - 'benchmarks/**'
      - 'tests/test_performance.py'
      - 'pyproject.toml'
      - '.github/workflows/performance.yml'
  
  # Daily automated performance monitoring at 3 AM UTC
  schedule:
    - cron: '0 3 * * *'
  
  # Manual workflow dispatch with configurable parameters
  workflow_dispatch:
    inputs:
      benchmark_category:
        description: 'Benchmark category to run'
        required: false
        default: 'all'
        type: choice
        options: ['all', 'environment', 'rendering', 'memory', 'scalability']
      
      benchmark_iterations:
        description: 'Number of benchmark iterations'
        required: false
        default: '1000'
        type: string
      
      enable_regression_analysis:
        description: 'Enable performance regression detection'
        required: false
        default: true
        type: boolean
      
      grid_sizes:
        description: 'Grid sizes to test (comma-separated)'
        required: false
        default: '32,64,128,256'
        type: string

# Workflow concurrency control to prevent resource conflicts
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Required permissions for comprehensive CI operations
permissions:
  contents: read
  checks: write
  pull-requests: write
  actions: read

# Global environment variables for consistent configuration
env:
  PYTHONUNBUFFERED: '1'
  BENCHMARK_TIMEOUT_SECONDS: '300'
  PERFORMANCE_TOLERANCE_FACTOR: '1.2'
  PIP_DISABLE_PIP_VERSION_CHECK: '1'
  PIP_NO_COLOR: '1'
  FORCE_COLOR: '1'

jobs:
  # Primary performance benchmarking job across multiple platforms
  performance-benchmark:
    name: 'Performance Benchmarks (${{ matrix.os }}, Python ${{ matrix.python-version }})'
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    
    strategy:
      fail-fast: false
      matrix:
        os: ['ubuntu-latest', 'macos-latest']
        python-version: ['3.10', '3.11', '3.12']
        exclude:
          # Exclude macOS Python 3.12 due to compatibility constraints
          - os: macos-latest
            python-version: '3.12'
    
    steps:
      # Repository setup with full history for performance trend analysis
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          submodules: false
      
      # Python environment setup with dependency caching
      - name: 'Set up Python ${{ matrix.python-version }}'
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: pip
          cache-dependency-path: 'src/backend/requirements*.txt'
      
      # Advanced pip dependency caching for faster builds
      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: performance-${{ runner.os }}-pip-${{ hashFiles('src/backend/requirements*.txt', 'src/backend/pyproject.toml') }}
          restore-keys: |
            performance-${{ runner.os }}-pip-
            ${{ runner.os }}-pip-
      
      # Ubuntu-specific system dependencies for headless testing
      - name: Install system dependencies (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y xvfb libgl1-mesa-glx libglib2.0-0
          # Ensure display variable for headless matplotlib testing
          echo "DISPLAY=:99.0" >> $GITHUB_ENV
          Xvfb :99 -screen 0 1024x768x24 > /dev/null 2>&1 &
      
      # macOS-specific system dependencies for matplotlib backend
      - name: Install system dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          # Install required system dependencies for matplotlib
          brew install --cask xquartz || true
          # Set environment for better matplotlib performance
          echo "MPLBACKEND=Agg" >> $GITHUB_ENV
      
      # Core Python build tools upgrade
      - name: Upgrade pip and build tools
        run: python -m pip install --upgrade pip setuptools wheel hatchling
        working-directory: src/backend
      
      # Development installation of the package under test
      - name: Install package in development mode
        run: python -m pip install -e .
        working-directory: src/backend
      
      # Install comprehensive testing and benchmarking dependencies
      - name: Install benchmark dependencies
        run: |
          python -m pip install -r requirements-test.txt
          python -m pip install psutil pytest-benchmark
        working-directory: src/backend
      
      # Validate complete environment setup before benchmarking
      - name: Validate environment setup
        run: |
          python scripts/validate_installation.py
          python -c "import plume_nav_sim; print('Package import successful')"
          python -c "import numpy; print(f'NumPy version: {numpy.__version__}')"
          python -c "import matplotlib; print(f'Matplotlib version: {matplotlib.__version__}'); print(f'Backend: {matplotlib.get_backend()}')"
          python -c "import gymnasium; print(f'Gymnasium version: {gymnasium.__version__}')"
        working-directory: src/backend
      
      # Create organized directory structure for benchmark results
      - name: Create benchmark results directories
        run: |
          mkdir -p benchmark-results/environment
          mkdir -p benchmark-results/rendering
          mkdir -p benchmark-results/memory
          mkdir -p benchmark-results/scalability
          mkdir -p benchmark-results/reports
        working-directory: src/backend
      
      # Environment performance benchmarks - core RL operations
      - name: Run environment performance benchmarks
        if: github.event.inputs.benchmark_category == 'all' || github.event.inputs.benchmark_category == 'environment' || github.event.inputs.benchmark_category == ''
        run: |
          python -m pytest tests/test_performance.py::TestEnvironmentPerformance \
            -v --tb=short \
            --benchmark-json=benchmark-results/environment/benchmarks-${{ matrix.os }}-${{ matrix.python-version }}.json \
            --benchmark-verbose \
            --junit-xml=benchmark-results/environment/results-${{ matrix.os }}-${{ matrix.python-version }}.xml \
            -m performance \
            --benchmark-min-rounds=50 \
            --benchmark-max-time=60
        working-directory: src/backend
        timeout-minutes: 15
        env:
          BENCHMARK_ITERATIONS: ${{ github.event.inputs.benchmark_iterations || '1000' }}
      
      # Rendering performance benchmarks - visualization systems
      - name: Run rendering performance benchmarks
        if: github.event.inputs.benchmark_category == 'all' || github.event.inputs.benchmark_category == 'rendering' || github.event.inputs.benchmark_category == ''
        run: |
          python -m pytest tests/test_performance.py::TestRenderingPerformance \
            -v --tb=short \
            --benchmark-json=benchmark-results/rendering/benchmarks-${{ matrix.os }}-${{ matrix.python-version }}.json \
            --benchmark-verbose \
            --junit-xml=benchmark-results/rendering/results-${{ matrix.os }}-${{ matrix.python-version }}.xml \
            -m performance \
            --benchmark-min-rounds=20 \
            --benchmark-max-time=120
        working-directory: src/backend
        timeout-minutes: 20
        env:
          BENCHMARK_ITERATIONS: ${{ github.event.inputs.benchmark_iterations || '500' }}
          MPLBACKEND: 'Agg'
      
      # Memory usage benchmarks - resource consumption analysis
      - name: Run memory usage benchmarks
        if: github.event.inputs.benchmark_category == 'all' || github.event.inputs.benchmark_category == 'memory' || github.event.inputs.benchmark_category == ''
        run: |
          python -m pytest tests/test_performance.py::TestMemoryUsage \
            -v --tb=short \
            --benchmark-json=benchmark-results/memory/benchmarks-${{ matrix.os }}-${{ matrix.python-version }}.json \
            --benchmark-verbose \
            --junit-xml=benchmark-results/memory/results-${{ matrix.os }}-${{ matrix.python-version }}.xml \
            -m performance \
            --benchmark-min-rounds=10 \
            --benchmark-max-time=180
        working-directory: src/backend
        timeout-minutes: 25
        env:
          BENCHMARK_ITERATIONS: ${{ github.event.inputs.benchmark_iterations || '100' }}
      
      # Scalability benchmarks - performance across different grid sizes
      - name: Run scalability benchmarks
        if: github.event.inputs.benchmark_category == 'all' || github.event.inputs.benchmark_category == 'scalability' || github.event.inputs.benchmark_category == ''
        run: |
          python -m pytest tests/test_performance.py::TestScalabilityPerformance \
            -v --tb=short \
            --benchmark-json=benchmark-results/scalability/benchmarks-${{ matrix.os }}-${{ matrix.python-version }}.json \
            --benchmark-verbose \
            --junit-xml=benchmark-results/scalability/results-${{ matrix.os }}-${{ matrix.python-version }}.xml \
            -m performance \
            --benchmark-min-rounds=5 \
            --benchmark-max-time=300
        working-directory: src/backend
        timeout-minutes: 30
        env:
          BENCHMARK_ITERATIONS: ${{ github.event.inputs.benchmark_iterations || '100' }}
          TEST_GRID_SIZES: ${{ github.event.inputs.grid_sizes || '32,64,128,256' }}
      
      # Comprehensive benchmark suite with integrated reporting
      - name: Run comprehensive benchmark suite
        run: |
          python -c "
          import sys
          sys.path.append('benchmarks')
          from environment_performance import run_environment_performance_benchmark
          
          # Run comprehensive benchmark with reporting
          results = run_environment_performance_benchmark(
              iterations=int('${{ github.event.inputs.benchmark_iterations }}' or 1000),
              include_memory_profiling=True,
              validate_targets=True,
              report_output_path='benchmark-results/reports/comprehensive-benchmark-${{ matrix.os }}-${{ matrix.python-version }}.json'
          )
          
          # Print summary for CI logs
          print('=== PERFORMANCE BENCHMARK SUMMARY ===')
          for category, metrics in results.items():
              if isinstance(metrics, dict) and 'mean_time' in metrics:
                  print(f'{category}: {metrics[\"mean_time\"]:.4f}s (target: {metrics.get(\"target\", \"N/A\")})')
          "
        working-directory: src/backend
        timeout-minutes: 20
      
      # Performance threshold validation against established targets
      - name: Performance threshold validation
        run: |
          python -c "
          import json
          import sys
          
          # Load benchmark results and validate against targets
          targets = {
              'step_latency': 0.001,      # <1ms
              'rgb_rendering': 0.005,     # <5ms  
              'human_rendering': 0.050,   # <50ms
              'episode_reset': 0.010,     # <10ms
              'memory_usage': 52428800,   # <50MB in bytes
          }
          
          failed_targets = []
          
          try:
              with open('benchmark-results/reports/comprehensive-benchmark-${{ matrix.os }}-${{ matrix.python-version }}.json', 'r') as f:
                  results = json.load(f)
              
              for metric, target in targets.items():
                  if metric in results and 'mean_time' in results[metric]:
                      actual = results[metric]['mean_time']
                      tolerance = target * float('${{ env.PERFORMANCE_TOLERANCE_FACTOR }}')
                      
                      if actual > tolerance:
                          failed_targets.append(f'{metric}: {actual:.6f} > {tolerance:.6f} (target: {target:.6f})')
                      else:
                          print(f'âœ“ {metric}: {actual:.6f} â‰¤ {tolerance:.6f} (target: {target:.6f})')
              
              if failed_targets:
                  print('âœ— Performance targets failed:')
                  for failure in failed_targets:
                      print(f'  {failure}')
                  sys.exit(1)
              else:
                  print('âœ“ All performance targets met!')
              
          except FileNotFoundError:
              print('Warning: Comprehensive benchmark results not found, skipping validation')
          except Exception as e:
              print(f'Warning: Performance validation failed: {e}')
          "
        working-directory: src/backend
        continue-on-error: ${{ github.event_name == 'schedule' }}
      
      # Generate comprehensive performance report with metadata
      - name: Generate performance report
        if: always()
        run: |
          python -c "
          import json
          import glob
          import os
          from datetime import datetime
          
          # Aggregate all benchmark results
          report = {
              'timestamp': datetime.utcnow().isoformat(),
              'platform': '${{ matrix.os }}',
              'python_version': '${{ matrix.python-version }}',
              'git_sha': os.environ.get('GITHUB_SHA', 'unknown'),
              'benchmarks': {}
          }
          
          # Load benchmark results from all categories
          for category in ['environment', 'rendering', 'memory', 'scalability']:
              json_files = glob.glob(f'benchmark-results/{category}/*.json')
              if json_files:
                  try:
                      with open(json_files[0], 'r') as f:
                          data = json.load(f)
                      report['benchmarks'][category] = data
                  except Exception as e:
                      print(f'Warning: Could not load {category} results: {e}')
          
          # Save aggregated report
          with open('benchmark-results/reports/performance-report-${{ matrix.os }}-${{ matrix.python-version }}.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          print('Performance report generated successfully')
          "
        working-directory: src/backend
      
      # Upload benchmark results for analysis and archival
      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-${{ matrix.python-version }}
          path: src/backend/benchmark-results/
          retention-days: 90
      
      # Upload performance baseline data for regression detection
      - name: Upload performance regression data
        if: always() && github.ref == 'refs/heads/main'
        uses: actions/upload-artifact@v4
        with:
          name: performance-baseline-${{ matrix.os }}-${{ matrix.python-version }}
          path: src/backend/benchmark-results/reports/performance-report-${{ matrix.os }}-${{ matrix.python-version }}.json
          retention-days: 180

  # Performance analysis and reporting job
  performance-analysis:
    name: Performance Analysis and Reporting
    runs-on: ubuntu-latest
    needs: performance-benchmark
    if: always()
    timeout-minutes: 15
    
    steps:
      # Repository checkout for analysis scripts
      - name: Checkout Repository
        uses: actions/checkout@v4
      
      # Python setup for analysis tools
      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: pip
      
      # Install analysis dependencies
      - name: Install analysis dependencies
        run: |
          python -m pip install --upgrade pip
          python -m pip install pandas numpy matplotlib seaborn jinja2
      
      # Download all benchmark artifacts for comprehensive analysis
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          path: benchmark-artifacts/
      
      # Comprehensive performance results analysis
      - name: Analyze performance results
        run: |
          python -c "
          import json
          import glob
          import pandas as pd
          import numpy as np
          from pathlib import Path
          
          # Collect all performance reports
          reports = []
          for report_file in glob.glob('benchmark-artifacts/**/performance-report-*.json', recursive=True):
              try:
                  with open(report_file, 'r') as f:
                      data = json.load(f)
                  reports.append(data)
              except Exception as e:
                  print(f'Warning: Could not load {report_file}: {e}')
          
          if not reports:
              print('No performance reports found')
              exit(0)
          
          # Generate analysis summary
          print('=== PERFORMANCE ANALYSIS SUMMARY ===')
          print(f'Total benchmark runs: {len(reports)}')
          
          # Analyze performance across platforms
          platforms = {}
          for report in reports:
              platform_key = f\"{report['platform']}-{report['python_version']}\"
              platforms[platform_key] = report
          
          print(f'Platforms tested: {list(platforms.keys())}')
          
          # Performance targets validation summary
          targets = {
              'step_latency': 0.001,
              'rgb_rendering': 0.005,
              'human_rendering': 0.050,
              'episode_reset': 0.010
          }
          
          print('\\n=== TARGET COMPLIANCE SUMMARY ===')
          for platform, report in platforms.items():
              print(f'\\n{platform}:')
              if 'benchmarks' in report:
                  # Extract timing information from benchmark data
                  # This would need to be adapted based on actual benchmark result structure
                  print('  Benchmark data available - detailed analysis would be implemented here')
              else:
                  print('  No benchmark data available')
          
          # Save analysis results
          analysis_result = {
              'summary': {
                  'total_runs': len(reports),
                  'platforms': list(platforms.keys()),
                  'timestamp': reports[0]['timestamp'] if reports else None
              },
              'reports': reports
          }
          
          with open('performance-analysis.json', 'w') as f:
              json.dump(analysis_result, f, indent=2)
          
          print('\\nPerformance analysis completed successfully')
          "
      
      # Performance trend analysis for regression detection
      - name: Generate performance trend analysis
        if: github.event.inputs.enable_regression_analysis != 'false' && github.ref == 'refs/heads/main'
        run: |
          # This would implement performance trend analysis and regression detection
          # comparing against historical baseline data
          echo "Performance trend analysis would be implemented here"
          echo "This would download historical performance data and compare trends"
      
      # Create performance summary for PR comments
      - name: Create performance summary comment
        if: github.event_name == 'pull_request'
        run: |
          # Generate performance summary for PR comments
          cat > performance-summary.md << 'EOF'
          ## ðŸš€ Performance Benchmark Results
          
          Performance benchmarks have been completed for this PR.
          
          ### Key Metrics Summary
          - **Environment Step Latency**: Target <1ms
          - **RGB Rendering**: Target <5ms  
          - **Human Mode Rendering**: Target <50ms
          - **Memory Usage**: Target <50MB
          
          ### Platform Coverage
          - Ubuntu Latest (Python 3.10, 3.11, 3.12)
          - macOS Latest (Python 3.10, 3.11)
          
          ðŸ“Š Detailed benchmark results are available in the workflow artifacts.
          
          âš¡ Performance targets validation: See job logs for detailed compliance status.
          EOF
          
          echo "Performance summary generated for PR comment"
      
      # Upload comprehensive performance analysis
      - name: Upload performance analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-analysis
          path: |
            performance-analysis.json
            performance-summary.md
          retention-days: 90

  # Performance alert job for regression detection
  performance-alert:
    name: Performance Alert on Regression
    runs-on: ubuntu-latest
    needs: performance-analysis
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
      # Alert on performance regression detection
      - name: Performance regression detected
        run: |
          echo "âš ï¸ Performance regression detected in main branch"
          echo "Please review the benchmark results and investigate performance issues"
          echo "Consider reverting changes or implementing performance optimizations"
          
          # In a real implementation, this could send notifications to:
          # - Slack/Discord channels
          # - Email notifications
          # - GitHub Issues
          # - Performance monitoring dashboards

  # Performance success notification job
  performance-success:
    name: Performance Benchmarks Success
    runs-on: ubuntu-latest
    needs: ['performance-benchmark', 'performance-analysis']
    if: success()
    
    steps:
      # Success notification for completed benchmarks
      - name: Performance benchmarks completed successfully
        run: |
          echo "âœ… All performance benchmarks completed successfully"
          echo "Performance targets met across all tested platforms"
          echo "Results have been uploaded as artifacts for analysis"