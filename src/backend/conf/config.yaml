# Default environment configuration
# This config uses discrete actions, concentration sensor, and sparse reward

defaults:
  - _self_
  - experiment: ???  # Must be overridden

# Environment settings
grid_size: [128, 128]
goal_location: [64, 64]
start_location: null  # null = grid center
max_steps: 1000
render_mode: null

# Action processor
action:
  type: discrete
  step_size: 1
  parameters: {}

# Observation model
observation:
  type: concentration
  n_sensors: 2
  sensor_distance: 1.0
  sensor_angles: null
  parameters: {}

# Reward function
reward:
  type: sparse  # Options: 'sparse' or 'step_penalty'
  goal_radius: 5.0
  parameters: {}  # For step_penalty: {goal_reward: 10.0, step_penalty: 0.01}

# Plume field
plume:
  sigma: 20.0
  normalize: true
  enable_caching: true
  parameters: {}
