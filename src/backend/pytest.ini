[pytest]
# Minimum pytest version requirement ensuring compatibility with modern testing features,
# Python 3.10+ support, and advanced fixture management capabilities
minversion = 8.0

# Primary test directory specification for pytest discovery and execution within backend
# project structure following standard pytest conventions
testpaths = tests

# Test file patterns for comprehensive test discovery supporting both standard pytest 
# naming (test_*.py) and alternative naming (*_test.py) conventions
python_files = test_*.py *_test.py

# Test class patterns supporting both prefix (Test*) and suffix (*Test) naming
# conventions for flexible test organization and discovery
python_classes = Test* *Test

# Test function pattern following standard pytest convention for function-based
# test discovery and execution
python_functions = test_*

# Additional pytest options including short test summary (-ra), strict marker
# enforcement (--strict-markers), strict configuration validation (--strict-config),
# concise traceback format (--tb=short), and slowest test reporting
# (--durations=10 --durations-min=1.0)
addopts = -ra --strict-markers --strict-config --tb=short --durations=10 --durations-min=1.0 --cov=plume_nav_sim --cov-report=term-missing --cov-report=html:tests/coverage/html --cov-report=xml:tests/coverage/coverage.xml --cov-branch --cov-fail-under=90 -x --maxfail=5 --disable-warnings

# Comprehensive test markers for categorizing and selectively executing different
# types of tests during development and CI/CD workflows with specific test targeting capabilities
markers =
    unit: marks unit tests for individual component validation
    integration: marks integration tests for cross-component validation
    performance: marks performance tests with timing and resource monitoring
    slow: marks slow-running tests requiring extended execution time
    reproducibility: marks reproducibility tests with deterministic seeding
    edge_case: marks edge case tests with boundary conditions
    memory: marks memory usage tests with leak detection
    scalability: marks scalability tests across different grid sizes
    regression: marks regression tests for performance trend analysis
    concurrent: marks concurrent execution tests with thread safety
    benchmark: marks benchmark tests with statistical analysis
    rendering: marks rendering system tests with visualization validation
    api_compliance: marks Gymnasium API compliance tests
    functional: marks functional requirement validation tests
    stress: marks stress tests with extreme parameter validation
    flaky: marks potentially unstable tests requiring retry logic
    gpu: marks tests requiring GPU acceleration (skipped if unavailable)
    network: marks tests requiring network connectivity
    timeout: marks tests with extended execution time requirements
    parametrized: marks parametrized tests with multiple input combinations

# Warning filter configuration treating most warnings as errors for strict testing
# while ignoring known matplotlib and gymnasium warnings that don't affect functionality
filterwarnings =
    error
    ignore::UserWarning:matplotlib.*
    ignore::DeprecationWarning:gymnasium.*
    ignore::PendingDeprecationWarning
    ignore::RuntimeWarning:numpy.*
    ignore::FutureWarning:gymnasium.spaces.*
    ignore::ResourceWarning
    ignore:.*backend.*:UserWarning:matplotlib.*

# Enable live logging to console during test execution for real-time debugging
# and development feedback
log_cli = true

# Console logging level set to INFO for comprehensive test execution feedback
# without verbose debug output
log_cli_level = INFO

# Structured console log format with timestamp, level, logger name, and message
# for clear test execution tracking
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s

# Console log timestamp format for precise test execution timing and debugging analysis
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Test execution log file for persistent logging and comprehensive test run analysis
log_file = tests/logs/pytest.log

# File logging level set to DEBUG for comprehensive test execution analysis
# and troubleshooting
log_file_level = DEBUG

# Detailed file log format with timestamp, level, filename, line number, function name,
# and message for comprehensive debugging support
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s

# File log timestamp format matching console format for consistent temporal analysis
log_file_date_format = %Y-%m-%d %H:%M:%S

# Console output style showing progress indicators for test execution feedback
# during long-running test suites
console_output_style = progress

# JUnit XML output format for CI/CD integration and test result reporting
# in continuous integration pipelines
junit_family = xunit2

# JUnit XML suite name for clear identification in CI/CD reporting and test result analysis
junit_suite_name = plume_nav_sim_test_suite

# JUnit XML output file location for CI/CD pipeline integration
junit_logging = all
junit_log_passing_tests = true

# Default test timeout of 300 seconds preventing indefinite test execution
# and ensuring robust CI/CD pipeline behavior
timeout = 300

# Timeout method using threading for reliable timeout enforcement without
# affecting test isolation
timeout_method = thread

# Files to ignore during test collection preventing execution of setup scripts
# and configuration files as tests
collect_ignore = setup.py conftest.py __pycache__ .pytest_cache build dist *.egg-info .tox .venv venv .git node_modules

# Mark empty parameter sets as expected failures preventing test suite interruption
# from parameterization edge cases
empty_parameter_set_mark = xfail

# Faulthandler timeout matching main timeout for comprehensive failure analysis
# and debugging support
faulthandler_timeout = 300

# Enable faulthandler for detailed crash analysis and debugging support
addopts_faulthandler = --tb=long --capture=no

# Test session configuration for comprehensive test execution control
session_start_timeout = 60

# Pytest cache configuration for improved test execution performance
cache_dir = tests/.pytest_cache

# Test output configuration for enhanced debugging and analysis
tb = short
showlocals = true
verbosity = 1

# Test collection configuration for comprehensive test discovery optimization
collect_ignore_glob = 
    *.pyc
    __pycache__
    .pytest_cache
    build
    dist
    *.egg-info
    .tox
    .venv
    venv
    .git
    node_modules
    *.log
    *.tmp

# Test execution order configuration for consistent and predictable test runs
random-order = false
random-order-bucket = none

[tool:pytest]
# Global patterns to ignore during test collection including compiled Python files,
# cache directories, build artifacts, and virtual environments
collect_ignore_glob = *.pyc __pycache__ .pytest_cache build dist *.egg-info .tox .venv venv .git node_modules

# Directories to exclude from recursive test discovery preventing collection
# in build artifacts, version control, and dependency directories
norecursedirs = *.egg .eggs dist build docs .tox .git __pycache__ .pytest_cache node_modules .mypy_cache .coverage htmlcov

# Test discovery configuration for optimized test collection and execution
python_paths = src tests

# Test module configuration for comprehensive test coverage and organization
testmon-off = true

[coverage:run]
# Source code directory for coverage analysis focusing on main package implementation
# excluding test code
source = plume_nav_sim

# Branch coverage analysis ensuring comprehensive test coverage including
# conditional logic paths and edge cases
branch = True

# Files and directories to exclude from coverage analysis including test files,
# setup scripts, and cache directories
omit = 
    */tests/*
    */test_*
    setup.py
    */conftest.py
    */__pycache__/*
    */.*
    */venv/*
    */virtualenv/*
    */build/*
    */dist/*
    */.pytest_cache/*
    */.coverage
    */htmlcov/*

# Parallel coverage collection supporting pytest-xdist parallel execution
# for accurate coverage measurement
parallel = True

# Coverage data file location for consolidated reporting across parallel processes
data_file = tests/coverage/.coverage

# Enable subprocess coverage for comprehensive test coverage measurement
concurrency = multiprocessing

# Coverage measurement context for detailed analysis and debugging
dynamic_context = test_function

[coverage:report]
# Coverage percentage precision for detailed analysis and quality metrics tracking
precision = 2

# Show missing line numbers in coverage reports for targeted test improvement efforts
show_missing = True

# Include fully covered files in reports for comprehensive visibility and validation
skip_covered = False

# Sort coverage report by coverage percentage for prioritized improvement targeting
sort = Cover

# Minimum coverage threshold of 90% for test suite success ensuring high-quality
# test coverage standards
fail_under = 90

# Exclude specific lines from coverage analysis using pragma comments
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

# Coverage report output directory for comprehensive analysis and documentation
show_contexts = True

[coverage:html]
# HTML coverage report output directory for detailed visualization and analysis
directory = tests/coverage/html

# HTML report title for clear identification and documentation
title = Plume Nav Sim Coverage Report

# Show coverage contexts in HTML report for detailed analysis
show_contexts = True

[coverage:xml]
# XML coverage report output file for CI/CD integration and automated analysis
output = tests/coverage/coverage.xml

[coverage:json]
# JSON coverage report output file for programmatic analysis and integration
output = tests/coverage/coverage.json

# Pretty-print JSON output for human readability
pretty_print = True

[tool:pytest-benchmark]
# Minimum benchmark rounds for statistical significance and reliable performance measurement
min_rounds = 10

# Maximum benchmark execution time per test preventing excessive benchmark duration
# while ensuring accuracy
max_time = 30.0

# Disable garbage collection during benchmarks for accurate timing measurement
# and reduced variance
disable_gc = True

# Enable benchmark warmup for stable timing baseline and accurate performance measurement
warmup = True

# Number of warmup iterations for performance baseline establishment and measurement stability
warmup_iterations = 100000

# Sort benchmark results by mean execution time for performance comparison and analysis
sort = mean

# Generate performance histograms for detailed timing distribution analysis
# and optimization insights
histogram = True

# Benchmark output format for comprehensive performance analysis and reporting
output_format = json

# Benchmark output file location for persistent performance data storage
json_output = tests/benchmarks/benchmark_results.json

# Benchmark comparison baseline for performance regression detection
compare = tests/benchmarks/benchmark_baseline.json

# Performance regression threshold for automated performance monitoring
compare-fail = mean:5% min:10% max:10%

[tool:pytest-xdist]
# Parallel execution configuration for enhanced development workflow performance
# and CI efficiency
dist = loadscope

# Load balancing strategy for optimal parallel test execution
loadbalance = True

# Parallel execution logging configuration for debugging and monitoring
log_cli_level = INFO

# Parallel execution timeout configuration for robust CI/CD pipeline execution
timeout = 600

[tool:pytest-timeout]
# Timeout configuration for individual test execution control
timeout = 60

# Timeout method for reliable timeout enforcement
method = thread

# Timeout grace period for cleanup operations
grace_period = 10

[tool:pytest-mock]
# Mock configuration for comprehensive component isolation and testing
mock_use_standalone_module = True

[tool:pytest-clarity]
# Enhanced assertion introspection for improved debugging and test failure analysis
override_django_settings = False

[tool:pytest-html]
# HTML test report configuration for comprehensive test result documentation
report_title = Plume Nav Sim Test Report

# HTML report output file location
html_report = tests/reports/pytest_report.html

# Include captured output in HTML report for comprehensive debugging
capture = sys

[tool:pytest-json-report]
# JSON test report configuration for programmatic analysis and CI/CD integration
json_report = tests/reports/pytest_report.json

# Include test results summary in JSON report
summary = True

# Include test metadata in JSON report
include_metadata = True

[tool:pytest-order]
# Test execution order configuration for consistent and predictable test runs
order_dependencies = True

# Order scope for test execution control
order_scope = session

[tool:pytest-instafail]
# Instant failure reporting for rapid feedback during development
instafail = True

[tool:pytest-sugar]
# Enhanced test output formatting for improved readability and user experience
sugar = True

[tool:pytest-profiling]
# Performance profiling configuration for detailed test execution analysis
prof = True

# Profiling output directory for comprehensive performance analysis
prof_path = tests/profiling

[tool:pytest-memprof]
# Memory profiling configuration for memory usage analysis and optimization
memprof = True

# Memory profiling output directory
memprof_path = tests/profiling/memory

[tool:pytest-dependency]
# Test dependency configuration for ordered test execution
dependency = True

# Dependency scope configuration
dependency_scope = session

[tool:pytest-repeat]
# Test repetition configuration for reliability and stability testing
repeat = 1

# Repeat scope configuration
repeat_scope = session

[tool:pytest-rerunfailures]
# Failed test rerun configuration for flaky test handling
reruns = 3

# Rerun delay configuration for stable test execution
reruns_delay = 1

# Only rerun on specific exceptions
only_rerun = AssertionError,ValueError,TimeoutError

[tool:pytest-env]
# Environment variable configuration for consistent test execution
PYTHONPATH = src:tests
PLUME_NAV_TEST_MODE = true
MATPLOTLIB_BACKEND = Agg
NUMBA_DISABLE_JIT = 1