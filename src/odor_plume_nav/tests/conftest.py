"""
Comprehensive pytest configuration and fixture management for odor plume navigation testing.

This module serves as the central testing infrastructure providing shared fixtures for CLI testing,
Hydra configuration composition, database session management, and comprehensive mocking strategies.
The implementation supports pytest-hydra integration, Click CliRunner setup, in-memory SQLite sessions,
and domain-specific mocks for scientific computing workflows.

Enhanced Testing Features:
- pytest-hydra fixtures for hierarchical configuration testing per Section 6.6.1.1
- Click CliRunner integration for comprehensive CLI interface validation
- In-memory SQLAlchemy database session fixtures for database testing per Section 6.6.5.2
- Deterministic test behavior through controlled randomization per Section 6.6.8
- Comprehensive mock framework integration supporting unittest.mock patterns
- Configuration override and validation test fixtures per Section 6.6.1.1

Test Coverage Targets:
- Overall: >80% line coverage per Section 6.6.3.1
- CLI modules: ≥85% coverage per enhanced testing standards
- Configuration schemas: ≥90% coverage per Section 6.6.3.1
- Database session management: ≥80% coverage per Section 6.6.5.2
- Utility modules: ≥85% coverage per Section 6.6.3.1

Architecture Integration:
- Supports unified package structure conventions
- Integrates with Hydra configuration system for parameter management
- Provides ready-to-activate database testing without external dependencies
- Maintains backward compatibility with existing test patterns
- Enables comprehensive integration testing across CLI, configuration, and database domains

Usage Examples:

    # Basic fixture usage in test functions
    def test_navigator_with_config(hydra_config_fixture, mock_seed_manager):
        # Configuration testing with deterministic randomization
        config = hydra_config_fixture({"navigator": {"max_speed": 2.0}})
        assert config.navigator.max_speed == 2.0
    
    # CLI testing with parameter validation
    def test_cli_command_execution(cli_runner_fixture):
        runner, env = cli_runner_fixture
        result = runner.invoke(main_command, ["--help"], env=env)
        assert result.exit_code == 0
    
    # Database session testing with isolation
    def test_database_operations(db_session_fixture):
        session = db_session_fixture
        if session:  # Graceful handling when database not enabled
            # Perform database operations
            pass
    
    # Comprehensive integration testing
    def test_full_system_integration(
        hydra_config_fixture, 
        cli_runner_fixture, 
        mock_video_plume, 
        db_session_fixture
    ):
        # Multi-domain integration test
        pass

Dependencies Integration:
- pytest ≥7.0.0: Enhanced fixture support and parameterized testing
- pytest-hydra ≥0.1.0: Specialized Hydra configuration testing plugin
- click-testing ≥8.0.0: Click CLI testing utilities and CliRunner support
- SQLAlchemy ≥2.0: Database session management with in-memory testing
- unittest.mock: Standard library mocking framework for dependency isolation
- numpy.testing: Specialized numerical array assertions with tolerance support

Configuration Management:
- Hierarchical configuration composition testing through pytest-hydra
- Environment variable interpolation validation with secure credential handling
- Configuration override scenario testing across development and production settings
- Schema validation testing with Pydantic integration and type safety verification

Authors: Generated by Blitzy Template Engine
Version: 2.0.0
License: MIT
"""

import os
import sys
import time
import tempfile
import warnings
from pathlib import Path
from typing import Any, Dict, Generator, Optional, Union, List, Tuple
from unittest.mock import Mock, MagicMock, patch
from contextlib import contextmanager

import pytest
import numpy as np

# Core testing framework imports
try:
    from click.testing import CliRunner
    CLICK_TESTING_AVAILABLE = True
except ImportError:
    CLICK_TESTING_AVAILABLE = False
    warnings.warn("Click testing not available. CLI tests will be skipped.", ImportWarning)

# Hydra testing integration
try:
    from hydra import compose, initialize_config_store
    from hydra.core.config_store import ConfigStore
    from hydra.core.global_hydra import GlobalHydra
    from omegaconf import DictConfig, OmegaConf
    HYDRA_AVAILABLE = True
except ImportError:
    HYDRA_AVAILABLE = False
    DictConfig = dict
    warnings.warn("Hydra not available. Configuration testing will be limited.", ImportWarning)

# pytest-hydra plugin integration
try:
    import pytest_hydra
    PYTEST_HYDRA_AVAILABLE = True
except ImportError:
    PYTEST_HYDRA_AVAILABLE = False
    warnings.warn("pytest-hydra not available. Advanced configuration testing disabled.", ImportWarning)

# Database testing infrastructure
try:
    from sqlalchemy import create_engine, text
    from sqlalchemy.orm import sessionmaker
    from sqlalchemy.pool import StaticPool
    SQLALCHEMY_AVAILABLE = True
except ImportError:
    SQLALCHEMY_AVAILABLE = False
    warnings.warn("SQLAlchemy not available. Database testing will be skipped.", ImportWarning)

# Import project modules with graceful fallback
try:
    from odor_plume_nav.config.schemas import (
        NavigatorConfig, VideoPlumeConfig, SingleAgentConfig, MultiAgentConfig
    )
    from odor_plume_nav.utils.seed_manager import SeedManager, SeedConfig
    SCHEMAS_AVAILABLE = True
except ImportError:
    SCHEMAS_AVAILABLE = False
    warnings.warn("Project schemas not available. Some fixtures will be limited.", ImportWarning)

try:
    from odor_plume_nav.cli.main import cli as main_cli
    from odor_plume_nav.db.session import SessionManager, get_session
    CLI_AVAILABLE = True
except ImportError:
    CLI_AVAILABLE = False
    warnings.warn("CLI modules not available. CLI testing will be limited.", ImportWarning)


# ================================================================================================
# PYTEST CONFIGURATION AND SETUP
# ================================================================================================

def pytest_configure(config):
    """
    Configure pytest environment for enhanced scientific computing testing.
    
    Sets up markers, plugins, and environment variables required for comprehensive
    testing across CLI, configuration, database, and scientific computation domains.
    """
    # Register custom markers for test categorization
    config.addinivalue_line(
        "markers", 
        "hydra: mark test as requiring Hydra configuration system"
    )
    config.addinivalue_line(
        "markers", 
        "cli: mark test as requiring CLI infrastructure"
    )
    config.addinivalue_line(
        "markers", 
        "database: mark test as requiring database session support"
    )
    config.addinivalue_line(
        "markers", 
        "integration: mark test as integration test spanning multiple components"
    )
    config.addinivalue_line(
        "markers", 
        "slow: mark test as slow running (>1s execution time)"
    )
    config.addinivalue_line(
        "markers", 
        "scientific: mark test as requiring scientific computing validation"
    )
    
    # Configure environment for headless testing
    os.environ.setdefault('MATPLOTLIB_BACKEND', 'Agg')
    os.environ.setdefault('PYTEST_RUNNING', 'true')
    os.environ.setdefault('HYDRA_DISABLE_LOGGING', 'true')
    
    # Disable user warnings during testing for cleaner output
    warnings.filterwarnings("ignore", category=UserWarning, module="hydra")
    warnings.filterwarnings("ignore", category=UserWarning, module="omegaconf")


def pytest_collection_modifyitems(config, items):
    """
    Modify test collection to add automatic markers and skip conditions.
    
    Automatically applies markers based on test names and dependencies,
    and skips tests when required dependencies are not available.
    """
    for item in items:
        # Auto-mark tests based on naming patterns
        if "hydra" in item.name.lower() or "config" in item.name.lower():
            item.add_marker(pytest.mark.hydra)
        
        if "cli" in item.name.lower() or "command" in item.name.lower():
            item.add_marker(pytest.mark.cli)
        
        if "database" in item.name.lower() or "session" in item.name.lower():
            item.add_marker(pytest.mark.database)
        
        if "integration" in item.nodeid or "test_integration" in item.name:
            item.add_marker(pytest.mark.integration)
        
        # Skip tests based on dependency availability
        if item.get_closest_marker("hydra") and not HYDRA_AVAILABLE:
            item.add_marker(pytest.mark.skip(reason="Hydra not available"))
        
        if item.get_closest_marker("cli") and not CLICK_TESTING_AVAILABLE:
            item.add_marker(pytest.mark.skip(reason="Click testing not available"))
        
        if item.get_closest_marker("database") and not SQLALCHEMY_AVAILABLE:
            item.add_marker(pytest.mark.skip(reason="SQLAlchemy not available"))


# ================================================================================================
# HYDRA CONFIGURATION TESTING FIXTURES
# ================================================================================================

@pytest.fixture(scope="session")
def hydra_config_store():
    """
    Initialize and provide access to Hydra ConfigStore for configuration testing.
    
    This session-scoped fixture ensures proper ConfigStore initialization and cleanup
    for hierarchical configuration composition testing across all test modules.
    
    Returns:
        ConfigStore instance for configuration schema registration
        
    Example:
        def test_config_registration(hydra_config_store):
            # Access registered configuration schemas
            assert "navigator_config" in hydra_config_store.list()
    """
    if not HYDRA_AVAILABLE:
        pytest.skip("Hydra not available for configuration testing")
    
    # Clear any existing Hydra global state
    if GlobalHydra().is_initialized():
        GlobalHydra.instance().clear()
    
    # Initialize ConfigStore
    cs = ConfigStore.instance()
    
    # Register test configuration schemas if available
    if SCHEMAS_AVAILABLE:
        cs.store(name="test_navigator_config", node=NavigatorConfig)
        cs.store(name="test_video_plume_config", node=VideoPlumeConfig)
        cs.store(name="test_single_agent_config", node=SingleAgentConfig)
        cs.store(name="test_multi_agent_config", node=MultiAgentConfig)
        cs.store(name="test_seed_config", node=SeedConfig)
    
    yield cs
    
    # Cleanup after session
    if GlobalHydra().is_initialized():
        GlobalHydra.instance().clear()


@pytest.fixture
def hydra_config_fixture(hydra_config_store, tmp_path):
    """
    Comprehensive Hydra configuration fixture for hierarchical composition testing.
    
    Provides a factory function for creating test configurations with override support,
    environment variable interpolation, and validation. Supports both simple parameter
    overrides and complex hierarchical configuration scenarios.
    
    Args:
        hydra_config_store: Session-scoped ConfigStore fixture
        tmp_path: pytest temporary directory fixture
        
    Returns:
        Factory function for creating DictConfig objects with overrides
        
    Example:
        def test_navigator_configuration(hydra_config_fixture):
            # Basic configuration with overrides
            config = hydra_config_fixture({
                "navigator": {"max_speed": 2.0, "num_agents": 3},
                "simulation": {"fps": 60}
            })
            assert config.navigator.max_speed == 2.0
            
            # Environment variable interpolation testing
            config = hydra_config_fixture({
                "video_plume": {"video_path": "${oc.env:TEST_VIDEO_PATH,default.mp4}"}
            })
    """
    if not HYDRA_AVAILABLE:
        pytest.skip("Hydra not available for configuration fixture")
    
    def _create_config(
        overrides: Optional[Dict[str, Any]] = None,
        config_name: str = "config",
        config_path: Optional[str] = None,
        env_vars: Optional[Dict[str, str]] = None
    ) -> DictConfig:
        """
        Create test configuration with overrides and environment variable support.
        
        Args:
            overrides: Configuration parameter overrides
            config_name: Base configuration name
            config_path: Configuration path (uses tmp_path if None)
            env_vars: Environment variables to set during configuration
            
        Returns:
            Configured DictConfig object
        """
        # Set up temporary environment variables
        original_env = {}
        if env_vars:
            for key, value in env_vars.items():
                original_env[key] = os.environ.get(key)
                os.environ[key] = value
        
        try:
            # Clear any existing Hydra state
            if GlobalHydra().is_initialized():
                GlobalHydra.instance().clear()
            
            # Create base configuration
            base_config = {
                "navigator": {
                    "position": [0.0, 0.0],
                    "orientation": 0.0,
                    "max_speed": 1.0,
                    "speed": 0.0
                },
                "video_plume": {
                    "video_path": str(tmp_path / "test_video.mp4"),
                    "flip": False,
                    "grayscale": True,
                    "kernel_size": 5,
                    "kernel_sigma": 1.0
                },
                "simulation": {
                    "num_steps": 100,
                    "fps": 30,
                    "max_duration": 10.0
                },
                "environment": {
                    "debug_mode": True,
                    "headless": True
                }
            }
            
            # Apply overrides
            if overrides:
                base_config = OmegaConf.merge(base_config, overrides)
            
            # Create DictConfig
            config = OmegaConf.create(base_config)
            
            # Validate configuration structure
            OmegaConf.set_struct(config, True)
            
            return config
            
        finally:
            # Restore original environment variables
            for key, original_value in original_env.items():
                if original_value is None:
                    os.environ.pop(key, None)
                else:
                    os.environ[key] = original_value
    
    return _create_config


@pytest.fixture
def hydra_override_fixture():
    """
    Fixture for testing Hydra configuration override scenarios.
    
    Provides utilities for testing command-line style parameter overrides,
    environment variable interpolation, and hierarchical configuration validation.
    
    Returns:
        Dictionary containing override testing utilities
        
    Example:
        def test_configuration_overrides(hydra_override_fixture):
            overrides = hydra_override_fixture["create_overrides"]([
                "navigator.max_speed=3.0",
                "simulation.fps=60"
            ])
            validator = hydra_override_fixture["validate_override"]
            assert validator(overrides, "navigator.max_speed", 3.0)
    """
    def create_overrides(override_list: List[str]) -> Dict[str, Any]:
        """Create override dictionary from command-line style strings."""
        overrides = {}
        for override in override_list:
            if '=' in override:
                key, value = override.split('=', 1)
                # Handle nested keys
                keys = key.split('.')
                current = overrides
                for k in keys[:-1]:
                    if k not in current:
                        current[k] = {}
                    current = current[k]
                
                # Type conversion
                if value.lower() in ('true', 'false'):
                    current[keys[-1]] = value.lower() == 'true'
                elif value.replace('.', '').replace('-', '').isdigit():
                    current[keys[-1]] = float(value) if '.' in value else int(value)
                else:
                    current[keys[-1]] = value
        
        return overrides
    
    def validate_override(config: DictConfig, key_path: str, expected_value: Any) -> bool:
        """Validate that override was applied correctly."""
        keys = key_path.split('.')
        current = config
        for key in keys:
            if key not in current:
                return False
            current = current[key]
        return current == expected_value
    
    def test_env_interpolation(config: DictConfig, env_vars: Dict[str, str]) -> bool:
        """Test environment variable interpolation in configuration."""
        # Set environment variables
        for key, value in env_vars.items():
            os.environ[key] = value
        
        try:
            # Resolve configuration
            resolved = OmegaConf.to_container(config, resolve=True)
            return resolved is not None
        except Exception:
            return False
        finally:
            # Cleanup environment variables
            for key in env_vars:
                os.environ.pop(key, None)
    
    return {
        "create_overrides": create_overrides,
        "validate_override": validate_override,
        "test_env_interpolation": test_env_interpolation
    }


# ================================================================================================
# CLI TESTING FIXTURES
# ================================================================================================

@pytest.fixture
def cli_runner_fixture():
    """
    Comprehensive CLI testing fixture using Click CliRunner.
    
    Provides isolated CliRunner instance with proper environment setup for testing
    command-line interface interactions, parameter validation, and error handling.
    Includes temporary directory management and environment variable isolation.
    
    Returns:
        Tuple of (CliRunner, environment_dict) for CLI testing
        
    Example:
        def test_cli_help_command(cli_runner_fixture):
            runner, env = cli_runner_fixture
            result = runner.invoke(main_cli, ["--help"], env=env)
            assert result.exit_code == 0
            assert "Usage:" in result.output
        
        def test_cli_with_overrides(cli_runner_fixture):
            runner, env = cli_runner_fixture
            result = runner.invoke(main_cli, [
                "run", 
                "navigator.max_speed=2.0",
                "--dry-run"
            ], env=env)
            assert result.exit_code == 0
    """
    if not CLICK_TESTING_AVAILABLE:
        pytest.skip("Click testing not available for CLI fixture")
    
    # Create isolated CliRunner
    runner = CliRunner(mix_stderr=False)
    
    # Set up isolated environment
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # Create test environment with isolated paths
        test_env = {
            'HYDRA_WORKING_DIR': str(temp_path / 'hydra'),
            'PYTEST_RUNNING': 'true',
            'MATPLOTLIB_BACKEND': 'Agg',
            'LOG_LEVEL': 'ERROR',  # Reduce logging noise in tests
            'HYDRA_DISABLE_LOGGING': 'true',
            'PYTHONPATH': os.environ.get('PYTHONPATH', ''),
            'PATH': os.environ.get('PATH', ''),
        }
        
        # Create necessary directories
        (temp_path / 'hydra').mkdir(parents=True, exist_ok=True)
        (temp_path / 'outputs').mkdir(parents=True, exist_ok=True)
        (temp_path / 'config').mkdir(parents=True, exist_ok=True)
        
        # Create minimal configuration files for CLI testing
        config_dir = temp_path / 'config'
        base_config = {
            'navigator': {'max_speed': 1.0, 'position': [0.0, 0.0]},
            'simulation': {'num_steps': 10, 'fps': 30},
            'environment': {'debug_mode': True, 'headless': True}
        }
        
        with open(config_dir / 'config.yaml', 'w') as f:
            import yaml
            yaml.dump(base_config, f)
        
        yield runner, test_env


@pytest.fixture
def cli_command_factory(cli_runner_fixture):
    """
    Factory fixture for creating and executing CLI commands with validation.
    
    Provides utilities for building CLI commands with parameters, executing them
    with proper error handling, and validating results against expected outcomes.
    
    Args:
        cli_runner_fixture: Base CLI runner fixture
        
    Returns:
        Dictionary containing CLI command utilities
        
    Example:
        def test_cli_command_execution(cli_command_factory):
            factory = cli_command_factory
            result = factory["execute_command"](
                command=["config", "validate"],
                expected_exit_code=0,
                expected_output_contains=["Configuration validation"]
            )
            assert result["success"]
    """
    runner, env = cli_runner_fixture
    
    def execute_command(
        command: List[str],
        expected_exit_code: int = 0,
        expected_output_contains: Optional[List[str]] = None,
        expected_error_contains: Optional[List[str]] = None,
        input_data: Optional[str] = None,
        timeout: float = 10.0
    ) -> Dict[str, Any]:
        """
        Execute CLI command with comprehensive validation.
        
        Args:
            command: List of command arguments
            expected_exit_code: Expected command exit code
            expected_output_contains: Strings that should be in output
            expected_error_contains: Strings that should be in error output
            input_data: Input data for interactive commands
            timeout: Command execution timeout
            
        Returns:
            Dictionary containing execution results and validation status
        """
        start_time = time.time()
        
        try:
            # Execute command
            if CLI_AVAILABLE:
                result = runner.invoke(
                    main_cli, 
                    command, 
                    input=input_data,
                    env=env,
                    catch_exceptions=False
                )
            else:
                # Mock result for testing when CLI not available
                result = Mock()
                result.exit_code = expected_exit_code
                result.output = "Mocked CLI output"
                result.stderr = ""
            
            execution_time = time.time() - start_time
            
            # Validate exit code
            exit_code_valid = result.exit_code == expected_exit_code
            
            # Validate output content
            output_valid = True
            if expected_output_contains:
                for expected in expected_output_contains:
                    if expected not in result.output:
                        output_valid = False
                        break
            
            # Validate error content
            error_valid = True
            if expected_error_contains:
                stderr_output = getattr(result, 'stderr', '') or ""
                for expected in expected_error_contains:
                    if expected not in stderr_output:
                        error_valid = False
                        break
            
            # Check performance requirements (CLI init < 2s per Section 6.6.8)
            performance_valid = execution_time < 2.0
            
            return {
                "success": exit_code_valid and output_valid and error_valid and performance_valid,
                "result": result,
                "exit_code": result.exit_code,
                "output": result.output,
                "stderr": getattr(result, 'stderr', ''),
                "execution_time": execution_time,
                "exit_code_valid": exit_code_valid,
                "output_valid": output_valid,
                "error_valid": error_valid,
                "performance_valid": performance_valid
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            return {
                "success": False,
                "result": None,
                "exit_code": -1,
                "output": "",
                "stderr": str(e),
                "execution_time": execution_time,
                "exit_code_valid": False,
                "output_valid": False,
                "error_valid": False,
                "performance_valid": execution_time < 2.0,
                "exception": e
            }
    
    def create_test_command(
        base_command: List[str],
        overrides: Optional[Dict[str, Any]] = None,
        flags: Optional[List[str]] = None
    ) -> List[str]:
        """Create CLI command with parameter overrides and flags."""
        command = base_command.copy()
        
        # Add flags
        if flags:
            command.extend(flags)
        
        # Add overrides in Hydra format
        if overrides:
            for key, value in overrides.items():
                command.append(f"{key}={value}")
        
        return command
    
    return {
        "execute_command": execute_command,
        "create_test_command": create_test_command,
        "runner": runner,
        "env": env
    }


# ================================================================================================
# DATABASE SESSION TESTING FIXTURES
# ================================================================================================

@pytest.fixture
def db_session_fixture():
    """
    In-memory SQLAlchemy database session fixture for isolated testing.
    
    Provides completely isolated database sessions using in-memory SQLite for testing
    database operations without external dependencies. Sessions are automatically
    cleaned up after test completion with proper transaction rollback.
    
    Returns:
        SQLAlchemy session object or None if database features disabled
        
    Example:
        def test_database_operations(db_session_fixture):
            session = db_session_fixture
            if session:  # Graceful handling when database not enabled
                # Perform database operations
                session.execute(text("SELECT 1"))
                # Automatic cleanup handled by fixture
            else:
                # Test file-based operations instead
                pass
    """
    if not SQLALCHEMY_AVAILABLE:
        yield None
        return
    
    # Create in-memory SQLite database
    engine = create_engine(
        "sqlite:///:memory:",
        echo=False,  # Disable SQL logging in tests
        poolclass=StaticPool,
        connect_args={
            "check_same_thread": False,
            "isolation_level": None  # Autocommit mode for testing
        }
    )
    
    # Create session factory
    SessionLocal = sessionmaker(
        bind=engine,
        autocommit=False,
        autoflush=True,
        expire_on_commit=False  # Keep objects accessible after commit
    )
    
    # Create session
    session = SessionLocal()
    
    try:
        # Validate connection
        session.execute(text("SELECT 1"))
        
        yield session
        
    except Exception as e:
        # Rollback on any error
        session.rollback()
        raise e
        
    finally:
        # Cleanup session and engine
        session.close()
        engine.dispose()


@pytest.fixture
def db_session_manager_fixture():
    """
    SessionManager fixture for testing database session management functionality.
    
    Provides access to SessionManager class for testing session lifecycle management,
    connection pooling, and database configuration scenarios.
    
    Returns:
        SessionManager instance configured for testing
        
    Example:
        def test_session_manager_lifecycle(db_session_manager_fixture):
            manager = db_session_manager_fixture
            assert manager.enabled == True  # Should be enabled for testing
            
            with manager.session() as session:
                if session:
                    session.execute(text("SELECT 1"))
    """
    if not SQLALCHEMY_AVAILABLE or not CLI_AVAILABLE:
        yield None
        return
    
    # Create test configuration
    test_config = {
        'url': 'sqlite:///:memory:',
        'enabled': True,
        'pool_size': 1,
        'max_overflow': 0,
        'echo': False,
        'autocommit': False,
        'autoflush': True,
        'expire_on_commit': False
    }
    
    # Create SessionManager instance
    manager = SessionManager.from_config(test_config)
    
    try:
        yield manager
    finally:
        # Cleanup manager resources
        manager.close()


# ================================================================================================
# SCIENTIFIC COMPUTING AND MOCKING FIXTURES
# ================================================================================================

@pytest.fixture
def mock_seed_manager():
    """
    Mock seed manager fixture for deterministic testing behavior.
    
    Provides controlled randomization for reproducible test execution across
    utilities and scientific computing components. Ensures test determinism
    while validating seed management functionality.
    
    Returns:
        Mock SeedManager instance with controlled behavior
        
    Example:
        def test_reproducible_computation(mock_seed_manager):
            # Deterministic test execution
            seed_manager = mock_seed_manager
            assert seed_manager.current_seed == 42
            
            # Test seed management functionality
            state = seed_manager.get_state()
            assert state is not None
    """
    mock_manager = Mock(spec=SeedManager)
    
    # Configure mock behavior
    mock_manager.current_seed = 42
    mock_manager.run_id = "test_run_001"
    mock_manager.environment_hash = "test_env_hash"
    mock_manager.enabled = True
    
    # Mock numpy generator
    mock_generator = Mock()
    mock_generator.random.return_value = 0.5
    mock_generator.integers.return_value = np.array([1, 2, 3, 4, 5])
    mock_manager.numpy_generator = mock_generator
    
    # Mock state management
    mock_state = {
        'python_state': ('dummy_state',),
        'numpy_legacy_state': ('dummy_numpy_state',),
        'numpy_generator_state': {'dummy': 'state'},
        'seed': 42,
        'timestamp': time.time()
    }
    mock_manager.get_state.return_value = mock_state
    mock_manager.restore_state.return_value = None
    
    # Mock initialization
    mock_manager.initialize.return_value = 42
    
    # Mock validation
    mock_manager.validate_reproducibility.return_value = True
    
    # Mock temporary seed context
    @contextmanager
    def mock_temporary_seed(seed):
        original_seed = mock_manager.current_seed
        mock_manager.current_seed = seed
        try:
            yield seed
        finally:
            mock_manager.current_seed = original_seed
    
    mock_manager.temporary_seed = mock_temporary_seed
    
    # Mock experiment seed generation
    mock_manager.generate_experiment_seeds.return_value = [42, 43, 44, 45, 46]
    
    return mock_manager


@pytest.fixture
def mock_video_capture():
    """
    Mock OpenCV VideoCapture fixture for isolated video processing testing.
    
    Provides predictable video capture behavior without requiring actual video files.
    Simulates frame sequences with configurable properties for comprehensive testing.
    
    Returns:
        Mock VideoCapture object with realistic behavior
        
    Example:
        def test_video_processing(mock_video_capture):
            cap = mock_video_capture
            assert cap.isOpened() == True
            
            ret, frame = cap.read()
            assert ret == True
            assert frame.shape == (480, 640, 3)  # Height, Width, Channels
    """
    mock_cap = Mock()
    
    # Configure basic properties
    mock_cap.isOpened.return_value = True
    mock_cap.get.side_effect = lambda prop: {
        1: 30.0,      # CAP_PROP_FPS
        3: 640.0,     # CAP_PROP_FRAME_WIDTH  
        4: 480.0,     # CAP_PROP_FRAME_HEIGHT
        7: 300.0,     # CAP_PROP_FRAME_COUNT
        0: 0.0,       # CAP_PROP_POS_FRAMES (current frame)
    }.get(prop, 0.0)
    
    # Generate predictable test frames
    def generate_test_frame(frame_number=0):
        """Generate synthetic test frame with controlled content."""
        # Create frame with gradient pattern for testing
        frame = np.zeros((480, 640, 3), dtype=np.uint8)
        
        # Add gradient pattern
        for y in range(480):
            for x in range(640):
                frame[y, x, 0] = (x + frame_number) % 256  # Red channel
                frame[y, x, 1] = (y + frame_number) % 256  # Green channel  
                frame[y, x, 2] = ((x + y + frame_number) // 2) % 256  # Blue channel
        
        return frame
    
    # Configure frame reading behavior
    frame_counter = [0]  # Use list for mutable counter
    
    def mock_read():
        """Mock read method returning synthetic frames."""
        if frame_counter[0] < 300:  # Total frame count
            frame = generate_test_frame(frame_counter[0])
            frame_counter[0] += 1
            return True, frame
        else:
            return False, None
    
    mock_cap.read = mock_read
    
    # Additional methods
    mock_cap.release.return_value = None
    mock_cap.set.return_value = True
    
    # Reset counter method for test isolation
    def reset_frame_counter():
        frame_counter[0] = 0
    
    mock_cap.reset_frame_counter = reset_frame_counter
    
    return mock_cap


@pytest.fixture
def mock_video_plume(mock_video_capture, tmp_path):
    """
    Comprehensive VideoPlume mock fixture for environment testing.
    
    Provides complete VideoPlume mock with realistic metadata, frame processing,
    and configuration integration for isolated environment testing.
    
    Args:
        mock_video_capture: Mock VideoCapture fixture
        tmp_path: pytest temporary path fixture
        
    Returns:
        Mock VideoPlume instance with comprehensive behavior
        
    Example:
        def test_plume_environment(mock_video_plume):
            plume = mock_video_plume
            assert plume.frame_count == 300
            assert plume.width == 640
            assert plume.height == 480
            
            frame = plume.get_frame(0)
            assert frame is not None
            assert isinstance(frame, np.ndarray)
    """
    mock_plume = Mock()
    
    # Configure basic properties
    mock_plume.frame_count = 300
    mock_plume.width = 640
    mock_plume.height = 480
    mock_plume.fps = 30.0
    mock_plume.video_path = tmp_path / "test_video.mp4"
    
    # Configuration properties
    mock_plume.flip = False
    mock_plume.grayscale = True
    mock_plume.kernel_size = 5
    mock_plume.kernel_sigma = 1.0
    mock_plume.threshold = None
    mock_plume.normalize = True
    
    # Mock VideoCapture integration
    mock_plume.cap = mock_video_capture
    
    def mock_get_frame(frame_index):
        """Mock get_frame method with preprocessing simulation."""
        if 0 <= frame_index < mock_plume.frame_count:
            # Get frame from mock capture
            ret, frame = mock_video_capture.read()
            if ret:
                # Simulate preprocessing
                if mock_plume.grayscale:
                    # Convert to grayscale (simplified)
                    frame = np.mean(frame, axis=2, dtype=np.uint8)
                    frame = np.expand_dims(frame, axis=2)
                
                if mock_plume.flip:
                    # Horizontal flip
                    frame = np.flip(frame, axis=1)
                
                if mock_plume.normalize:
                    # Normalize to [0, 1]
                    frame = frame.astype(np.float32) / 255.0
                
                return frame
        
        return None
    
    mock_plume.get_frame = mock_get_frame
    
    # Mock metadata methods
    def mock_get_metadata():
        """Mock metadata extraction."""
        return {
            'frame_count': mock_plume.frame_count,
            'width': mock_plume.width,
            'height': mock_plume.height,
            'fps': mock_plume.fps,
            'duration': mock_plume.frame_count / mock_plume.fps,
            'video_path': str(mock_plume.video_path),
            'preprocessing': {
                'flip': mock_plume.flip,
                'grayscale': mock_plume.grayscale,
                'kernel_size': mock_plume.kernel_size,
                'kernel_sigma': mock_plume.kernel_sigma,
                'normalize': mock_plume.normalize
            }
        }
    
    mock_plume.get_metadata = mock_get_metadata
    
    def mock_get_metadata_string():
        """Mock metadata string formatting."""
        metadata = mock_get_metadata()
        return f"""VideoPlume Metadata:
Frame Count: {metadata['frame_count']}
Dimensions: {metadata['width']}x{metadata['height']}
FPS: {metadata['fps']}
Duration: {metadata['duration']:.2f}s
Path: {metadata['video_path']}
Preprocessing: {metadata['preprocessing']}"""
    
    mock_plume.get_metadata_string = mock_get_metadata_string
    
    # Mock context manager methods
    mock_plume.__enter__ = lambda self: self
    mock_plume.__exit__ = lambda self, *args: None
    
    # Mock close method
    mock_plume.close = Mock()
    
    return mock_plume


@pytest.fixture
def mock_navigator():
    """
    Mock Navigator fixture for simulation testing.
    
    Provides predictable navigator behavior for testing simulation workflows
    without requiring complex navigation logic execution.
    
    Returns:
        Mock Navigator instance implementing NavigatorProtocol
        
    Example:
        def test_navigation_simulation(mock_navigator):
            navigator = mock_navigator
            assert navigator.num_agents == 2
            
            positions, orientations = navigator.step([0.1, 0.2])
            assert positions.shape == (2, 2)  # 2 agents, 2D positions
    """
    mock_nav = Mock()
    
    # Configure basic properties
    mock_nav.num_agents = 2
    mock_nav.positions = np.array([[0.0, 0.0], [1.0, 1.0]])
    mock_nav.orientations = np.array([0.0, 90.0])
    mock_nav.speeds = np.array([1.0, 1.0])
    mock_nav.max_speeds = np.array([2.0, 2.0])
    
    def mock_step(odor_readings):
        """Mock step method with realistic position updates."""
        # Simple movement simulation
        dt = 0.1  # Time step
        
        # Update positions based on current orientation and speed
        for i in range(mock_nav.num_agents):
            angle_rad = np.radians(mock_nav.orientations[i])
            dx = mock_nav.speeds[i] * np.cos(angle_rad) * dt
            dy = mock_nav.speeds[i] * np.sin(angle_rad) * dt
            
            mock_nav.positions[i, 0] += dx
            mock_nav.positions[i, 1] += dy
            
            # Simple orientation update based on odor gradient
            if i < len(odor_readings):
                mock_nav.orientations[i] += odor_readings[i] * 10.0  # Simple response
                mock_nav.orientations[i] %= 360.0
        
        return mock_nav.positions.copy(), mock_nav.orientations.copy()
    
    mock_nav.step = mock_step
    
    # Mock initialization method
    mock_nav.initialize = Mock()
    
    # Mock configuration access
    mock_nav.config = {
        'num_agents': mock_nav.num_agents,
        'max_speed': 2.0,
        'angular_velocity': 0.0
    }
    
    return mock_nav


# ================================================================================================
# TEMPORARY FILE AND DIRECTORY MANAGEMENT FIXTURES
# ================================================================================================

@pytest.fixture
def test_data_directory(tmp_path):
    """
    Comprehensive test data directory fixture with realistic file structure.
    
    Creates temporary directory structure with sample configuration files,
    mock video files, and test data for comprehensive integration testing.
    
    Args:
        tmp_path: pytest temporary path fixture
        
    Returns:
        Path to test data directory with structured content
        
    Example:
        def test_file_operations(test_data_directory):
            data_dir = test_data_directory
            config_file = data_dir / "conf" / "config.yaml"
            assert config_file.exists()
            
            video_file = data_dir / "videos" / "test_plume.mp4"
            assert video_file.exists()
    """
    data_dir = tmp_path / "test_data"
    data_dir.mkdir()
    
    # Create configuration directory structure
    conf_dir = data_dir / "conf"
    conf_dir.mkdir()
    
    # Create sample configuration files
    base_config = {
        'navigator': {
            'max_speed': 1.0,
            'position': [0.0, 0.0],
            'orientation': 0.0
        },
        'video_plume': {
            'video_path': '${oc.env:TEST_VIDEO_PATH,videos/test_plume.mp4}',
            'flip': False,
            'grayscale': True,
            'kernel_size': 5,
            'kernel_sigma': 1.0
        },
        'simulation': {
            'num_steps': 100,
            'fps': 30,
            'max_duration': 10.0
        }
    }
    
    import yaml
    with open(conf_dir / "config.yaml", 'w') as f:
        yaml.dump(base_config, f)
    
    with open(conf_dir / "base.yaml", 'w') as f:
        yaml.dump(base_config, f)
    
    # Create local configuration directory
    local_dir = conf_dir / "local"
    local_dir.mkdir()
    
    # Create credentials template
    credentials_template = {
        'database': {
            'url': 'sqlite:///test.db',
            'username': '${oc.env:DB_USERNAME,test_user}',
            'password': '${oc.env:DB_PASSWORD,test_pass}'
        }
    }
    
    with open(local_dir / "credentials.yaml.template", 'w') as f:
        yaml.dump(credentials_template, f)
    
    # Create video directory with mock files
    videos_dir = data_dir / "videos"
    videos_dir.mkdir()
    
    # Create minimal mock video file (empty file for path testing)
    mock_video_file = videos_dir / "test_plume.mp4"
    mock_video_file.write_bytes(b"MOCK_VIDEO_DATA")
    
    # Create outputs directory
    outputs_dir = data_dir / "outputs"
    outputs_dir.mkdir()
    
    # Create workflow directory structure
    workflow_dir = data_dir / "workflow"
    workflow_dir.mkdir()
    
    dvc_dir = workflow_dir / "dvc"
    dvc_dir.mkdir()
    
    snakemake_dir = workflow_dir / "snakemake"
    snakemake_dir.mkdir()
    
    # Create sample DVC pipeline file
    dvc_pipeline = {
        'stages': {
            'data_preparation': {
                'cmd': 'python scripts/prepare_data.py',
                'deps': ['scripts/prepare_data.py', 'data/raw/'],
                'outs': ['data/processed/']
            },
            'training': {
                'cmd': 'python scripts/train.py',
                'deps': ['scripts/train.py', 'data/processed/'],
                'outs': ['models/navigator.pkl']
            }
        }
    }
    
    with open(dvc_dir / "dvc.yaml", 'w') as f:
        yaml.dump(dvc_pipeline, f)
    
    # Create sample Snakemake workflow
    snakemake_content = """
rule all:
    input: "results/final_output.txt"

rule process_data:
    input: "data/input.txt" 
    output: "data/processed.txt"
    shell: "python scripts/process.py {input} {output}"

rule analyze_results:
    input: "data/processed.txt"
    output: "results/final_output.txt"
    shell: "python scripts/analyze.py {input} {output}"
"""
    
    with open(snakemake_dir / "Snakefile", 'w') as f:
        f.write(snakemake_content)
    
    return data_dir


@pytest.fixture
def config_files_fixture(test_data_directory):
    """
    Configuration files fixture providing validated parameter sets.
    
    Supplies various configuration scenarios for testing configuration
    validation, override mechanisms, and error handling.
    
    Args:
        test_data_directory: Test data directory fixture
        
    Returns:
        Dictionary containing configuration file paths and utilities
        
    Example:
        def test_configuration_loading(config_files_fixture):
            configs = config_files_fixture
            base_config = configs["load_config"]("base")
            assert "navigator" in base_config
            
            invalid_config = configs["create_invalid_config"]()
            # Test error handling with invalid configuration
    """
    def load_config(config_name: str) -> Dict[str, Any]:
        """Load configuration from test data directory."""
        config_file = test_data_directory / "conf" / f"{config_name}.yaml"
        if config_file.exists():
            import yaml
            with open(config_file, 'r') as f:
                return yaml.safe_load(f)
        return {}
    
    def create_invalid_config() -> Dict[str, Any]:
        """Create intentionally invalid configuration for error testing."""
        return {
            'navigator': {
                'max_speed': -1.0,  # Invalid: negative speed
                'position': [0.0],  # Invalid: wrong dimensions
                'num_agents': 0     # Invalid: zero agents
            },
            'video_plume': {
                'video_path': '/nonexistent/path.mp4',  # Invalid: missing file
                'kernel_size': 4,   # Invalid: even kernel size
                'kernel_sigma': -1.0  # Invalid: negative sigma
            }
        }
    
    def create_valid_test_configs() -> Dict[str, Dict[str, Any]]:
        """Create various valid test configurations."""
        return {
            'single_agent': {
                'navigator': {
                    'position': [0.0, 0.0],
                    'orientation': 0.0,
                    'max_speed': 2.0,
                    'speed': 1.0
                }
            },
            'multi_agent': {
                'navigator': {
                    'positions': [[0.0, 0.0], [1.0, 1.0], [2.0, 2.0]],
                    'orientations': [0.0, 90.0, 180.0],
                    'max_speeds': [2.0, 2.0, 2.0],
                    'speeds': [1.0, 1.0, 1.0],
                    'num_agents': 3
                }
            },
            'minimal': {
                'navigator': {'max_speed': 1.0},
                'simulation': {'num_steps': 10}
            }
        }
    
    def validate_config_structure(config: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """Validate configuration structure and return errors."""
        errors = []
        
        # Check required sections
        if 'navigator' not in config:
            errors.append("Missing 'navigator' section")
        
        # Validate navigator section
        if 'navigator' in config:
            nav_config = config['navigator']
            if 'max_speed' in nav_config and nav_config['max_speed'] <= 0:
                errors.append("navigator.max_speed must be positive")
            
            # Check for conflicting single/multi agent configuration
            has_position = 'position' in nav_config
            has_positions = 'positions' in nav_config
            if has_position and has_positions:
                errors.append("Cannot specify both 'position' and 'positions'")
        
        return len(errors) == 0, errors
    
    return {
        'load_config': load_config,
        'create_invalid_config': create_invalid_config,
        'create_valid_test_configs': create_valid_test_configs,
        'validate_config_structure': validate_config_structure,
        'config_dir': test_data_directory / "conf"
    }


# ================================================================================================
# INTEGRATION AND PERFORMANCE TESTING FIXTURES
# ================================================================================================

@pytest.fixture
def performance_monitor():
    """
    Performance monitoring fixture for validating timing requirements.
    
    Provides utilities for measuring and validating performance against specified
    SLA requirements including CLI initialization, database connections, and
    configuration loading times.
    
    Returns:
        Dictionary containing performance monitoring utilities
        
    Example:
        def test_cli_performance(performance_monitor):
            monitor = performance_monitor
            
            with monitor["time_operation"]("cli_init") as timer:
                # Perform CLI initialization
                pass
            
            assert timer.duration < 2.0  # CLI init < 2s requirement
    """
    
    @contextmanager
    def time_operation(operation_name: str):
        """Context manager for timing operations."""
        class Timer:
            def __init__(self, name):
                self.name = name
                self.start_time = None
                self.end_time = None
                self.duration = None
        
        timer = Timer(operation_name)
        timer.start_time = time.perf_counter()
        
        try:
            yield timer
        finally:
            timer.end_time = time.perf_counter()
            timer.duration = timer.end_time - timer.start_time
    
    def validate_performance_requirements(timings: Dict[str, float]) -> Dict[str, bool]:
        """Validate timing against performance requirements."""
        requirements = {
            'cli_init': 2.0,           # CLI command initialization < 2s
            'config_loading': 0.5,     # Configuration loading < 500ms  
            'db_connection': 0.1,      # Database connection < 100ms
            'frame_processing': 0.033, # Frame processing < 33ms (30 FPS)
            'agent_step': 0.001,       # Single agent step < 1ms
            'multi_agent_step_10': 0.005,  # 10 agents step < 5ms
            'multi_agent_step_100': 0.05,  # 100 agents step < 50ms
        }
        
        results = {}
        for operation, duration in timings.items():
            if operation in requirements:
                results[operation] = duration <= requirements[operation]
            else:
                results[operation] = True  # Unknown operations pass
        
        return results
    
    def create_performance_report(timings: Dict[str, float]) -> str:
        """Generate performance report string."""
        validation_results = validate_performance_requirements(timings)
        
        report_lines = ["Performance Report:"]
        report_lines.append("=" * 50)
        
        for operation, duration in timings.items():
            status = "✓ PASS" if validation_results.get(operation, True) else "✗ FAIL"
            report_lines.append(f"{operation:<30} {duration*1000:>8.2f}ms {status}")
        
        overall_pass = all(validation_results.values())
        status_line = "Overall: PASS" if overall_pass else "Overall: FAIL"
        report_lines.append("=" * 50)
        report_lines.append(status_line)
        
        return "\n".join(report_lines)
    
    return {
        'time_operation': time_operation,
        'validate_requirements': validate_performance_requirements,
        'create_report': create_performance_report
    }


@pytest.fixture(scope="session")
def integration_test_environment():
    """
    Session-scoped integration test environment setup.
    
    Provides comprehensive environment setup for integration tests spanning
    multiple system components including CLI, configuration, database, and
    workflow orchestration.
    
    Returns:
        Dictionary containing integration test utilities and environment
        
    Example:
        def test_full_system_integration(integration_test_environment):
            env = integration_test_environment
            
            # Access shared test environment
            config = env["get_integration_config"]()
            temp_dir = env["temp_directory"]
            
            # Perform cross-component integration testing
    """
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        
        # Create comprehensive test environment structure
        env_structure = {
            'conf': temp_path / 'conf',
            'data': temp_path / 'data',
            'outputs': temp_path / 'outputs',
            'logs': temp_path / 'logs',
            'cache': temp_path / 'cache'
        }
        
        # Create directories
        for directory in env_structure.values():
            directory.mkdir(parents=True, exist_ok=True)
        
        # Create integration test configuration
        integration_config = {
            'navigator': {
                'max_speed': 1.5,
                'position': [0.0, 0.0],
                'orientation': 0.0
            },
            'video_plume': {
                'video_path': str(env_structure['data'] / 'integration_test.mp4'),
                'flip': False,
                'grayscale': True,
                'kernel_size': 5,
                'kernel_sigma': 1.0
            },
            'simulation': {
                'num_steps': 50,
                'fps': 30,
                'max_duration': 5.0
            },
            'database': {
                'enabled': True,
                'url': f'sqlite:///{env_structure["data"]}/integration_test.db'
            },
            'environment': {
                'debug_mode': True,
                'headless': True,
                'temp_directory': str(temp_path)
            }
        }
        
        # Save integration configuration
        import yaml
        config_file = env_structure['conf'] / 'integration_config.yaml'
        with open(config_file, 'w') as f:
            yaml.dump(integration_config, f)
        
        # Create mock video file for integration testing
        mock_video = env_structure['data'] / 'integration_test.mp4'
        mock_video.write_bytes(b"INTEGRATION_TEST_VIDEO_DATA")
        
        def get_integration_config():
            """Get integration test configuration."""
            return integration_config.copy()
        
        def cleanup_integration_environment():
            """Cleanup integration test artifacts."""
            # Additional cleanup if needed
            pass
        
        yield {
            'temp_directory': temp_path,
            'directories': env_structure,
            'get_integration_config': get_integration_config,
            'cleanup': cleanup_integration_environment,
            'config_file': config_file
        }
        
        # Session cleanup handled by temporary directory context manager


# ================================================================================================
# COMPREHENSIVE FIXTURE COMBINATIONS
# ================================================================================================

@pytest.fixture
def comprehensive_test_setup(
    hydra_config_fixture,
    cli_runner_fixture, 
    db_session_fixture,
    mock_seed_manager,
    mock_video_plume,
    test_data_directory,
    performance_monitor
):
    """
    Comprehensive test setup fixture combining all testing domains.
    
    Provides unified access to all testing fixtures for comprehensive integration
    testing across CLI, configuration, database, and scientific computing domains.
    
    Args:
        hydra_config_fixture: Hydra configuration testing fixture
        cli_runner_fixture: CLI testing infrastructure
        db_session_fixture: Database session testing
        mock_seed_manager: Seed management mocking
        mock_video_plume: Video environment mocking
        test_data_directory: Test data directory structure
        performance_monitor: Performance monitoring utilities
        
    Returns:
        Dictionary containing all test infrastructure components
        
    Example:
        def test_comprehensive_system(comprehensive_test_setup):
            setup = comprehensive_test_setup
            
            # Configuration testing
            config = setup["config_factory"]({"navigator": {"max_speed": 3.0}})
            
            # CLI testing
            cli_result = setup["cli_execute"](["config", "validate"])
            
            # Database testing
            with setup["db_session"]() as session:
                if session:
                    # Database operations
                    pass
            
            # Performance validation
            with setup["performance"]["time_operation"]("full_test") as timer:
                # Comprehensive test operations
                pass
    """
    cli_runner, cli_env = cli_runner_fixture
    
    def create_integrated_config(overrides: Optional[Dict[str, Any]] = None):
        """Create configuration integrated with test environment."""
        base_overrides = {
            'environment': {
                'temp_directory': str(test_data_directory),
                'debug_mode': True,
                'headless': True
            },
            'video_plume': {
                'video_path': str(test_data_directory / 'videos' / 'test_plume.mp4')
            }
        }
        
        if overrides:
            base_overrides = {**base_overrides, **overrides}
        
        return hydra_config_fixture(base_overrides)
    
    def execute_cli_with_config(command: List[str], config_overrides: Optional[Dict[str, Any]] = None):
        """Execute CLI command with configuration integration."""
        # Create configuration if overrides provided
        if config_overrides:
            config = create_integrated_config(config_overrides)
            # Add config overrides to CLI command
            for key, value in config_overrides.items():
                command.append(f"{key}={value}")
        
        return cli_runner.invoke(main_cli if CLI_AVAILABLE else Mock(), command, env=cli_env)
    
    @contextmanager
    def managed_db_session():
        """Context manager for database session with graceful fallback."""
        if db_session_fixture:
            yield db_session_fixture
        else:
            yield None
    
    return {
        # Configuration testing
        'config_factory': create_integrated_config,
        'hydra_config': hydra_config_fixture,
        
        # CLI testing
        'cli_runner': cli_runner,
        'cli_env': cli_env,
        'cli_execute': execute_cli_with_config,
        
        # Database testing
        'db_session': managed_db_session,
        'db_session_raw': db_session_fixture,
        
        # Mocking infrastructure
        'seed_manager': mock_seed_manager,
        'video_plume': mock_video_plume,
        
        # File system and data
        'test_directory': test_data_directory,
        
        # Performance monitoring
        'performance': performance_monitor,
        
        # Integration utilities
        'create_test_environment': lambda: {
            'config': create_integrated_config(),
            'cli_runner': cli_runner,
            'db_session': db_session_fixture,
            'temp_path': test_data_directory
        }
    }