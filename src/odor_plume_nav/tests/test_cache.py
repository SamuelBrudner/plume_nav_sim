"""
Comprehensive unit test suite for FrameCache implementation.

This module provides exhaustive validation of the FrameCache class implementing LRU and 
full-preload caching strategies. Tests validate cache hit rates, memory management, 
thread safety, and eviction policies essential for maintaining sub-10ms step latency 
requirements in reinforcement learning training workflows.

Test Categories:
- FrameCache core functionality and mode switching (LRU, preload, direct I/O)
- LRU eviction policy compliance and correctness validation
- Memory boundary enforcement and intelligent pressure management
- Thread-safe concurrent access validation for multi-agent scenarios
- Cache statistics accuracy with ±1% error bounds validation
- Zero-copy frame retrieval verification ensuring NumPy array returns
- Property-based testing with Hypothesis for LRU correctness across arbitrary patterns
- Performance benchmarking ensuring sub-10ms step latency compliance

Performance Requirements:
- Cache hit rate: ≥90% for sequential access patterns (Section 0.5.1)
- Memory limit compliance: ≤2 GiB default with intelligent eviction (Section 0.5.1)
- Thread safety: Verified with concurrent access scenarios (Section 0.5.1)
- Statistics accuracy: Hit-rate calculations within ±1% error bounds (Section 6.6.5.4.2)
- Coverage requirement: ≥80% for new cache modules (Section 6.6.6.1.1)

Integration Dependencies:
- NumPy ≥1.24.0: Zero-copy frame storage and retrieval validation
- threading.RLock: Multi-agent thread safety coordination testing
- collections.OrderedDict: LRU eviction policy implementation verification
- psutil: Memory monitoring and pressure detection validation
- Hypothesis: Property-based testing for LRU invariant verification

Authors: Generated by Blitzy Platform
Version: 1.0.0
License: MIT
"""

import pytest
import numpy as np
import time
import threading
import psutil
import gc
from typing import Dict, Any, List, Tuple, Optional, Union
from unittest.mock import Mock, patch, MagicMock
from collections import OrderedDict
from contextlib import contextmanager

# Property-based testing framework
try:
    from hypothesis import given, strategies as st, assume
    HYPOTHESIS_AVAILABLE = True
except ImportError:
    HYPOTHESIS_AVAILABLE = False
    # Create mock decorators for when Hypothesis is not available
    def given(*args, **kwargs):
        def decorator(func):
            return func
        return decorator
    
    class MockStrategies:
        @staticmethod
        def integers(min_value=0, max_value=100):
            return lambda: 42
        @staticmethod
        def lists(strategy, min_size=0, max_size=10):
            return lambda: [42] * min_size
    st = MockStrategies()

# Import the cache module - will be available once created
try:
    from odor_plume_nav.cache.frame_cache import FrameCache, CacheMode
    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False
    # Create mock classes for testing infrastructure
    class CacheMode:
        NONE = "none"
        LRU = "lru" 
        ALL = "all"
    
    class FrameCache:
        def __init__(self, mode="lru", max_memory=2*1024*1024*1024):
            pass

# Test tolerance for numerical precision validation per research standards
NUMERICAL_PRECISION_TOLERANCE = 1e-6

# Performance thresholds per Section 6.6.5.4.1
CACHE_HIT_RATE_THRESHOLD = 0.90  # ≥90% hit rate requirement
CACHE_MEMORY_LIMIT_DEFAULT = 2 * 1024 * 1024 * 1024  # 2 GiB default limit
FRAME_RETRIEVAL_LATENCY_MS = 1.0  # ≤1ms from cache requirement
STEP_LATENCY_WITH_CACHE_MS = 10.0  # ≤10ms step execution with cache

# Cache statistics error tolerance per Section 6.6.5.4.2
CACHE_STATISTICS_ERROR_TOLERANCE = 0.01  # ±1% error bounds


@pytest.mark.cache
class TestFrameCacheBasicFunctionality:
    """Test core FrameCache functionality and initialization."""
    
    @pytest.fixture
    def default_cache(self):
        """Create default LRU cache for basic testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available - tests will be implemented when cache module is created")
        return FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)  # 100MB for testing
    
    @pytest.fixture
    def preload_cache(self):
        """Create full-preload cache for testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available - tests will be implemented when cache module is created")
        return FrameCache(mode=CacheMode.ALL, max_memory=200*1024*1024)  # 200MB for testing
    
    @pytest.fixture
    def direct_cache(self):
        """Create direct I/O passthrough cache for testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available - tests will be implemented when cache module is created")
        return FrameCache(mode=CacheMode.NONE)
    
    @pytest.fixture
    def sample_frame(self):
        """Generate sample video frame for testing."""
        # Create realistic video frame: 640x480 RGB (standard test resolution)
        frame = np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        return frame
    
    @pytest.fixture
    def frame_sequence(self):
        """Generate sequence of test frames."""
        frames = []
        for i in range(50):
            # Create distinctive frames with patterns for validation
            frame = np.full((480, 640, 3), i % 256, dtype=np.uint8)
            frames.append(frame)
        return frames
    
    def test_cache_initialization_with_modes(self):
        """Test FrameCache initialization with different operational modes."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Test LRU mode initialization
        lru_cache = FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)
        assert lru_cache.mode == CacheMode.LRU
        assert lru_cache.max_memory == 100*1024*1024
        assert lru_cache.hit_rate == 0.0  # Initial hit rate should be 0
        assert lru_cache.memory_usage == 0  # No frames cached initially
        
        # Test preload mode initialization
        preload_cache = FrameCache(mode=CacheMode.ALL, max_memory=200*1024*1024)
        assert preload_cache.mode == CacheMode.ALL
        assert preload_cache.max_memory == 200*1024*1024
        
        # Test direct I/O mode initialization
        direct_cache = FrameCache(mode=CacheMode.NONE)
        assert direct_cache.mode == CacheMode.NONE
        # Direct mode should not enforce memory limits
    
    def test_cache_default_parameters(self):
        """Test FrameCache initialization with default parameters."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache()
        assert cache.mode == CacheMode.LRU  # Default mode should be LRU
        assert cache.max_memory == CACHE_MEMORY_LIMIT_DEFAULT  # 2 GiB default
        assert cache.hit_rate == 0.0
        assert cache.memory_usage == 0
        assert hasattr(cache, '_lock')  # Should have thread safety lock
    
    def test_cache_parameter_validation(self):
        """Test parameter validation during cache initialization."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Test invalid mode handling
        with pytest.raises((ValueError, TypeError)):
            FrameCache(mode="invalid_mode")
        
        # Test negative memory limit
        with pytest.raises((ValueError, TypeError)):
            FrameCache(max_memory=-1)
        
        # Test zero memory limit for non-direct modes
        with pytest.raises((ValueError, TypeError)):
            FrameCache(mode=CacheMode.LRU, max_memory=0)
    
    def test_cache_basic_statistics_properties(self, default_cache):
        """Test basic cache statistics properties and initial values."""
        cache = default_cache
        
        # Test initial statistics
        assert cache.hits == 0
        assert cache.misses == 0
        assert cache.evictions == 0
        assert cache.hit_rate == 0.0
        assert cache.memory_usage == 0
        assert cache.cache_size == 0  # Number of cached frames
        
        # Test statistics type validation
        assert isinstance(cache.hits, int)
        assert isinstance(cache.misses, int)
        assert isinstance(cache.evictions, int)
        assert isinstance(cache.hit_rate, float)
        assert isinstance(cache.memory_usage, int)
        assert isinstance(cache.cache_size, int)
    
    def test_cache_mode_switching_validation(self):
        """Test cache mode switching and configuration validation."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Test that mode switching is not allowed after initialization
        cache = FrameCache(mode=CacheMode.LRU)
        
        # Mode should be read-only after initialization
        with pytest.raises((AttributeError, RuntimeError)):
            cache.mode = CacheMode.ALL
        
        # Memory limit should be configurable via method if supported
        if hasattr(cache, 'set_memory_limit'):
            cache.set_memory_limit(50*1024*1024)
            assert cache.max_memory == 50*1024*1024


@pytest.mark.cache
class TestFrameCacheLRUEviction:
    """Test LRU eviction policy implementation and correctness."""
    
    @pytest.fixture
    def small_cache(self):
        """Create small cache for eviction testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        # Small cache to force evictions - approximately 3 frames
        return FrameCache(mode=CacheMode.LRU, max_memory=3*480*640*3)
    
    @pytest.fixture
    def mock_video_source(self):
        """Mock video source for frame generation."""
        def get_frame(frame_id):
            # Generate unique frame based on ID
            frame = np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
            return frame
        return get_frame
    
    def test_lru_basic_eviction_order(self, small_cache, mock_video_source):
        """Test basic LRU eviction order compliance."""
        cache = small_cache
        
        # Cache should be able to hold approximately 3 frames
        frame_ids = [0, 1, 2, 3, 4]
        
        # Fill cache beyond capacity
        for frame_id in frame_ids:
            frame = mock_video_source(frame_id)
            
            # Mock the cache.get method to simulate frame loading
            with patch.object(cache, '_load_frame', return_value=frame):
                cached_frame = cache.get(frame_id)
                assert cached_frame is not None
        
        # Verify eviction statistics
        assert cache.evictions > 0, "Cache should have performed evictions"
        assert cache.misses == len(frame_ids), "All initial accesses should be misses"
        
        # Test LRU order: access frames in specific pattern
        # Access frame 3 and 4 to make them recently used
        with patch.object(cache, '_load_frame', return_value=mock_video_source(3)):
            cache.get(3)  # This should be a hit if frame 3 still in cache
        
        with patch.object(cache, '_load_frame', return_value=mock_video_source(4)):
            cache.get(4)  # This should be a hit if frame 4 still in cache
    
    def test_lru_eviction_with_memory_pressure(self, default_cache, frame_sequence):
        """Test LRU eviction under memory pressure conditions."""
        cache = default_cache
        
        # Calculate frame size for memory management
        frame_size = frame_sequence[0].nbytes
        max_frames = cache.max_memory // frame_size
        
        # Fill cache beyond capacity to trigger evictions
        with patch.object(cache, '_load_frame') as mock_load:
            mock_load.side_effect = lambda fid: frame_sequence[fid % len(frame_sequence)]
            
            for i in range(max_frames + 10):  # Exceed capacity
                cache.get(i)
                
                # Verify memory limit compliance
                assert cache.memory_usage <= cache.max_memory, \
                    f"Cache memory usage {cache.memory_usage} exceeded limit {cache.max_memory}"
                
                # Verify eviction occurred when memory pressure exists
                if cache.memory_usage >= cache.max_memory * 0.9:  # 90% threshold
                    assert cache.evictions > 0, "Evictions should occur under memory pressure"
    
    def test_lru_access_pattern_updates(self, default_cache, mock_video_source):
        """Test that LRU access patterns correctly update frame recency."""
        cache = default_cache
        
        frame_ids = [1, 2, 3, 4, 5]
        
        # Initial frame loading
        with patch.object(cache, '_load_frame') as mock_load:
            mock_load.side_effect = lambda fid: mock_video_source(fid)
            
            # Load all frames
            for frame_id in frame_ids:
                cache.get(frame_id)
            
            # Re-access frame 1 to make it most recently used
            original_hits = cache.hits
            cache.get(1)  # Should be a cache hit
            
            # Verify hit was recorded
            assert cache.hits == original_hits + 1, "Frame re-access should register as hit"
            
            # Access new frame to potentially trigger eviction
            cache.get(6)
            
            # Frame 1 should still be in cache (most recently used)
            hits_before = cache.hits
            cache.get(1)
            assert cache.hits == hits_before + 1, "Recently accessed frame should remain cached"
    
    @pytest.mark.skipif(not HYPOTHESIS_AVAILABLE, reason="Hypothesis not available")
    @given(st.lists(st.integers(min_value=0, max_value=99), min_size=10, max_size=200))
    def test_lru_eviction_invariants_property_based(self, access_sequence):
        """Property-based test for LRU eviction correctness across arbitrary access patterns."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Create cache with known capacity
        cache = FrameCache(mode=CacheMode.LRU, max_memory=10*480*640*3)  # ~10 frames
        
        accessed_frames = []
        frame_access_order = []
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            for frame_id in access_sequence:
                cache.get(frame_id)
                accessed_frames.append(frame_id)
                
                # Track access order for LRU validation
                if frame_id in frame_access_order:
                    frame_access_order.remove(frame_id)
                frame_access_order.append(frame_id)
                
                # Invariant: Cache never exceeds memory limit
                assert cache.memory_usage <= cache.max_memory, \
                    f"Cache exceeded memory limit: {cache.memory_usage} > {cache.max_memory}"
                
                # Invariant: Statistics remain consistent
                assert cache.hits + cache.misses == len(accessed_frames), \
                    "Hit + miss count should equal total accesses"
                
                # Invariant: Hit rate is within valid range
                assert 0.0 <= cache.hit_rate <= 1.0, \
                    f"Hit rate {cache.hit_rate} outside valid range [0.0, 1.0]"
    
    def test_lru_eviction_with_repeated_patterns(self, default_cache):
        """Test LRU behavior with repeated access patterns."""
        cache = default_cache
        
        # Create cyclical access pattern to test working set behavior
        pattern = [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            initial_misses = cache.misses
            
            # First cycle - all misses
            for frame_id in pattern[:5]:
                cache.get(frame_id)
            
            first_cycle_misses = cache.misses - initial_misses
            
            # Second cycle - should have some hits if cache size sufficient
            for frame_id in pattern[5:]:
                cache.get(frame_id)
            
            second_cycle_misses = cache.misses - initial_misses - first_cycle_misses
            
            # Verify improved hit rate in second cycle
            if cache.memory_usage < cache.max_memory:
                assert second_cycle_misses < first_cycle_misses, \
                    "Second cycle should have fewer misses due to caching"


@pytest.mark.cache  
class TestFrameCacheMemoryManagement:
    """Test memory boundary compliance and intelligent eviction policies."""
    
    def test_memory_limit_enforcement(self):
        """Test strict memory limit enforcement under load."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Create cache with strict memory limit
        memory_limit = 50 * 1024 * 1024  # 50MB
        cache = FrameCache(mode=CacheMode.LRU, max_memory=memory_limit)
        
        def mock_large_frame(frame_id):
            # Create larger frames to quickly reach memory limit
            return np.random.randint(0, 256, (720, 1280, 3), dtype=np.uint8)  # HD frames
        
        with patch.object(cache, '_load_frame', side_effect=mock_large_frame):
            # Load frames until memory pressure
            for i in range(20):  # Load many frames
                cache.get(i)
                
                # Critical invariant: never exceed memory limit
                assert cache.memory_usage <= memory_limit, \
                    f"Memory usage {cache.memory_usage} exceeded limit {memory_limit}"
                
                # Verify evictions occur when approaching limit
                if cache.memory_usage >= memory_limit * 0.8:  # 80% threshold
                    assert cache.evictions > 0, "Evictions should occur near memory limit"
    
    def test_memory_pressure_detection(self, default_cache):
        """Test memory pressure detection and response."""
        cache = default_cache
        
        # Mock psutil to simulate memory pressure
        with patch('psutil.virtual_memory') as mock_memory:
            mock_memory.return_value.percent = 85.0  # High memory usage
            
            def mock_load_frame(frame_id):
                return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
            
            with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                # Load frames under simulated memory pressure
                for i in range(10):
                    cache.get(i)
                
                # Cache should respond to system memory pressure
                # Implementation-specific behavior validation
                if hasattr(cache, '_memory_pressure_threshold'):
                    assert cache._memory_pressure_threshold < 1.0, \
                        "Cache should lower memory threshold under pressure"
    
    def test_memory_cleanup_on_eviction(self, default_cache):
        """Test proper memory cleanup during frame eviction."""
        cache = default_cache
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Fill cache to capacity
            initial_memory = cache.memory_usage
            
            # Load enough frames to trigger evictions
            frame_count = 0
            while cache.evictions == 0 and frame_count < 100:
                cache.get(frame_count)
                frame_count += 1
            
            # Verify evictions occurred
            assert cache.evictions > 0, "Evictions should have occurred"
            
            # Memory usage should remain bounded
            assert cache.memory_usage <= cache.max_memory, \
                "Memory should be bounded after evictions"
            
            # Trigger garbage collection to verify cleanup
            gc.collect()
            
            # Cache should maintain consistent state after GC
            assert cache.memory_usage <= cache.max_memory
            assert cache.cache_size >= 0
    
    def test_memory_fragmentation_prevention(self):
        """Test prevention of memory fragmentation in cache operations."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)
        
        def mock_variable_frame(frame_id):
            # Create frames of varying sizes to test fragmentation
            sizes = [(240, 320, 3), (480, 640, 3), (720, 1280, 3)]
            size = sizes[frame_id % len(sizes)]
            return np.random.randint(0, 256, size, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_variable_frame):
            # Load frames with varying sizes
            for i in range(30):
                cache.get(i)
                
                # Memory usage should remain efficient
                assert cache.memory_usage <= cache.max_memory
                
                # Cache should maintain reasonable utilization
                if cache.cache_size > 0:
                    avg_frame_size = cache.memory_usage / cache.cache_size
                    assert avg_frame_size > 0, "Average frame size should be positive"
    
    @given(st.integers(min_value=1, max_value=500))
    @pytest.mark.skipif(not HYPOTHESIS_AVAILABLE, reason="Hypothesis not available")
    def test_cache_memory_boundary_compliance_property(self, cache_size_mb):
        """Property-based test for memory boundary compliance under variable sizing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        assume(cache_size_mb > 0)
        max_memory = cache_size_mb * 1024 * 1024
        cache = FrameCache(mode=CacheMode.LRU, max_memory=max_memory)
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Attempt to exceed memory limit
            for i in range(cache_size_mb * 2):  # 2x expected capacity
                cache.get(i)
                
                # Invariant: Memory usage never exceeds configured limit
                assert cache.memory_usage <= max_memory, \
                    f"Cache exceeded memory limit: {cache.memory_usage} > {max_memory}"
                
                # Invariant: Cache maintains functionality under memory pressure
                assert cache.cache_size >= 0, "Cache size should remain non-negative"
                assert cache.hit_rate >= 0.0, "Hit rate should remain non-negative"


@pytest.mark.cache
class TestFrameCacheThreadSafety:
    """Test thread-safe concurrent access validation for multi-agent scenarios."""
    
    @pytest.fixture
    def thread_safe_cache(self):
        """Create cache configured for thread safety testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        return FrameCache(mode=CacheMode.LRU, max_memory=200*1024*1024)
    
    def test_concurrent_cache_access_basic(self, thread_safe_cache):
        """Test basic concurrent access from multiple threads."""
        cache = thread_safe_cache
        results = {}
        errors = []
        
        def mock_load_frame(frame_id):
            # Simulate frame loading with small delay
            time.sleep(0.001)  # 1ms delay
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        def worker_thread(thread_id, frame_ids):
            """Worker thread for concurrent cache access."""
            thread_results = []
            try:
                with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                    for frame_id in frame_ids:
                        frame = cache.get(frame_id)
                        thread_results.append((frame_id, frame is not None))
                        
                        # Verify thread safety of statistics
                        hit_rate = cache.hit_rate
                        assert 0.0 <= hit_rate <= 1.0, f"Invalid hit rate: {hit_rate}"
                        
                results[thread_id] = thread_results
            except Exception as e:
                errors.append(f"Thread {thread_id}: {e}")
        
        # Create multiple threads accessing overlapping frame ranges
        threads = []
        for i in range(4):  # 4 concurrent threads
            frame_ids = list(range(i*5, (i+1)*5 + 5))  # Overlapping ranges
            thread = threading.Thread(target=worker_thread, args=(i, frame_ids))
            threads.append(thread)
        
        # Start all threads
        for thread in threads:
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join(timeout=10.0)  # 10 second timeout
        
        # Verify no errors occurred
        assert len(errors) == 0, f"Thread safety errors: {errors}"
        
        # Verify all threads completed successfully
        assert len(results) == 4, "All threads should have completed"
        
        # Verify cache statistics integrity
        assert cache.hits + cache.misses > 0, "Cache should have recorded accesses"
        assert 0.0 <= cache.hit_rate <= 1.0, "Hit rate should be in valid range"
    
    def test_concurrent_memory_pressure_handling(self, thread_safe_cache):
        """Test thread safety under concurrent memory pressure scenarios."""
        cache = thread_safe_cache
        memory_errors = []
        
        def mock_large_frame(frame_id):
            # Create large frames to induce memory pressure
            return np.random.randint(0, 256, (720, 1280, 3), dtype=np.uint8)
        
        def memory_pressure_worker(thread_id, num_frames):
            """Worker that creates memory pressure."""
            try:
                with patch.object(cache, '_load_frame', side_effect=mock_large_frame):
                    for i in range(num_frames):
                        frame_id = thread_id * 1000 + i  # Unique frame IDs per thread
                        cache.get(frame_id)
                        
                        # Verify memory limit compliance under pressure
                        if cache.memory_usage > cache.max_memory:
                            memory_errors.append(f"Thread {thread_id}: Memory limit exceeded")
                            break
            except Exception as e:
                memory_errors.append(f"Thread {thread_id}: {e}")
        
        # Create threads that will cause memory pressure
        threads = []
        for i in range(3):
            thread = threading.Thread(
                target=memory_pressure_worker, 
                args=(i, 20)  # Each thread loads 20 large frames
            )
            threads.append(thread)
        
        # Start threads
        for thread in threads:
            thread.start()
        
        # Wait for completion
        for thread in threads:
            thread.join(timeout=15.0)
        
        # Verify no memory violations occurred
        assert len(memory_errors) == 0, f"Memory safety errors: {memory_errors}"
        
        # Verify cache remained functional
        assert cache.memory_usage <= cache.max_memory, "Memory limit should be maintained"
        assert cache.cache_size >= 0, "Cache should remain in valid state"
    
    def test_concurrent_statistics_consistency(self, thread_safe_cache):
        """Test consistency of cache statistics under concurrent access."""
        cache = thread_safe_cache
        statistics_samples = []
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        def statistics_monitor():
            """Monitor cache statistics for consistency."""
            with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                for _ in range(100):  # Sample statistics 100 times
                    stats = {
                        'hits': cache.hits,
                        'misses': cache.misses,
                        'evictions': cache.evictions,
                        'hit_rate': cache.hit_rate,
                        'memory_usage': cache.memory_usage,
                        'cache_size': cache.cache_size
                    }
                    statistics_samples.append(stats)
                    time.sleep(0.01)  # 10ms sampling interval
        
        def cache_accessor(frame_range):
            """Continuously access cache frames."""
            with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                for _ in range(50):  # Multiple access cycles
                    for frame_id in frame_range:
                        cache.get(frame_id)
                    time.sleep(0.005)  # 5ms between cycles
        
        # Start monitoring thread
        monitor_thread = threading.Thread(target=statistics_monitor)
        monitor_thread.start()
        
        # Start multiple accessor threads
        accessor_threads = []
        for i in range(3):
            frame_range = list(range(i*10, (i+1)*10))
            thread = threading.Thread(target=cache_accessor, args=(frame_range,))
            accessor_threads.append(thread)
            thread.start()
        
        # Wait for all threads
        for thread in accessor_threads:
            thread.join(timeout=10.0)
        
        monitor_thread.join(timeout=2.0)
        
        # Verify statistics consistency
        assert len(statistics_samples) > 0, "Should have collected statistics samples"
        
        for stats in statistics_samples:
            # Verify basic invariants
            assert stats['hits'] >= 0, "Hits should be non-negative"
            assert stats['misses'] >= 0, "Misses should be non-negative"
            assert stats['evictions'] >= 0, "Evictions should be non-negative"
            assert 0.0 <= stats['hit_rate'] <= 1.0, "Hit rate should be in [0,1]"
            assert stats['memory_usage'] >= 0, "Memory usage should be non-negative"
            assert stats['cache_size'] >= 0, "Cache size should be non-negative"
            
            # Verify statistics relationships
            total_accesses = stats['hits'] + stats['misses']
            if total_accesses > 0:
                expected_hit_rate = stats['hits'] / total_accesses
                actual_hit_rate = stats['hit_rate']
                
                # Allow for small floating point differences
                assert abs(actual_hit_rate - expected_hit_rate) < 0.001, \
                    f"Hit rate inconsistency: {actual_hit_rate} vs {expected_hit_rate}"
    
    def test_deadlock_prevention(self, thread_safe_cache):
        """Test deadlock prevention in concurrent cache operations."""
        cache = thread_safe_cache
        completion_flags = [False] * 4
        
        def mock_load_frame(frame_id):
            # Simulate varying load times
            time.sleep(0.001 * (frame_id % 5))
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        def complex_cache_operations(thread_id):
            """Perform complex cache operations that could cause deadlocks."""
            try:
                with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                    for cycle in range(10):
                        # Mix of operations that could cause lock contention
                        frame_ids = [thread_id * 100 + cycle * 10 + i for i in range(5)]
                        
                        # Access frames in forward order
                        for frame_id in frame_ids:
                            cache.get(frame_id)
                        
                        # Access frames in reverse order
                        for frame_id in reversed(frame_ids):
                            cache.get(frame_id)
                        
                        # Check cache statistics (may acquire locks)
                        _ = cache.hit_rate
                        _ = cache.memory_usage
                        
                        # Small delay to increase contention probability
                        time.sleep(0.001)
                
                completion_flags[thread_id] = True
                
            except Exception as e:
                pytest.fail(f"Thread {thread_id} failed with: {e}")
        
        # Start multiple threads with complex operations
        threads = []
        for i in range(4):
            thread = threading.Thread(target=complex_cache_operations, args=(i,))
            threads.append(thread)
            thread.start()
        
        # Wait for completion with timeout to detect deadlocks
        start_time = time.time()
        for i, thread in enumerate(threads):
            remaining_time = max(0, 20.0 - (time.time() - start_time))
            thread.join(timeout=remaining_time)
            
            if thread.is_alive():
                pytest.fail(f"Thread {i} did not complete - possible deadlock")
        
        # Verify all threads completed successfully
        assert all(completion_flags), "All threads should have completed successfully"
        
        # Verify cache remained in consistent state
        assert cache.memory_usage <= cache.max_memory
        assert cache.hits + cache.misses > 0  # Some operations should have occurred


@pytest.mark.cache
class TestFrameCacheStatistics:
    """Test cache statistics accuracy and consistency validation."""
    
    @pytest.fixture
    def statistics_cache(self):
        """Create cache for statistics validation testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        return FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)
    
    def test_hit_rate_calculation_accuracy(self, statistics_cache):
        """Test hit rate calculation accuracy within ±1% error bounds."""
        cache = statistics_cache
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Controlled access pattern for precise hit rate validation
            frame_ids = [1, 2, 3, 1, 2, 3, 1, 2, 3]  # 3 unique frames, 6 repeat accesses
            
            for frame_id in frame_ids:
                cache.get(frame_id)
            
            # Expected: 3 misses (first access), 6 hits (repeat accesses)
            expected_hits = 6
            expected_misses = 3
            expected_hit_rate = expected_hits / (expected_hits + expected_misses)  # 6/9 = 0.667
            
            # Verify statistics with tolerance
            assert cache.hits == expected_hits, f"Expected {expected_hits} hits, got {cache.hits}"
            assert cache.misses == expected_misses, f"Expected {expected_misses} misses, got {cache.misses}"
            
            # Verify hit rate accuracy within ±1% error bounds
            hit_rate_error = abs(cache.hit_rate - expected_hit_rate)
            assert hit_rate_error <= CACHE_STATISTICS_ERROR_TOLERANCE, \
                f"Hit rate error {hit_rate_error} exceeds tolerance {CACHE_STATISTICS_ERROR_TOLERANCE}"
    
    def test_statistics_consistency_under_eviction(self, statistics_cache):
        """Test statistics remain consistent during cache evictions."""
        cache = statistics_cache
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Fill cache beyond capacity to force evictions
            frame_count = 0
            previous_stats = None
            
            while frame_count < 50:  # Load many frames
                cache.get(frame_count)
                
                # Capture current statistics
                current_stats = {
                    'hits': cache.hits,
                    'misses': cache.misses,
                    'evictions': cache.evictions,
                    'hit_rate': cache.hit_rate
                }
                
                # Verify statistics monotonicity and consistency
                if previous_stats:
                    # Hits and misses should only increase
                    assert current_stats['hits'] >= previous_stats['hits'], \
                        "Hits should be monotonically non-decreasing"
                    assert current_stats['misses'] >= previous_stats['misses'], \
                        "Misses should be monotonically non-decreasing"
                    assert current_stats['evictions'] >= previous_stats['evictions'], \
                        "Evictions should be monotonically non-decreasing"
                    
                    # Hit rate should remain within valid bounds
                    assert 0.0 <= current_stats['hit_rate'] <= 1.0, \
                        f"Hit rate {current_stats['hit_rate']} outside valid bounds"
                    
                    # Verify hit rate calculation consistency
                    total_accesses = current_stats['hits'] + current_stats['misses']
                    if total_accesses > 0:
                        calculated_hit_rate = current_stats['hits'] / total_accesses
                        rate_difference = abs(current_stats['hit_rate'] - calculated_hit_rate)
                        assert rate_difference <= CACHE_STATISTICS_ERROR_TOLERANCE, \
                            f"Hit rate calculation inconsistency: {rate_difference}"
                
                previous_stats = current_stats
                frame_count += 1
    
    def test_memory_usage_statistics_accuracy(self, statistics_cache):
        """Test memory usage statistics accuracy and consistency."""
        cache = statistics_cache
        
        def mock_fixed_frame(frame_id):
            # Fixed size frames for precise memory calculations
            return np.zeros((100, 100, 3), dtype=np.uint8)  # 30KB frames
        
        with patch.object(cache, '_load_frame', side_effect=mock_fixed_frame):
            frame_size = 100 * 100 * 3  # 30KB per frame
            
            # Load frames and verify memory tracking
            for i in range(5):
                cache.get(i)
                
                # Verify memory usage tracking
                expected_memory = min((i + 1) * frame_size, cache.max_memory)
                
                # Allow for metadata overhead in memory calculation
                memory_tolerance = frame_size * 0.1  # 10% tolerance for metadata
                memory_difference = abs(cache.memory_usage - expected_memory)
                
                assert memory_difference <= memory_tolerance, \
                    f"Memory usage {cache.memory_usage} differs from expected {expected_memory}"
                
                # Verify cache size consistency
                expected_cache_size = min(i + 1, cache.max_memory // frame_size)
                assert abs(cache.cache_size - expected_cache_size) <= 1, \
                    f"Cache size {cache.cache_size} inconsistent with expected {expected_cache_size}"
    
    def test_eviction_statistics_precision(self, statistics_cache):
        """Test eviction statistics tracking precision."""
        cache = statistics_cache
        
        # Calculate approximate cache capacity
        frame_size = 480 * 640 * 3
        approximate_capacity = cache.max_memory // frame_size
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Load frames beyond capacity to trigger evictions
            frames_to_load = approximate_capacity + 10
            
            for i in range(frames_to_load):
                cache.get(i)
                
                # Verify eviction counting precision
                if cache.evictions > 0:
                    # Once evictions start, they should be tracked accurately
                    previous_evictions = cache.evictions
                    
                    # Load one more frame to trigger potential eviction
                    cache.get(frames_to_load + i)
                    
                    # Evictions should increase by 0 or 1 (no more than 1 per frame load)
                    eviction_increase = cache.evictions - previous_evictions
                    assert 0 <= eviction_increase <= 1, \
                        f"Eviction count increased by {eviction_increase}, should be 0 or 1"
                    
                    break
    
    def test_statistics_reset_functionality(self, statistics_cache):
        """Test statistics reset functionality if available."""
        cache = statistics_cache
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Generate some cache activity
            for i in range(10):
                cache.get(i % 5)  # Mix of hits and misses
            
            # Verify statistics exist
            assert cache.hits > 0 or cache.misses > 0, "Should have some cache activity"
            
            # Test statistics reset if method exists
            if hasattr(cache, 'reset_statistics'):
                cache.reset_statistics()
                
                # Verify reset worked
                assert cache.hits == 0, "Hits should be reset to 0"
                assert cache.misses == 0, "Misses should be reset to 0"
                assert cache.evictions == 0, "Evictions should be reset to 0"
                assert cache.hit_rate == 0.0, "Hit rate should be reset to 0.0"
                
                # Cache content should remain intact
                assert cache.cache_size >= 0, "Cache content should remain"
                assert cache.memory_usage >= 0, "Memory usage should remain valid"
    
    @given(st.lists(st.integers(min_value=0, max_value=20), min_size=10, max_size=100))
    @pytest.mark.skipif(not HYPOTHESIS_AVAILABLE, reason="Hypothesis not available")
    def test_hit_rate_statistics_property_based(self, access_sequence):
        """Property-based test for hit rate statistics accuracy."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache(mode=CacheMode.LRU, max_memory=50*1024*1024)
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            access_count = {}
            total_accesses = 0
            
            for frame_id in access_sequence:
                cache.get(frame_id)
                access_count[frame_id] = access_count.get(frame_id, 0) + 1
                total_accesses += 1
                
                # Verify hit rate bounds
                assert 0.0 <= cache.hit_rate <= 1.0, \
                    f"Hit rate {cache.hit_rate} outside bounds [0.0, 1.0]"
                
                # Verify access counting
                assert cache.hits + cache.misses == total_accesses, \
                    "Total hits + misses should equal total accesses"
                
                # Verify hit rate calculation
                if total_accesses > 0:
                    calculated_hit_rate = cache.hits / total_accesses
                    assert abs(cache.hit_rate - calculated_hit_rate) <= CACHE_STATISTICS_ERROR_TOLERANCE, \
                        f"Hit rate calculation error: {abs(cache.hit_rate - calculated_hit_rate)}"


@pytest.mark.cache
class TestFrameCacheZeroCopyRetrieval:
    """Test zero-copy frame retrieval ensuring NumPy array returns without copies."""
    
    @pytest.fixture
    def zero_copy_cache(self):
        """Create cache for zero-copy testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        return FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)
    
    def test_zero_copy_frame_return(self, zero_copy_cache, sample_frame):
        """Test that cache returns frames without unnecessary copying."""
        cache = zero_copy_cache
        
        def mock_load_frame(frame_id):
            # Return the sample frame for consistent testing
            return sample_frame
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # First access - cache miss, frame should be stored
            frame1 = cache.get(0)
            
            # Verify frame properties
            assert isinstance(frame1, np.ndarray), "Should return NumPy array"
            assert frame1.shape == sample_frame.shape, "Shape should match original"
            assert frame1.dtype == sample_frame.dtype, "Dtype should match original"
            
            # Second access - cache hit, should return same data
            frame2 = cache.get(0)
            
            # Verify data integrity
            np.testing.assert_array_equal(frame1, frame2, "Cached frame should match original")
            
            # Test zero-copy behavior if possible
            if hasattr(cache, '_frames'):  # Implementation-specific
                # Frames should reference same underlying data if zero-copy implemented
                # This is implementation dependent and may not be directly testable
                pass
    
    def test_frame_immutability_protection(self, zero_copy_cache, sample_frame):
        """Test that returned frames are protected from external modification."""
        cache = zero_copy_cache
        
        def mock_load_frame(frame_id):
            return sample_frame.copy()  # Return copy to allow modification testing
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Get frame from cache
            cached_frame = cache.get(0)
            original_value = cached_frame[0, 0, 0]
            
            # Attempt to modify returned frame
            try:
                cached_frame[0, 0, 0] = 255  # Try to modify
                
                # Get frame again to check if cache was affected
                frame_again = cache.get(0)
                
                if hasattr(cache, '_immutable_returns') and cache._immutable_returns:
                    # If cache implements immutable returns, original should be preserved
                    assert frame_again[0, 0, 0] == original_value, \
                        "Cache should protect against external modifications"
                else:
                    # If not implementing immutable returns, document the behavior
                    pass
                    
            except (ValueError, RuntimeError):
                # Cache may return read-only views to prevent modification
                pass
    
    def test_memory_efficient_frame_storage(self, zero_copy_cache):
        """Test memory-efficient frame storage without unnecessary duplication."""
        cache = zero_copy_cache
        
        # Create distinctive frames for testing
        def create_test_frame(frame_id):
            frame = np.zeros((100, 100, 3), dtype=np.uint8)
            frame.fill(frame_id % 256)  # Fill with distinctive value
            return frame
        
        def mock_load_frame(frame_id):
            return create_test_frame(frame_id)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Load several frames
            frames = []
            for i in range(5):
                frame = cache.get(i)
                frames.append(frame)
                
                # Verify each frame is distinctive
                expected_value = i % 256
                assert np.all(frame == expected_value), \
                    f"Frame {i} should be filled with value {expected_value}"
            
            # Verify memory efficiency - each frame should be stored once
            total_expected_memory = len(frames) * frames[0].nbytes
            
            # Allow for reasonable metadata overhead
            memory_tolerance = total_expected_memory * 0.2  # 20% tolerance
            assert cache.memory_usage <= total_expected_memory + memory_tolerance, \
                f"Memory usage {cache.memory_usage} suggests inefficient storage"
    
    def test_numpy_array_properties_preservation(self, zero_copy_cache):
        """Test that NumPy array properties are preserved through caching."""
        cache = zero_copy_cache
        
        # Test various NumPy array configurations
        test_arrays = [
            np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8),  # Standard RGB
            np.random.rand(240, 320, 1).astype(np.float32),           # Grayscale float
            np.random.randint(0, 65536, (360, 480, 3), dtype=np.uint16), # 16-bit
        ]
        
        def mock_load_frame(frame_id):
            return test_arrays[frame_id % len(test_arrays)]
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            for i, original_array in enumerate(test_arrays):
                cached_array = cache.get(i)
                
                # Verify array properties preservation
                assert cached_array.shape == original_array.shape, \
                    f"Shape mismatch for array {i}: {cached_array.shape} vs {original_array.shape}"
                assert cached_array.dtype == original_array.dtype, \
                    f"Dtype mismatch for array {i}: {cached_array.dtype} vs {original_array.dtype}"
                assert cached_array.ndim == original_array.ndim, \
                    f"Ndim mismatch for array {i}: {cached_array.ndim} vs {original_array.ndim}"
                
                # Verify data preservation
                np.testing.assert_array_equal(cached_array, original_array, 
                    f"Data mismatch for array {i}")
                
                # Verify contiguity if original was contiguous
                if original_array.flags.c_contiguous:
                    assert cached_array.flags.c_contiguous, \
                        f"C-contiguity lost for array {i}"
    
    def test_frame_view_vs_copy_behavior(self, zero_copy_cache, sample_frame):
        """Test whether cache returns views or copies of frames."""
        cache = zero_copy_cache
        
        def mock_load_frame(frame_id):
            return sample_frame
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Get frame multiple times
            frame1 = cache.get(0)
            frame2 = cache.get(0)
            
            # Test if frames share memory (view behavior)
            shares_memory = np.shares_memory(frame1, frame2)
            
            if shares_memory:
                # Cache implements view-based zero-copy
                assert frame1.base is not None or frame2.base is not None, \
                    "Views should have base reference"
            else:
                # Cache returns copies for safety
                # Verify copies are correct
                np.testing.assert_array_equal(frame1, frame2, 
                    "Copied frames should have identical data")
            
            # Regardless of implementation, data should be identical
            np.testing.assert_array_equal(frame1, frame2)
            assert frame1.shape == frame2.shape
            assert frame1.dtype == frame2.dtype


@pytest.mark.cache
class TestFrameCachePerformanceBenchmarks:
    """Test performance benchmarks ensuring sub-10ms step latency compliance."""
    
    @pytest.fixture
    def performance_cache(self):
        """Create cache optimized for performance testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        return FrameCache(mode=CacheMode.LRU, max_memory=500*1024*1024)  # 500MB for performance
    
    @contextmanager
    def timer_context(self):
        """High-precision timing context manager."""
        start_time = time.perf_counter()
        try:
            yield
        finally:
            end_time = time.perf_counter()
            self.last_execution_time = end_time - start_time
    
    def test_cache_hit_latency_performance(self, performance_cache, sample_frame):
        """Test cache hit latency meets ≤1ms requirement."""
        cache = performance_cache
        
        def mock_load_frame(frame_id):
            return sample_frame
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Pre-populate cache
            cache.get(0)  # Initial miss to populate cache
            
            # Warm up cache access
            for _ in range(10):
                cache.get(0)
            
            # Measure cache hit performance
            hit_times = []
            for _ in range(100):  # Multiple measurements for statistical validity
                with self.timer_context():
                    frame = cache.get(0)  # Should be cache hit
                    assert frame is not None
                
                hit_times.append(self.last_execution_time * 1000)  # Convert to ms
            
            # Verify performance requirements
            avg_hit_time = np.mean(hit_times)
            max_hit_time = np.max(hit_times)
            
            assert avg_hit_time <= FRAME_RETRIEVAL_LATENCY_MS, \
                f"Average cache hit time {avg_hit_time:.3f}ms exceeds {FRAME_RETRIEVAL_LATENCY_MS}ms limit"
            
            assert max_hit_time <= FRAME_RETRIEVAL_LATENCY_MS * 2, \
                f"Maximum cache hit time {max_hit_time:.3f}ms exceeds reasonable bound"
            
            # Verify 95th percentile performance
            p95_hit_time = np.percentile(hit_times, 95)
            assert p95_hit_time <= FRAME_RETRIEVAL_LATENCY_MS * 1.5, \
                f"95th percentile hit time {p95_hit_time:.3f}ms exceeds performance target"
    
    def test_cache_miss_latency_bounds(self, performance_cache):
        """Test cache miss latency remains reasonable."""
        cache = performance_cache
        
        def mock_load_frame(frame_id):
            # Simulate realistic frame loading time
            time.sleep(0.001)  # 1ms simulated I/O
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Measure cache miss performance
            miss_times = []
            for i in range(20):  # Unique frames to ensure misses
                with self.timer_context():
                    frame = cache.get(i)
                    assert frame is not None
                
                miss_times.append(self.last_execution_time * 1000)  # Convert to ms
            
            # Cache miss should include I/O time but remain reasonable
            avg_miss_time = np.mean(miss_times)
            max_miss_time = np.max(miss_times)
            
            # Miss time should be dominated by I/O (1ms) plus cache overhead
            assert avg_miss_time >= 1.0, "Miss time should include I/O latency"
            assert avg_miss_time <= 5.0, f"Average miss time {avg_miss_time:.3f}ms too high"
            assert max_miss_time <= 10.0, f"Maximum miss time {max_miss_time:.3f}ms too high"
    
    def test_sequential_access_hit_rate_performance(self, performance_cache):
        """Test cache achieves ≥90% hit rate for sequential access patterns."""
        cache = performance_cache
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Sequential access pattern - common in video processing
            sequence_length = 100
            working_set_size = 20  # Frames in active working set
            
            # First pass - populate cache
            for i in range(working_set_size):
                cache.get(i)
            
            # Second pass - should achieve high hit rate
            initial_hits = cache.hits
            initial_misses = cache.misses
            
            for cycle in range(5):  # Multiple cycles through working set
                for i in range(working_set_size):
                    cache.get(i)
            
            # Calculate hit rate for second pass
            second_pass_hits = cache.hits - initial_hits
            second_pass_misses = cache.misses - initial_misses
            second_pass_total = second_pass_hits + second_pass_misses
            
            if second_pass_total > 0:
                second_pass_hit_rate = second_pass_hits / second_pass_total
                
                assert second_pass_hit_rate >= CACHE_HIT_RATE_THRESHOLD, \
                    f"Sequential access hit rate {second_pass_hit_rate:.3f} below {CACHE_HIT_RATE_THRESHOLD} requirement"
    
    def test_memory_pressure_performance_impact(self, performance_cache):
        """Test performance impact under memory pressure conditions."""
        cache = performance_cache
        
        def mock_large_frame(frame_id):
            # Larger frames to create memory pressure faster
            return np.random.randint(0, 256, (720, 1280, 3), dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_large_frame):
            # Phase 1: Fill cache to ~80% capacity
            frame_size = 720 * 1280 * 3
            target_frames = int(cache.max_memory * 0.8 // frame_size)
            
            phase1_times = []
            for i in range(target_frames):
                with self.timer_context():
                    cache.get(i)
                phase1_times.append(self.last_execution_time * 1000)
            
            avg_phase1_time = np.mean(phase1_times)
            
            # Phase 2: Continue loading to trigger evictions
            phase2_times = []
            for i in range(target_frames, target_frames + 20):
                with self.timer_context():
                    cache.get(i)
                phase2_times.append(self.last_execution_time * 1000)
            
            avg_phase2_time = np.mean(phase2_times)
            
            # Performance should not degrade significantly under memory pressure
            performance_degradation = avg_phase2_time / avg_phase1_time
            assert performance_degradation <= 2.0, \
                f"Performance degraded by {performance_degradation:.2f}x under memory pressure"
            
            # Cache should still function efficiently
            assert cache.memory_usage <= cache.max_memory, "Memory limit maintained"
            assert cache.evictions > 0, "Evictions should have occurred"
    
    def test_concurrent_access_performance_scaling(self, performance_cache):
        """Test performance scaling under concurrent access scenarios."""
        cache = performance_cache
        results = {}
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        def concurrent_access_worker(thread_id, num_accesses):
            """Worker for concurrent performance testing."""
            thread_times = []
            
            with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                # Pre-populate some frames for this thread
                base_offset = thread_id * 20
                for i in range(10):  # Pre-populate 10 frames
                    cache.get(base_offset + i)
                
                # Measure performance of mixed hit/miss pattern
                for i in range(num_accesses):
                    frame_id = base_offset + (i % 15)  # Mix of hits and misses
                    
                    start_time = time.perf_counter()
                    frame = cache.get(frame_id)
                    end_time = time.perf_counter()
                    
                    assert frame is not None
                    thread_times.append((end_time - start_time) * 1000)  # ms
                
                results[thread_id] = {
                    'times': thread_times,
                    'avg_time': np.mean(thread_times),
                    'max_time': np.max(thread_times)
                }
        
        # Test with increasing number of concurrent threads
        for num_threads in [1, 2, 4]:
            threads = []
            results.clear()
            
            # Start concurrent workers
            for i in range(num_threads):
                thread = threading.Thread(
                    target=concurrent_access_worker, 
                    args=(i, 50)  # 50 accesses per thread
                )
                threads.append(thread)
                thread.start()
            
            # Wait for completion
            for thread in threads:
                thread.join(timeout=30.0)
            
            # Analyze performance scaling
            if len(results) == num_threads:
                avg_times = [result['avg_time'] for result in results.values()]
                max_times = [result['max_time'] for result in results.values()]
                
                overall_avg = np.mean(avg_times)
                overall_max = np.max(max_times)
                
                # Performance should remain reasonable under concurrent load
                assert overall_avg <= FRAME_RETRIEVAL_LATENCY_MS * 3, \
                    f"Average time {overall_avg:.3f}ms too high with {num_threads} threads"
                assert overall_max <= FRAME_RETRIEVAL_LATENCY_MS * 5, \
                    f"Maximum time {overall_max:.3f}ms too high with {num_threads} threads"


@pytest.mark.cache
@pytest.mark.skipif(not HYPOTHESIS_AVAILABLE, reason="Hypothesis not available for property-based testing")
class TestFrameCachePropertyBased:
    """Property-based testing with Hypothesis for LRU correctness across arbitrary patterns."""
    
    @pytest.fixture
    def property_cache(self):
        """Create cache for property-based testing."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        return FrameCache(mode=CacheMode.LRU, max_memory=50*1024*1024)  # 50MB for testing
    
    @given(st.lists(st.integers(min_value=0, max_value=99), min_size=20, max_size=200))
    def test_lru_invariant_maintenance_property(self, access_sequence, property_cache):
        """Property-based test ensuring LRU invariants under arbitrary access patterns."""
        cache = property_cache
        
        def mock_load_frame(frame_id):
            return np.full((100, 100, 3), frame_id % 256, dtype=np.uint8)
        
        access_history = []
        unique_accesses = set()
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            for frame_id in access_sequence:
                cache.get(frame_id)
                access_history.append(frame_id)
                unique_accesses.add(frame_id)
                
                # Invariant 1: Memory limit never exceeded
                assert cache.memory_usage <= cache.max_memory, \
                    f"Memory invariant violated: {cache.memory_usage} > {cache.max_memory}"
                
                # Invariant 2: Cache statistics consistency
                assert cache.hits + cache.misses == len(access_history), \
                    "Statistics invariant violated: hits + misses != total accesses"
                
                # Invariant 3: Hit rate bounds
                assert 0.0 <= cache.hit_rate <= 1.0, \
                    f"Hit rate bounds violated: {cache.hit_rate}"
                
                # Invariant 4: Non-negative statistics
                assert cache.hits >= 0, "Hits should be non-negative"
                assert cache.misses >= 0, "Misses should be non-negative"
                assert cache.evictions >= 0, "Evictions should be non-negative"
                assert cache.memory_usage >= 0, "Memory usage should be non-negative"
                assert cache.cache_size >= 0, "Cache size should be non-negative"
    
    @given(st.integers(min_value=1, max_value=20), st.lists(st.integers(min_value=0, max_value=50), min_size=10, max_size=100))
    def test_cache_capacity_compliance_property(self, cache_capacity_mb, access_sequence, property_cache):
        """Property-based test for cache capacity compliance under variable sizing."""
        assume(cache_capacity_mb > 0)
        
        # Reconfigure cache with specified capacity
        max_memory = cache_capacity_mb * 1024 * 1024
        cache = FrameCache(mode=CacheMode.LRU, max_memory=max_memory)
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (200, 200, 3), dtype=np.uint8)  # ~120KB frames
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            for frame_id in access_sequence:
                cache.get(frame_id)
                
                # Primary invariant: Memory compliance
                assert cache.memory_usage <= max_memory, \
                    f"Memory limit exceeded: {cache.memory_usage} > {max_memory}"
                
                # Capacity utilization should be reasonable
                if cache.cache_size > 0:
                    avg_frame_size = cache.memory_usage / cache.cache_size
                    assert avg_frame_size > 0, "Average frame size should be positive"
                    
                # Cache should remain functional under all conditions
                assert hasattr(cache, 'hit_rate'), "Cache should maintain hit_rate property"
                assert callable(getattr(cache, 'get')), "Cache should maintain get method"
    
    @given(st.lists(st.integers(min_value=0, max_value=10), min_size=50, max_size=200))
    def test_working_set_behavior_property(self, access_sequence, property_cache):
        """Property-based test for working set behavior and hit rate optimization."""
        cache = property_cache
        
        def mock_load_frame(frame_id):
            return np.full((150, 150, 3), frame_id % 256, dtype=np.uint8)
        
        # Analyze access pattern
        unique_frames = set(access_sequence)
        working_set_size = len(unique_frames)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # First pass - establish working set
            for frame_id in access_sequence:
                cache.get(frame_id)
            
            first_pass_hit_rate = cache.hit_rate
            
            # Second pass - should show improved hit rate for smaller working sets
            initial_stats = (cache.hits, cache.misses)
            
            for frame_id in access_sequence:
                cache.get(frame_id)
            
            second_pass_hits = cache.hits - initial_stats[0]
            second_pass_total = (cache.hits + cache.misses) - sum(initial_stats)
            
            if second_pass_total > 0:
                second_pass_hit_rate = second_pass_hits / second_pass_total
                
                # For small working sets, second pass should have better hit rate
                if working_set_size <= 10:  # Small working set
                    assert second_pass_hit_rate >= first_pass_hit_rate, \
                        f"Hit rate should improve for small working sets: {second_pass_hit_rate} vs {first_pass_hit_rate}"
                
                # Hit rate should be reasonable for any working set
                assert 0.0 <= second_pass_hit_rate <= 1.0, \
                    f"Hit rate out of bounds: {second_pass_hit_rate}"
    
    @given(st.lists(st.integers(min_value=0, max_value=100), min_size=30, max_size=150))
    def test_eviction_fairness_property(self, access_sequence, property_cache):
        """Property-based test for fair LRU eviction behavior."""
        cache = property_cache
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (200, 200, 3), dtype=np.uint8)
        
        frame_last_access = {}
        access_order = []
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            for i, frame_id in enumerate(access_sequence):
                cache.get(frame_id)
                frame_last_access[frame_id] = i
                access_order.append(frame_id)
                
                # Track eviction behavior
                if cache.evictions > 0:
                    # Verify that cache maintains some recently accessed frames
                    # This is a general fairness check rather than strict LRU verification
                    recent_accesses = access_order[-min(len(access_order), 5):]
                    
                    # At least some recent accesses should result in cache hits
                    # (This tests that recent frames aren't immediately evicted)
                    recent_hit_count = 0
                    for recent_frame in recent_accesses:
                        # Test if frame is likely in cache by checking hit rate improvement
                        pre_hits = cache.hits
                        cache.get(recent_frame)
                        if cache.hits > pre_hits:
                            recent_hit_count += 1
                    
                    # Some level of recency should be preserved
                    # (Implementation-dependent, but general fairness principle)
                    if len(recent_accesses) > 2:
                        fairness_ratio = recent_hit_count / len(recent_accesses)
                        # This is a loose fairness check - exact behavior depends on implementation
                        assert fairness_ratio >= 0.0, "Fairness ratio should be non-negative"


@pytest.mark.cache  
class TestFrameCacheModeSwitching:
    """Test cache mode switching between LRU, full-preload, and direct I/O modes."""
    
    def test_lru_mode_functionality(self):
        """Test LRU mode specific functionality and behavior."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache(mode=CacheMode.LRU, max_memory=50*1024*1024)
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Test LRU-specific behavior
            assert cache.mode == CacheMode.LRU
            
            # LRU should cache frames
            frame1 = cache.get(1)
            assert cache.cache_size == 1, "LRU mode should cache frames"
            assert cache.misses == 1, "First access should be a miss"
            
            # LRU should return cached frames on repeat access
            frame1_again = cache.get(1)
            assert cache.hits == 1, "Second access should be a hit"
            np.testing.assert_array_equal(frame1, frame1_again)
            
            # LRU should perform evictions when memory limit reached
            frame_size = frame1.nbytes
            max_frames = cache.max_memory // frame_size
            
            # Load more frames than capacity
            for i in range(max_frames + 5):
                cache.get(i)
            
            # Should have triggered evictions
            assert cache.evictions > 0, "LRU mode should perform evictions"
            assert cache.memory_usage <= cache.max_memory, "Should respect memory limit"
    
    def test_preload_mode_functionality(self):
        """Test full-preload mode specific functionality and behavior."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache(mode=CacheMode.ALL, max_memory=100*1024*1024)
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Test preload-specific behavior
            assert cache.mode == CacheMode.ALL
            
            # Preload mode might have different caching strategy
            if hasattr(cache, 'preload'):
                # Test preload functionality if available
                frame_range = range(10)
                cache.preload(frame_range)
                
                # After preload, accesses should be hits
                initial_hits = cache.hits
                for frame_id in frame_range:
                    cache.get(frame_id)
                
                hit_improvement = cache.hits - initial_hits
                assert hit_improvement > 0, "Preloaded frames should result in cache hits"
    
    def test_direct_io_mode_functionality(self):
        """Test direct I/O passthrough mode functionality."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = FrameCache(mode=CacheMode.NONE)
        
        def mock_load_frame(frame_id):
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
            # Test direct I/O behavior
            assert cache.mode == CacheMode.NONE
            
            # Direct mode should not cache frames
            frame1 = cache.get(1)
            assert isinstance(frame1, np.ndarray), "Should still return frames"
            
            # Direct mode may not maintain cache statistics the same way
            # Verify it doesn't crash and returns valid frames
            frame2 = cache.get(2)
            assert isinstance(frame2, np.ndarray)
            
            # Direct mode should have minimal memory usage
            if hasattr(cache, 'memory_usage'):
                # Memory usage should be minimal (no persistent caching)
                assert cache.memory_usage <= 1024*1024, "Direct mode should use minimal memory"  # 1MB tolerance
    
    def test_cache_mode_configuration_validation(self):
        """Test cache mode configuration and validation."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Test valid mode configurations
        valid_modes = [CacheMode.LRU, CacheMode.ALL, CacheMode.NONE]
        
        for mode in valid_modes:
            cache = FrameCache(mode=mode)
            assert cache.mode == mode, f"Mode {mode} should be properly set"
        
        # Test string mode configurations if supported
        try:
            lru_cache = FrameCache(mode="lru")
            assert lru_cache.mode in [CacheMode.LRU, "lru"], "String mode should be accepted"
            
            all_cache = FrameCache(mode="all") 
            assert all_cache.mode in [CacheMode.ALL, "all"], "String mode should be accepted"
            
            none_cache = FrameCache(mode="none")
            assert none_cache.mode in [CacheMode.NONE, "none"], "String mode should be accepted"
            
        except (ValueError, TypeError):
            # Implementation may only support enum modes
            pass
    
    def test_mode_specific_memory_handling(self):
        """Test mode-specific memory handling differences."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        memory_limit = 50*1024*1024  # 50MB
        
        # Test LRU memory handling
        lru_cache = FrameCache(mode=CacheMode.LRU, max_memory=memory_limit)
        assert lru_cache.max_memory == memory_limit, "LRU should respect memory limit"
        
        # Test preload memory handling  
        preload_cache = FrameCache(mode=CacheMode.ALL, max_memory=memory_limit)
        assert preload_cache.max_memory == memory_limit, "Preload should respect memory limit"
        
        # Test direct I/O memory handling
        direct_cache = FrameCache(mode=CacheMode.NONE)
        # Direct mode may not have memory limits or have different defaults
        
        def mock_load_frame(frame_id):
            return np.random.randint(0, 256, (480, 640, 3), dtype=np.uint8)
        
        # Test memory behavior under load
        for cache in [lru_cache, preload_cache]:
            with patch.object(cache, '_load_frame', side_effect=mock_load_frame):
                # Load frames beyond memory capacity
                frame_size = 480 * 640 * 3
                target_frames = (memory_limit // frame_size) + 10
                
                for i in range(target_frames):
                    cache.get(i)
                    
                    # Memory limit should be respected
                    assert cache.memory_usage <= memory_limit * 1.1, \
                        f"Memory usage {cache.memory_usage} exceeded limit {memory_limit} significantly"
    
    def test_mode_performance_characteristics(self):
        """Test performance characteristics differences between modes."""
        if not CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        def mock_load_frame(frame_id):
            # Simulate I/O delay
            time.sleep(0.001)  # 1ms
            return np.full((480, 640, 3), frame_id % 256, dtype=np.uint8)
        
        # Test LRU performance
        lru_cache = FrameCache(mode=CacheMode.LRU, max_memory=100*1024*1024)
        with patch.object(lru_cache, '_load_frame', side_effect=mock_load_frame):
            # Warm up cache
            for i in range(5):
                lru_cache.get(i)
            
            # Measure hit performance
            start_time = time.perf_counter()
            for _ in range(20):
                lru_cache.get(0)  # Should be cache hit
            lru_hit_time = (time.perf_counter() - start_time) / 20
        
        # Test direct I/O performance
        direct_cache = FrameCache(mode=CacheMode.NONE)
        with patch.object(direct_cache, '_load_frame', side_effect=mock_load_frame):
            # Measure direct access performance
            start_time = time.perf_counter()
            for _ in range(5):  # Fewer iterations due to I/O overhead
                direct_cache.get(0)  # Should go through I/O each time
            direct_access_time = (time.perf_counter() - start_time) / 5
        
        # LRU cache hits should be significantly faster than direct I/O
        assert lru_hit_time < direct_access_time, \
            f"LRU cache hits ({lru_hit_time:.6f}s) should be faster than direct I/O ({direct_access_time:.6f}s)"
        
        # LRU hits should be sub-millisecond
        assert lru_hit_time <= 0.001, f"LRU cache hits should be ≤1ms, got {lru_hit_time:.6f}s"


if __name__ == "__main__":
    # Run cache tests with appropriate markers and coverage
    pytest.main([
        __file__,
        "-v",
        "--tb=short", 
        "-m", "cache",
        "--cov=odor_plume_nav.cache",
        "--cov-report=term-missing",
        "--cov-fail-under=80"  # 80% coverage requirement for new modules
    ])