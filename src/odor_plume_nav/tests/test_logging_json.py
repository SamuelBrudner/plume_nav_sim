"""
Comprehensive JSON logging validation test suite for structured Loguru observability.

This module provides extensive validation of the Loguru-based structured logging system,
ensuring JSON-formatted outputs conform to logging.yaml schema requirements and maintain
machine-parseable consistency across all deployment environments. The test suite validates
enterprise-grade observability capabilities essential for production robotics deployments.

Key Testing Areas:
- JSON sink validation ensuring Loguru structured logging compliance per Section 0.4.1
- Logging.yaml schema validation for dual sink architecture per Section 6.6.3.2.5
- Correlation ID injection verification in multi-agent scenarios per Section 6.5.2.3
- Performance statistics inclusion validation in JSON log outputs per Section 6.5.2.2
- Machine-parseable format compliance with standard tools (jq) per Section 0.5.1
- Structured log format consistency across deployment environments per Section 6.6.1.1

Performance Requirements:
- JSON parsing validation must complete within 100ms per record per Section 6.6.8.2
- Correlation ID injection overhead must remain <1ms per log entry
- Log format consistency validation must achieve >95% compliance across environments

Integration Testing:
- Validates integration with FrameCache performance statistics per Section 5.3.4
- Tests dual sink architecture (console + JSON) per Section 5.3.5
- Ensures compatibility with Hydra configuration system per Section 6.6.3.2.5

Test Coverage Targets:
- JSON schema validation: 100% coverage per Section 6.6.6.1.1
- Correlation context injection: 95% coverage per Section 6.6.6.1.1
- Performance statistics integration: 90% coverage per Section 6.6.6.1.1
- Multi-environment consistency: 85% coverage per Section 6.6.6.1.1

Authors: Generated by Blitzy Platform
Version: 2.0.0
License: MIT
"""

import os
import json
import time
import uuid
import tempfile
import subprocess
import threading
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, Tuple
from unittest.mock import Mock, patch, MagicMock
from contextlib import contextmanager
from dataclasses import asdict

import pytest
import yaml
import numpy as np
from hypothesis import given, strategies as st, assume

# Import project modules with graceful fallback
try:
    from loguru import logger
    from odor_plume_nav.utils.logging_setup import (
        setup_logger, get_module_logger, LoggingConfig, CorrelationContext,
        correlation_context, get_correlation_context, _load_logging_yaml,
        _setup_yaml_sinks, _create_json_formatter, update_cache_metrics,
        log_cache_memory_pressure_violation, PerformanceMetrics
    )
    LOGGING_AVAILABLE = True
except ImportError:
    LOGGING_AVAILABLE = False


# Test markers for categorization
pytestmark = [
    pytest.mark.integration,
    pytest.mark.logging,
]


class TestJSONSinkValidation:
    """
    Test suite for JSON sink validation ensuring Loguru structured logging compliance.
    
    Validates that JSON-formatted log outputs conform to the logging.yaml schema
    requirements and maintain machine-parseable consistency across deployment
    environments per Section 0.4.1 requirements.
    """
    
    @pytest.fixture(autouse=True)
    def setup_logging_environment(self, tmp_path):
        """Setup isolated logging environment for JSON validation testing."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Loguru logging not available for JSON validation tests")
        
        # Clear any existing loggers
        logger.remove()
        
        # Create test directory structure
        self.test_dir = tmp_path / "logging_test"
        self.test_dir.mkdir()
        self.log_dir = self.test_dir / "logs"
        self.log_dir.mkdir()
        
        # Test file paths
        self.json_log_file = self.log_dir / "test_structured.json"
        self.console_log_file = self.log_dir / "test_console.log"
        
        yield self.test_dir
        
        # Cleanup loggers after each test
        logger.remove()
    
    def test_json_sink_basic_functionality(self, setup_logging_environment):
        """Test basic JSON sink functionality with structured output validation."""
        # Configure JSON sink
        logger.add(
            str(self.json_log_file),
            format=_create_json_formatter(),
            level="INFO",
            serialize=True,
            enqueue=False  # Synchronous for testing
        )
        
        # Generate test log entry
        test_message = "JSON sink validation test"
        test_correlation_id = str(uuid.uuid4())
        
        logger.bind(
            correlation_id=test_correlation_id,
            module="test_logging_json",
            metric_type="json_validation"
        ).info(test_message)
        
        # Validate JSON file creation and structure
        assert self.json_log_file.exists(), "JSON log file should be created"
        
        # Read and parse JSON log entry
        with open(self.json_log_file, 'r') as f:
            log_line = f.readline().strip()
        
        # Validate JSON parsing
        try:
            log_record = json.loads(log_line)
        except json.JSONDecodeError as e:
            pytest.fail(f"Failed to parse JSON log record: {e}")
        
        # Validate required fields per logging.yaml schema
        required_fields = [
            "timestamp", "level", "logger", "message", 
            "correlation_id", "module", "metric_type"
        ]
        
        for field in required_fields:
            assert field in log_record, f"Required field '{field}' missing from JSON log"
        
        # Validate field values
        assert log_record["message"] == test_message
        assert log_record["correlation_id"] == test_correlation_id
        assert log_record["module"] == "test_logging_json"
        assert log_record["metric_type"] == "json_validation"
        assert log_record["level"] == "INFO"
    
    def test_json_structure_compliance_with_schema(self, setup_logging_environment):
        """Test JSON log structure compliance with logging.yaml schema definition."""
        # Load logging.yaml schema for validation
        logging_yaml_path = Path(__file__).parent.parent.parent.parent / "logging.yaml"
        
        if not logging_yaml_path.exists():
            pytest.skip("logging.yaml not found for schema validation")
        
        with open(logging_yaml_path, 'r') as f:
            logging_config = yaml.safe_load(f)
        
        # Extract expected JSON schema structure
        json_sink_config = logging_config.get("sinks", {}).get("json", {})
        
        # Configure logger based on schema
        logger.add(
            str(self.json_log_file),
            format=_create_json_formatter(),
            level=json_sink_config.get("level", "INFO"),
            serialize=True,
            rotation=json_sink_config.get("rotation", "10 MB"),
            retention=json_sink_config.get("retention", "30 days")
        )
        
        # Generate comprehensive test log entry
        with correlation_context("json_validation", episode_id="test_episode_001") as ctx:
            # Add performance metrics
            ctx.add_metadata(
                system_info={"platform": "test", "memory_gb": 16},
                experiment_config={"mode": "validation"}
            )
            
            # Update cache metrics
            update_cache_metrics(
                ctx,
                cache_hit_count=150,
                cache_miss_count=25,
                cache_evictions=5,
                cache_memory_usage_mb=512.5,
                cache_memory_limit_mb=2048.0
            )
            
            logger.bind(**ctx.bind_context()).info(
                "Comprehensive JSON schema validation test",
                extra={
                    "metric_type": "schema_validation",
                    "performance_metrics": {
                        "step_latency_ms": 8.5,
                        "fps": 32.1,
                        "memory_usage_mb": 1024.0
                    },
                    "test_category": "json_compliance"
                }
            )
        
        # Read and validate JSON structure
        with open(self.json_log_file, 'r') as f:
            log_line = f.readline().strip()
        
        log_record = json.loads(log_line)
        
        # Validate comprehensive schema compliance
        expected_fields = [
            "timestamp", "level", "logger", "function", "line", "message",
            "correlation_id", "request_id", "module", "thread_id", "process_id",
            "step_count", "episode_id", "metric_type", "performance_metrics",
            "cache_stats", "system_info", "experiment_config"
        ]
        
        for field in expected_fields:
            assert field in log_record, f"Schema field '{field}' missing from JSON log"
        
        # Validate cache statistics structure
        cache_stats = log_record.get("cache_stats", {})
        expected_cache_fields = [
            "cache_hit_count", "cache_miss_count", "cache_evictions",
            "cache_hit_rate", "cache_memory_usage_mb", "cache_memory_limit_mb"
        ]
        
        for field in expected_cache_fields:
            assert field in cache_stats, f"Cache field '{field}' missing from JSON log"
        
        # Validate data types
        assert isinstance(log_record["step_count"], int)
        assert isinstance(cache_stats["cache_hit_rate"], float)
        assert isinstance(log_record["performance_metrics"], dict)
    
    def test_json_parsing_with_standard_tools(self, setup_logging_environment):
        """Test JSON log parsing with standard tools (jq) per Section 0.5.1."""
        # Configure JSON sink
        logger.add(
            str(self.json_log_file),
            format=_create_json_formatter(),
            level="INFO",
            serialize=True
        )
        
        # Generate multiple log entries for comprehensive testing
        test_entries = [
            {"message": "Test entry 1", "value": 100},
            {"message": "Test entry 2", "value": 200},
            {"message": "Test entry 3", "value": 300}
        ]
        
        for i, entry in enumerate(test_entries):
            logger.bind(
                correlation_id=f"test_{i:03d}",
                module="jq_validation",
                metric_type="parsing_test",
                test_value=entry["value"]
            ).info(entry["message"])
        
        # Validate file contains valid JSON
        assert self.json_log_file.exists()
        
        # Test JSON parsing with Python (simulating jq)
        log_records = []
        with open(self.json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        record = json.loads(line)
                        log_records.append(record)
                    except json.JSONDecodeError as e:
                        pytest.fail(f"Invalid JSON in log file: {e}")
        
        # Validate all entries were parsed correctly
        assert len(log_records) == len(test_entries)
        
        # Test jq-like queries on parsed data
        info_records = [r for r in log_records if r.get("level") == "INFO"]
        assert len(info_records) == len(test_entries)
        
        # Test filtering by correlation_id pattern
        test_correlation_records = [
            r for r in log_records 
            if r.get("correlation_id", "").startswith("test_")
        ]
        assert len(test_correlation_records) == len(test_entries)
        
        # Test extracting specific fields (simulating jq '.test_value')
        test_values = [r.get("test_value") for r in log_records]
        expected_values = [entry["value"] for entry in test_entries]
        assert test_values == expected_values
        
        # Test complex filtering (simulating jq 'select(.test_value > 150)')
        high_value_records = [
            r for r in log_records 
            if r.get("test_value", 0) > 150
        ]
        assert len(high_value_records) == 2  # entries 2 and 3
    
    def test_json_format_consistency_across_log_levels(self, setup_logging_environment):
        """Test JSON format consistency across different log levels."""
        logger.add(
            str(self.json_log_file),
            format=_create_json_formatter(),
            level="TRACE",  # Capture all levels
            serialize=True
        )
        
        # Test all log levels
        log_levels = ["TRACE", "DEBUG", "INFO", "SUCCESS", "WARNING", "ERROR", "CRITICAL"]
        correlation_id = str(uuid.uuid4())
        
        for level in log_levels:
            getattr(logger, level.lower())(
                f"Test message for {level} level",
                correlation_id=correlation_id,
                module="level_consistency",
                metric_type="level_validation",
                log_level_test=level
            )
        
        # Parse all log entries
        log_records = []
        with open(self.json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    log_records.append(json.loads(line))
        
        assert len(log_records) == len(log_levels)
        
        # Validate consistent JSON structure across all levels
        base_schema = set(log_records[0].keys())
        
        for i, record in enumerate(log_records):
            # Ensure all records have the same basic schema
            assert set(record.keys()) == base_schema, (
                f"Schema inconsistency in {log_levels[i]} level log"
            )
            
            # Validate level-specific fields
            assert record["level"] == log_levels[i]
            assert record["correlation_id"] == correlation_id
            assert record["log_level_test"] == log_levels[i]
            
            # Validate timestamp format consistency
            assert "timestamp" in record
            assert isinstance(record["timestamp"], str)
            
            # Validate message format
            expected_message = f"Test message for {log_levels[i]} level"
            assert record["message"] == expected_message


class TestLoggingYAMLSchemaValidation:
    """
    Test suite for logging.yaml schema validation and sink configuration.
    
    Validates proper loading and application of logging configuration from
    logging.yaml file, ensuring dual sink architecture compliance per
    Section 6.6.3.2.5 requirements.
    """
    
    @pytest.fixture
    def sample_logging_config(self, tmp_path):
        """Create sample logging.yaml configuration for testing."""
        config_dir = tmp_path / "config"
        config_dir.mkdir()
        
        # Create comprehensive logging configuration
        logging_config = {
            "sinks": {
                "console": {
                    "enabled": True,
                    "sink": "sys.stderr",
                    "format": ("<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
                              "<level>{level: <8}</level> | "
                              "<cyan>{name}</cyan> | "
                              "<magenta>correlation_id={extra[correlation_id]}</magenta> - "
                              "<level>{message}</level>"),
                    "level": "INFO",
                    "colorize": True,
                    "enqueue": True,
                    "backtrace": True,
                    "diagnose": True
                },
                "json": {
                    "enabled": True,
                    "sink": str(tmp_path / "test_output.json"),
                    "format": "json",
                    "level": "DEBUG",
                    "rotation": "10 MB",
                    "retention": "7 days",
                    "compression": "gz",
                    "serialize": True,
                    "enqueue": True,
                    "backtrace": True,
                    "diagnose": True
                }
            },
            "performance": {
                "monitoring_enabled": True,
                "thresholds": {
                    "environment_step": 0.010,
                    "frame_processing": 0.033,
                    "cache_hit_rate_threshold": 0.90
                }
            },
            "correlation": {
                "enabled": True,
                "default_context": {
                    "correlation_id": "none",
                    "module": "system",
                    "step_count": 0
                }
            }
        }
        
        config_file = config_dir / "logging.yaml"
        with open(config_file, 'w') as f:
            yaml.dump(logging_config, f)
        
        return config_file, logging_config
    
    def test_logging_yaml_loading_and_parsing(self, sample_logging_config):
        """Test successful loading and parsing of logging.yaml configuration."""
        config_file, expected_config = sample_logging_config
        
        # Test configuration loading
        loaded_config = _load_logging_yaml(config_file)
        
        assert loaded_config is not None, "Failed to load logging configuration"
        assert isinstance(loaded_config, dict), "Loaded config should be a dictionary"
        
        # Validate basic structure
        assert "sinks" in loaded_config, "Configuration should contain 'sinks' section"
        assert "performance" in loaded_config, "Configuration should contain 'performance' section"
        assert "correlation" in loaded_config, "Configuration should contain 'correlation' section"
        
        # Validate sink configurations
        sinks = loaded_config["sinks"]
        assert "console" in sinks, "Console sink configuration missing"
        assert "json" in sinks, "JSON sink configuration missing"
        
        # Validate console sink configuration
        console_config = sinks["console"]
        assert console_config["enabled"] is True
        assert console_config["level"] == "INFO"
        assert console_config["colorize"] is True
        
        # Validate JSON sink configuration
        json_config = sinks["json"]
        assert json_config["enabled"] is True
        assert json_config["level"] == "DEBUG"
        assert json_config["serialize"] is True
        assert json_config["rotation"] == "10 MB"
        assert json_config["retention"] == "7 days"
    
    def test_invalid_logging_yaml_handling(self, tmp_path):
        """Test handling of invalid or malformed logging.yaml files."""
        # Test non-existent file
        non_existent_file = tmp_path / "missing.yaml"
        result = _load_logging_yaml(non_existent_file)
        assert result is None, "Should return None for non-existent file"
        
        # Test invalid YAML syntax
        invalid_yaml_file = tmp_path / "invalid.yaml"
        with open(invalid_yaml_file, 'w') as f:
            f.write("invalid: yaml: content: [unclosed")
        
        result = _load_logging_yaml(invalid_yaml_file)
        assert result is None, "Should return None for invalid YAML"
        
        # Test empty file
        empty_file = tmp_path / "empty.yaml"
        empty_file.write_text("")
        
        result = _load_logging_yaml(empty_file)
        # Empty YAML file loads as None, which should be handled gracefully
        assert result is None or result == {}, "Should handle empty YAML gracefully"
        
        # Test non-dictionary content
        non_dict_file = tmp_path / "non_dict.yaml"
        with open(non_dict_file, 'w') as f:
            yaml.dump(["list", "instead", "of", "dict"], f)
        
        result = _load_logging_yaml(non_dict_file)
        assert result is None, "Should return None for non-dictionary YAML"
    
    def test_dual_sink_architecture_setup(self, sample_logging_config, tmp_path):
        """Test proper setup of dual sink architecture from logging.yaml."""
        config_file, expected_config = sample_logging_config
        
        # Clear existing loggers
        logger.remove()
        
        # Load and apply configuration
        loaded_config = _load_logging_yaml(config_file)
        assert loaded_config is not None
        
        # Setup dual sink architecture
        default_context = {"correlation_id": "test", "module": "test"}
        
        try:
            _setup_yaml_sinks(loaded_config["sinks"], default_context)
            
            # Generate test log entries
            test_message = "Dual sink architecture test"
            logger.bind(**default_context).info(test_message)
            
            # Validate JSON sink output
            json_output_file = Path(loaded_config["sinks"]["json"]["sink"])
            assert json_output_file.exists(), "JSON sink should create output file"
            
            # Read and validate JSON output
            with open(json_output_file, 'r') as f:
                json_content = f.readline().strip()
            
            json_record = json.loads(json_content)
            assert json_record["message"] == test_message
            assert json_record["correlation_id"] == "test"
            
        finally:
            # Cleanup
            logger.remove()
    
    def test_environment_specific_configuration_loading(self, tmp_path):
        """Test environment-specific configuration profiles from logging.yaml."""
        # Create environment-specific configuration
        environments_config = {
            "environments": {
                "development": {
                    "console": {
                        "enabled": True,
                        "level": "DEBUG",
                        "colorize": True
                    },
                    "json": {
                        "enabled": True,
                        "level": "DEBUG",
                        "rotation": "5 MB",
                        "retention": "7 days"
                    }
                },
                "production": {
                    "console": {
                        "enabled": True,
                        "level": "INFO",
                        "colorize": False
                    },
                    "json": {
                        "enabled": True,
                        "level": "INFO",
                        "rotation": "50 MB",
                        "retention": "90 days"
                    }
                }
            }
        }
        
        config_file = tmp_path / "env_config.yaml"
        with open(config_file, 'w') as f:
            yaml.dump(environments_config, f)
        
        # Test configuration loading
        loaded_config = _load_logging_yaml(config_file)
        assert loaded_config is not None
        
        # Validate environment configurations
        environments = loaded_config["environments"]
        
        # Validate development environment
        dev_config = environments["development"]
        assert dev_config["console"]["level"] == "DEBUG"
        assert dev_config["console"]["colorize"] is True
        assert dev_config["json"]["rotation"] == "5 MB"
        
        # Validate production environment
        prod_config = environments["production"]
        assert prod_config["console"]["level"] == "INFO"
        assert prod_config["console"]["colorize"] is False
        assert prod_config["json"]["rotation"] == "50 MB"
        assert prod_config["json"]["retention"] == "90 days"


class TestCorrelationIDInjection:
    """
    Test suite for correlation ID injection verification in multi-agent scenarios.
    
    Validates that correlation IDs are properly generated, injected, and tracked
    across complex multi-agent simulation workflows per Section 6.5.2.3
    requirements.
    """
    
    @pytest.fixture
    def correlation_test_environment(self, tmp_path):
        """Setup correlation testing environment with JSON logging."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Correlation testing requires Loguru logging")
        
        # Clear existing loggers
        logger.remove()
        
        # Setup JSON logging for correlation tracking
        json_log_file = tmp_path / "correlation_test.json"
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="DEBUG",
            serialize=True,
            enqueue=False  # Synchronous for testing
        )
        
        return json_log_file
    
    def test_correlation_id_generation_and_injection(self, correlation_test_environment):
        """Test automatic correlation ID generation and injection into log records."""
        json_log_file = correlation_test_environment
        
        # Test automatic correlation ID generation
        with correlation_context("test_operation") as ctx:
            # Validate correlation ID was generated
            assert ctx.correlation_id is not None
            assert len(ctx.correlation_id) > 0
            assert isinstance(ctx.correlation_id, str)
            
            # Generate log entry with correlation context
            logger.bind(**ctx.bind_context()).info("Correlation ID test message")
        
        # Read and validate log entry
        with open(json_log_file, 'r') as f:
            log_content = f.readline().strip()
        
        log_record = json.loads(log_content)
        
        # Validate correlation ID injection
        assert "correlation_id" in log_record
        assert log_record["correlation_id"] is not None
        assert log_record["correlation_id"] != "none"
        assert len(log_record["correlation_id"]) > 0
    
    def test_custom_correlation_id_propagation(self, correlation_test_environment):
        """Test custom correlation ID propagation through log contexts."""
        json_log_file = correlation_test_environment
        
        # Use custom correlation ID
        custom_correlation_id = "custom_test_correlation_12345"
        
        with correlation_context(
            "custom_correlation_test",
            correlation_id=custom_correlation_id
        ) as ctx:
            # Validate custom ID is used
            assert ctx.correlation_id == custom_correlation_id
            
            # Generate multiple log entries
            for i in range(3):
                logger.bind(**ctx.bind_context()).info(f"Log entry {i}")
        
        # Read and validate all log entries
        log_records = []
        with open(json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    log_records.append(json.loads(line))
        
        assert len(log_records) == 3
        
        # Validate correlation ID consistency
        for record in log_records:
            assert record["correlation_id"] == custom_correlation_id
    
    def test_episode_id_tracking_in_rl_scenarios(self, correlation_test_environment):
        """Test episode ID tracking for reinforcement learning scenarios."""
        json_log_file = correlation_test_environment
        
        # Simulate RL training episodes
        episode_ids = ["episode_001", "episode_002", "episode_003"]
        
        for episode_id in episode_ids:
            with correlation_context(
                "rl_training_step",
                episode_id=episode_id
            ) as ctx:
                # Simulate episode steps
                for step in range(5):
                    ctx.increment_step()
                    logger.bind(**ctx.bind_context()).info(
                        f"RL step {step}",
                        action="move_forward",
                        reward=0.5
                    )
        
        # Read and validate all log entries
        log_records = []
        with open(json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    log_records.append(json.loads(line))
        
        # Should have 15 total log entries (3 episodes × 5 steps)
        assert len(log_records) == 15
        
        # Validate episode ID tracking
        episode_groups = {}
        for record in log_records:
            episode_id = record.get("episode_id")
            assert episode_id is not None
            
            if episode_id not in episode_groups:
                episode_groups[episode_id] = []
            episode_groups[episode_id].append(record)
        
        # Validate episode separation
        assert len(episode_groups) == 3
        
        for episode_id in episode_ids:
            assert episode_id in episode_groups
            episode_records = episode_groups[episode_id]
            assert len(episode_records) == 5
            
            # Validate step counting within episodes
            step_counts = [r["step_count"] for r in episode_records]
            assert step_counts == [1, 2, 3, 4, 5]
    
    def test_multi_agent_correlation_isolation(self, correlation_test_environment):
        """Test correlation ID isolation in multi-agent scenarios."""
        json_log_file = correlation_test_environment
        
        # Simulate multiple agents with separate correlation contexts
        agent_configs = [
            {"agent_id": "agent_001", "episode": "ep_001"},
            {"agent_id": "agent_002", "episode": "ep_001"},
            {"agent_id": "agent_003", "episode": "ep_002"}
        ]
        
        agent_contexts = []
        
        # Create separate correlation contexts for each agent
        for config in agent_configs:
            ctx = CorrelationContext(
                request_id=f"request_{config['agent_id']}",
                episode_id=config["episode"]
            )
            ctx.add_metadata(agent_id=config["agent_id"])
            agent_contexts.append((config, ctx))
        
        # Generate log entries from multiple agents
        for config, ctx in agent_contexts:
            logger.bind(**ctx.bind_context()).info(
                f"Agent {config['agent_id']} action",
                action_type="navigate",
                episode=config["episode"]
            )
        
        # Read and validate log entries
        log_records = []
        with open(json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    log_records.append(json.loads(line))
        
        assert len(log_records) == len(agent_configs)
        
        # Validate correlation ID isolation
        correlation_ids = [r["correlation_id"] for r in log_records]
        request_ids = [r["request_id"] for r in log_records]
        
        # All correlation IDs should be unique
        assert len(set(correlation_ids)) == len(correlation_ids)
        assert len(set(request_ids)) == len(request_ids)
        
        # Validate agent-specific metadata
        for i, record in enumerate(log_records):
            expected_agent_id = agent_configs[i]["agent_id"]
            expected_episode = agent_configs[i]["episode"]
            
            assert record["agent_id"] == expected_agent_id
            assert record["episode_id"] == expected_episode
            assert record["episode"] == expected_episode
    
    @given(st.integers(min_value=1, max_value=10))
    def test_correlation_id_uniqueness_under_load(self, correlation_test_environment, num_contexts):
        """Property-based test for correlation ID uniqueness under concurrent load."""
        assume(num_contexts >= 1)
        
        # Generate multiple correlation contexts
        correlation_ids = []
        
        for i in range(num_contexts):
            with correlation_context(f"load_test_{i}") as ctx:
                correlation_ids.append(ctx.correlation_id)
        
        # Validate all correlation IDs are unique
        assert len(set(correlation_ids)) == len(correlation_ids), (
            "Correlation IDs should be unique across all contexts"
        )
        
        # Validate correlation ID format (should be UUID-like)
        for correlation_id in correlation_ids:
            assert isinstance(correlation_id, str)
            assert len(correlation_id) > 10  # Reasonable minimum length
            assert correlation_id != "none"


class TestPerformanceStatisticsInclusion:
    """
    Test suite for performance statistics inclusion validation in JSON log outputs.
    
    Validates that cache metrics, timing information, and system performance
    statistics are properly embedded in structured log context per Section
    6.5.2.2 requirements.
    """
    
    @pytest.fixture
    def performance_logging_environment(self, tmp_path):
        """Setup performance statistics logging environment."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Performance statistics testing requires Loguru logging")
        
        # Clear existing loggers
        logger.remove()
        
        # Setup JSON logging for performance tracking
        json_log_file = tmp_path / "performance_test.json"
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="DEBUG",
            serialize=True,
            enqueue=False
        )
        
        return json_log_file
    
    def test_cache_statistics_inclusion_in_logs(self, performance_logging_environment):
        """Test automatic inclusion of cache statistics in JSON log outputs."""
        json_log_file = performance_logging_environment
        
        # Create correlation context with cache metrics
        with correlation_context("cache_statistics_test") as ctx:
            # Update cache metrics
            update_cache_metrics(
                ctx,
                cache_hit_count=250,
                cache_miss_count=50,
                cache_evictions=10,
                cache_memory_usage_mb=1024.5,
                cache_memory_limit_mb=2048.0
            )
            
            # Generate log entry with cache context
            logger.bind(**ctx.bind_context()).info(
                "Cache operation completed",
                operation="frame_retrieval",
                frame_id=42
            )
        
        # Read and validate log entry
        with open(json_log_file, 'r') as f:
            log_content = f.readline().strip()
        
        log_record = json.loads(log_content)
        
        # Validate cache statistics presence
        assert "cache_stats" in log_record
        cache_stats = log_record["cache_stats"]
        
        # Validate cache statistics content
        expected_cache_fields = [
            "cache_hit_count", "cache_miss_count", "cache_evictions",
            "cache_hit_rate", "cache_memory_usage_mb", "cache_memory_limit_mb"
        ]
        
        for field in expected_cache_fields:
            assert field in cache_stats, f"Cache field '{field}' missing from log"
        
        # Validate cache statistics values
        assert cache_stats["cache_hit_count"] == 250
        assert cache_stats["cache_miss_count"] == 50
        assert cache_stats["cache_evictions"] == 10
        assert cache_stats["cache_memory_usage_mb"] == 1024.5
        assert cache_stats["cache_memory_limit_mb"] == 2048.0
        
        # Validate calculated hit rate
        expected_hit_rate = 250 / (250 + 50)  # 0.833...
        assert abs(cache_stats["cache_hit_rate"] - expected_hit_rate) < 0.001
    
    def test_performance_metrics_embedding(self, performance_logging_environment):
        """Test embedding of performance metrics in structured log context."""
        json_log_file = performance_logging_environment
        
        # Create performance metrics
        performance_metrics = PerformanceMetrics(
            operation_name="environment_step",
            start_time=time.time() - 0.008,  # 8ms ago
            metadata={"agent_count": 5, "environment_type": "video_plume"}
        )
        performance_metrics.complete()
        
        # Generate log entry with performance metrics
        logger.bind(
            correlation_id="perf_test_001",
            module="performance_validation",
            metric_type="step_execution"
        ).info(
            "Environment step completed",
            extra={
                "performance_metrics": performance_metrics.to_dict(),
                "step_latency_ms": performance_metrics.duration * 1000,
                "fps_estimate": 1.0 / performance_metrics.duration if performance_metrics.duration > 0 else 0
            }
        )
        
        # Read and validate log entry
        with open(json_log_file, 'r') as f:
            log_content = f.readline().strip()
        
        log_record = json.loads(log_content)
        
        # Validate performance metrics inclusion
        assert "performance_metrics" in log_record
        perf_data = log_record["performance_metrics"]
        
        # Validate performance metrics structure
        assert "operation_name" in perf_data
        assert "duration" in perf_data
        assert "metadata" in perf_data
        
        assert perf_data["operation_name"] == "environment_step"
        assert isinstance(perf_data["duration"], float)
        assert perf_data["metadata"]["agent_count"] == 5
        
        # Validate additional performance fields
        assert "step_latency_ms" in log_record
        assert "fps_estimate" in log_record
        assert isinstance(log_record["step_latency_ms"], float)
        assert isinstance(log_record["fps_estimate"], float)
    
    def test_memory_pressure_violation_logging(self, performance_logging_environment):
        """Test memory pressure violation logging with ResourceError category."""
        json_log_file = performance_logging_environment
        
        # Simulate memory pressure scenario
        current_usage_mb = 1900.0  # Close to 2GB limit
        limit_mb = 2048.0  # 2GB limit
        
        # Log memory pressure violation
        log_cache_memory_pressure_violation(
            current_usage_mb=current_usage_mb,
            limit_mb=limit_mb,
            threshold_ratio=0.9
        )
        
        # Read and validate log entry
        with open(json_log_file, 'r') as f:
            log_content = f.readline().strip()
        
        log_record = json.loads(log_content)
        
        # Validate memory pressure log structure
        assert log_record["level"] == "WARNING"
        assert "memory pressure violation" in log_record["message"].lower()
        
        # Validate memory pressure metrics
        assert "metric_type" in log_record
        assert log_record["metric_type"] == "memory_pressure_violation"
        
        assert "resource_category" in log_record
        assert log_record["resource_category"] == "cache_memory"
        
        assert "current_usage_mb" in log_record
        assert "limit_mb" in log_record
        assert "usage_ratio" in log_record
        
        assert log_record["current_usage_mb"] == current_usage_mb
        assert log_record["limit_mb"] == limit_mb
        
        # Validate calculated usage ratio
        expected_ratio = current_usage_mb / limit_mb
        assert abs(log_record["usage_ratio"] - expected_ratio) < 0.001
    
    def test_system_performance_context_inclusion(self, performance_logging_environment):
        """Test inclusion of system performance context in log records."""
        json_log_file = performance_logging_environment
        
        # Create comprehensive performance context
        with correlation_context("system_performance_test") as ctx:
            # Add system performance metadata
            ctx.add_metadata(
                system_performance={
                    "cpu_usage_percent": 45.2,
                    "memory_usage_gb": 8.5,
                    "disk_io_mb_per_sec": 125.0,
                    "network_latency_ms": 12.3
                },
                environment_metrics={
                    "simulation_fps": 32.1,
                    "active_agents": 8,
                    "total_steps": 1500
                }
            )
            
            # Update cache metrics
            update_cache_metrics(
                ctx,
                cache_hit_count=1200,
                cache_miss_count=150,
                cache_memory_usage_mb=512.0
            )
            
            # Generate comprehensive log entry
            logger.bind(**ctx.bind_context()).info(
                "System performance checkpoint",
                extra={
                    "metric_type": "performance_checkpoint",
                    "checkpoint_type": "system_wide",
                    "performance_threshold_compliance": True
                }
            )
        
        # Read and validate log entry
        with open(json_log_file, 'r') as f:
            log_content = f.readline().strip()
        
        log_record = json.loads(log_content)
        
        # Validate system performance context
        assert "system_performance" in log_record
        system_perf = log_record["system_performance"]
        
        assert "cpu_usage_percent" in system_perf
        assert "memory_usage_gb" in system_perf
        assert "disk_io_mb_per_sec" in system_perf
        assert "network_latency_ms" in system_perf
        
        # Validate environment metrics
        assert "environment_metrics" in log_record
        env_metrics = log_record["environment_metrics"]
        
        assert "simulation_fps" in env_metrics
        assert "active_agents" in env_metrics
        assert "total_steps" in env_metrics
        
        # Validate cache statistics
        assert "cache_stats" in log_record
        cache_stats = log_record["cache_stats"]
        
        assert cache_stats["cache_hit_count"] == 1200
        assert cache_stats["cache_miss_count"] == 150
        
        # Validate additional performance context
        assert "metric_type" in log_record
        assert log_record["metric_type"] == "performance_checkpoint"


class TestDualSinkArchitecture:
    """
    Test suite for dual sink architecture testing (console + JSON).
    
    Validates the proper operation of both console and JSON sinks
    simultaneously, ensuring format consistency and performance
    per Section 5.3.5 requirements.
    """
    
    @pytest.fixture
    def dual_sink_environment(self, tmp_path):
        """Setup dual sink logging environment with console and JSON outputs."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Dual sink testing requires Loguru logging")
        
        # Clear existing loggers
        logger.remove()
        
        # Setup file paths
        json_log_file = tmp_path / "dual_sink_test.json"
        console_log_file = tmp_path / "console_output.log"
        
        # Configure JSON sink
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="DEBUG",
            serialize=True,
            enqueue=False
        )
        
        # Configure console-style sink (to file for testing)
        console_format = (
            "<green>{time:YYYY-MM-DD HH:mm:ss.SSS}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan> | "
            "<magenta>correlation_id={extra[correlation_id]}</magenta> - "
            "<level>{message}</level>"
        )
        
        logger.add(
            str(console_log_file),
            format=console_format,
            level="INFO",
            serialize=False,
            enqueue=False,
            colorize=False  # Disable colors for file output testing
        )
        
        return json_log_file, console_log_file
    
    def test_dual_sink_simultaneous_output(self, dual_sink_environment):
        """Test simultaneous output to both JSON and console sinks."""
        json_log_file, console_log_file = dual_sink_environment
        
        # Generate test log entries at different levels
        test_entries = [
            ("DEBUG", "Debug level message - JSON only"),
            ("INFO", "Info level message - both sinks"),
            ("WARNING", "Warning level message - both sinks"),
            ("ERROR", "Error level message - both sinks")
        ]
        
        correlation_id = str(uuid.uuid4())
        
        for level, message in test_entries:
            getattr(logger, level.lower())(
                message,
                correlation_id=correlation_id,
                module="dual_sink_test",
                test_level=level
            )
        
        # Validate JSON sink output
        json_records = []
        with open(json_log_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    json_records.append(json.loads(line))
        
        # Should capture all 4 entries (DEBUG level and above)
        assert len(json_records) == 4
        
        # Validate console sink output
        console_lines = []
        with open(console_log_file, 'r') as f:
            console_lines = [line.strip() for line in f.readlines()]
        
        # Should capture 3 entries (INFO level and above, excluding DEBUG)
        assert len(console_lines) == 3
        
        # Validate JSON records structure
        for i, record in enumerate(json_records):
            expected_level, expected_message = test_entries[i]
            assert record["level"] == expected_level
            assert record["message"] == expected_message
            assert record["correlation_id"] == correlation_id
            assert record["test_level"] == expected_level
        
        # Validate console output format
        for line in console_lines:
            assert correlation_id in line  # Correlation ID should be present
            assert " | " in line  # Should use pipe separator format
            # Should not contain JSON-style brackets or quotes
            assert not line.startswith("{")
            assert not line.endswith("}")
    
    def test_sink_specific_formatting_differences(self, dual_sink_environment):
        """Test format differences between JSON and console sinks."""
        json_log_file, console_log_file = dual_sink_environment
        
        # Generate log entry with complex data
        complex_data = {
            "nested_object": {
                "value": 42,
                "array": [1, 2, 3],
                "boolean": True
            },
            "performance_metrics": {
                "duration_ms": 8.5,
                "memory_usage": 1024
            }
        }
        
        logger.info(
            "Complex data formatting test",
            correlation_id="format_test_001",
            module="format_validation",
            **complex_data
        )
        
        # Read JSON output
        with open(json_log_file, 'r') as f:
            json_content = f.readline().strip()
        
        json_record = json.loads(json_content)
        
        # Read console output
        with open(console_log_file, 'r') as f:
            console_content = f.readline().strip()
        
        # Validate JSON sink preserves complex data structure
        assert "nested_object" in json_record
        assert json_record["nested_object"]["value"] == 42
        assert json_record["nested_object"]["array"] == [1, 2, 3]
        assert json_record["nested_object"]["boolean"] is True
        
        assert "performance_metrics" in json_record
        assert json_record["performance_metrics"]["duration_ms"] == 8.5
        
        # Validate console sink uses human-readable format
        assert "Complex data formatting test" in console_content
        assert "correlation_id=format_test_001" in console_content
        # Console output should not contain complex JSON structures
        assert "{" not in console_content or console_content.count("{") < 3
    
    def test_sink_level_filtering_independence(self, dual_sink_environment):
        """Test independent level filtering for JSON and console sinks."""
        json_log_file, console_log_file = dual_sink_environment
        
        # Generate entries at various levels
        levels_and_messages = [
            ("TRACE", "Trace level - should not appear anywhere"),
            ("DEBUG", "Debug level - JSON only"),
            ("INFO", "Info level - both sinks"),
            ("WARNING", "Warning level - both sinks"),
            ("ERROR", "Error level - both sinks"),
            ("CRITICAL", "Critical level - both sinks")
        ]
        
        # First, add TRACE level to logger to capture trace messages
        logger.remove()
        
        # Re-setup with TRACE level for JSON
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="TRACE",  # Capture all levels
            serialize=True,
            enqueue=False
        )
        
        # Console still at INFO level
        logger.add(
            str(console_log_file),
            format="<level>{level: <8}</level> | <level>{message}</level>",
            level="INFO",
            serialize=False,
            enqueue=False
        )
        
        # Generate all log entries
        for level, message in levels_and_messages:
            getattr(logger, level.lower())(message)
        
        # Count entries in each sink
        json_count = 0
        with open(json_log_file, 'r') as f:
            for line in f:
                if line.strip():
                    json_count += 1
        
        console_count = 0
        with open(console_log_file, 'r') as f:
            for line in f:
                if line.strip():
                    console_count += 1
        
        # JSON should capture TRACE, DEBUG, INFO, WARNING, ERROR, CRITICAL (6 entries)
        # Note: Python's loguru might not support TRACE by default, so adjust expectations
        # JSON should capture DEBUG, INFO, WARNING, ERROR, CRITICAL (5 entries minimum)
        assert json_count >= 5
        
        # Console should capture INFO, WARNING, ERROR, CRITICAL (4 entries)
        assert console_count == 4
    
    def test_sink_failure_isolation(self, dual_sink_environment, tmp_path):
        """Test that failure in one sink doesn't affect the other."""
        json_log_file, console_log_file = dual_sink_environment
        
        # Create a scenario where JSON sink might fail (permission issue simulation)
        # We'll use a non-writable location for the second JSON sink
        
        logger.remove()
        
        # Setup working console sink
        logger.add(
            str(console_log_file),
            format="<level>{level: <8}</level> | <level>{message}</level>",
            level="INFO",
            serialize=False,
            enqueue=False
        )
        
        # Setup JSON sink that will work initially
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="DEBUG",
            serialize=True,
            enqueue=False
        )
        
        # Generate initial log entries (should work for both)
        logger.info("Before potential failure")
        
        # Verify both sinks received the initial entry
        assert json_log_file.exists()
        assert console_log_file.exists()
        
        # Generate additional entries after "failure" scenario
        logger.info("After potential failure handling")
        
        # At minimum, console sink should still be working
        with open(console_log_file, 'r') as f:
            console_lines = [line.strip() for line in f.readlines()]
        
        # Should have at least 2 entries in console
        assert len(console_lines) >= 2
        assert "Before potential failure" in console_lines[0]
        assert "After potential failure handling" in console_lines[1]


class TestStructuredFormatConsistency:
    """
    Test suite for structured log format consistency across deployment environments.
    
    Validates that JSON log formats remain consistent across development,
    production, CI/CD, and HPC deployment scenarios per Section 6.6.1.1
    requirements.
    """
    
    @pytest.fixture
    def multi_environment_configs(self, tmp_path):
        """Create multiple environment-specific logging configurations."""
        environments = {
            "development": {
                "sinks": {
                    "console": {
                        "enabled": True,
                        "level": "DEBUG",
                        "colorize": True,
                        "format": "enhanced"
                    },
                    "json": {
                        "enabled": True,
                        "level": "DEBUG",
                        "format": "json",
                        "rotation": "5 MB",
                        "retention": "7 days"
                    }
                }
            },
            "production": {
                "sinks": {
                    "console": {
                        "enabled": True,
                        "level": "INFO",
                        "colorize": False,
                        "format": "production"
                    },
                    "json": {
                        "enabled": True,
                        "level": "INFO",
                        "format": "json",
                        "rotation": "50 MB",
                        "retention": "90 days"
                    }
                }
            },
            "ci_cd": {
                "sinks": {
                    "console": {
                        "enabled": True,
                        "level": "WARNING",
                        "colorize": False,
                        "format": "minimal"
                    },
                    "json": {
                        "enabled": True,
                        "level": "INFO",
                        "format": "json",
                        "rotation": False,
                        "retention": False
                    }
                }
            },
            "hpc": {
                "sinks": {
                    "console": {
                        "enabled": False
                    },
                    "json": {
                        "enabled": True,
                        "level": "WARNING",
                        "format": "json",
                        "rotation": "100 MB",
                        "retention": "30 days"
                    }
                }
            }
        }
        
        config_files = {}
        
        for env_name, config in environments.items():
            config_file = tmp_path / f"logging_{env_name}.yaml"
            with open(config_file, 'w') as f:
                yaml.dump(config, f)
            config_files[env_name] = config_file
        
        return config_files
    
    def test_json_schema_consistency_across_environments(self, multi_environment_configs, tmp_path):
        """Test JSON schema consistency across different deployment environments."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Environment consistency testing requires Loguru logging")
        
        # Test log records from each environment
        environment_records = {}
        
        for env_name, config_file in multi_environment_configs.items():
            # Clear existing loggers
            logger.remove()
            
            # Load environment-specific configuration
            config = _load_logging_yaml(config_file)
            json_config = config["sinks"]["json"]
            
            if not json_config.get("enabled", False):
                continue  # Skip environments without JSON logging
            
            # Setup JSON logging for this environment
            json_output_file = tmp_path / f"{env_name}_output.json"
            
            logger.add(
                str(json_output_file),
                format=_create_json_formatter(),
                level=json_config.get("level", "INFO"),
                serialize=True,
                enqueue=False
            )
            
            # Generate test log entry
            with correlation_context(f"{env_name}_test") as ctx:
                # Add environment-specific metadata
                ctx.add_metadata(
                    environment=env_name,
                    deployment_type=env_name,
                    config_source=str(config_file)
                )
                
                # Add performance metrics
                update_cache_metrics(
                    ctx,
                    cache_hit_count=100,
                    cache_miss_count=20,
                    cache_memory_usage_mb=256.0
                )
                
                logger.bind(**ctx.bind_context()).info(
                    f"Test log entry for {env_name} environment",
                    extra={
                        "metric_type": "environment_validation",
                        "test_timestamp": time.time(),
                        "environment_config": env_name
                    }
                )
            
            # Read generated log record
            if json_output_file.exists():
                with open(json_output_file, 'r') as f:
                    log_content = f.readline().strip()
                    if log_content:
                        environment_records[env_name] = json.loads(log_content)
        
        # Validate we have records from multiple environments
        assert len(environment_records) >= 3, "Should have records from multiple environments"
        
        # Validate schema consistency across environments
        base_schema = None
        
        for env_name, record in environment_records.items():
            # Define core schema fields that should be consistent
            core_fields = {
                "timestamp", "level", "logger", "message", "correlation_id",
                "module", "metric_type", "environment", "deployment_type"
            }
            
            # Check for core fields
            record_fields = set(record.keys())
            missing_fields = core_fields - record_fields
            
            assert not missing_fields, (
                f"Environment {env_name} missing core fields: {missing_fields}"
            )
            
            # Compare schema structure across environments
            if base_schema is None:
                base_schema = record_fields
            else:
                # Core fields should be consistent
                core_intersection = base_schema & record_fields
                assert len(core_intersection) >= len(core_fields), (
                    f"Schema inconsistency in {env_name}: missing {core_fields - core_intersection}"
                )
            
            # Validate environment-specific fields
            assert record["environment"] == env_name
            assert record["deployment_type"] == env_name
            
            # Validate cache statistics structure
            if "cache_stats" in record:
                cache_stats = record["cache_stats"]
                expected_cache_fields = {
                    "cache_hit_count", "cache_miss_count", "cache_hit_rate",
                    "cache_memory_usage_mb"
                }
                cache_fields = set(cache_stats.keys())
                
                assert expected_cache_fields.issubset(cache_fields), (
                    f"Cache stats schema inconsistent in {env_name}"
                )
    
    def test_log_level_consistency_across_environments(self, multi_environment_configs, tmp_path):
        """Test log level filtering consistency across environments."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Log level testing requires Loguru logging")
        
        # Test levels that should be captured in each environment
        test_levels = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
        environment_level_counts = {}
        
        for env_name, config_file in multi_environment_configs.items():
            config = _load_logging_yaml(config_file)
            json_config = config["sinks"]["json"]
            
            if not json_config.get("enabled", False):
                continue
            
            # Clear and setup logger
            logger.remove()
            
            json_output_file = tmp_path / f"{env_name}_levels.json"
            configured_level = json_config.get("level", "INFO")
            
            logger.add(
                str(json_output_file),
                format=_create_json_formatter(),
                level=configured_level,
                serialize=True,
                enqueue=False
            )
            
            # Generate entries at all test levels
            for level in test_levels:
                getattr(logger, level.lower())(
                    f"{level} level test for {env_name}",
                    environment=env_name,
                    test_level=level
                )
            
            # Count captured entries
            level_count = 0
            if json_output_file.exists():
                with open(json_output_file, 'r') as f:
                    for line in f:
                        if line.strip():
                            level_count += 1
            
            environment_level_counts[env_name] = {
                "configured_level": configured_level,
                "captured_count": level_count
            }
        
        # Validate level filtering consistency
        for env_name, level_info in environment_level_counts.items():
            configured_level = level_info["configured_level"]
            captured_count = level_info["captured_count"]
            
            # Calculate expected count based on configured level
            level_hierarchy = ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
            level_index = level_hierarchy.index(configured_level)
            expected_count = len(level_hierarchy) - level_index
            
            assert captured_count == expected_count, (
                f"Environment {env_name} with level {configured_level} "
                f"captured {captured_count} entries, expected {expected_count}"
            )
    
    def test_metadata_preservation_across_environments(self, multi_environment_configs, tmp_path):
        """Test metadata preservation consistency across deployment environments."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Metadata testing requires Loguru logging")
        
        # Standard metadata that should be preserved in all environments
        standard_metadata = {
            "operation_type": "metadata_validation",
            "test_parameter": 42,
            "nested_data": {
                "sub_parameter": "test_value",
                "numeric_value": 3.14
            },
            "performance_data": {
                "latency_ms": 5.2,
                "throughput": 1000
            }
        }
        
        environment_metadata = {}
        
        for env_name, config_file in multi_environment_configs.items():
            config = _load_logging_yaml(config_file)
            json_config = config["sinks"]["json"]
            
            if not json_config.get("enabled", False):
                continue
            
            # Setup environment logging
            logger.remove()
            
            json_output_file = tmp_path / f"{env_name}_metadata.json"
            
            logger.add(
                str(json_output_file),
                format=_create_json_formatter(),
                level=json_config.get("level", "INFO"),
                serialize=True,
                enqueue=False
            )
            
            # Generate log with standard metadata
            logger.info(
                f"Metadata preservation test for {env_name}",
                environment=env_name,
                **standard_metadata
            )
            
            # Read and store metadata
            if json_output_file.exists():
                with open(json_output_file, 'r') as f:
                    log_content = f.readline().strip()
                    if log_content:
                        record = json.loads(log_content)
                        environment_metadata[env_name] = record
        
        # Validate metadata consistency across environments
        for env_name, record in environment_metadata.items():
            # Check standard metadata preservation
            for key, expected_value in standard_metadata.items():
                assert key in record, f"Metadata key '{key}' missing in {env_name}"
                
                if isinstance(expected_value, dict):
                    # For nested objects, check structure
                    assert isinstance(record[key], dict), (
                        f"Nested metadata '{key}' not preserved as dict in {env_name}"
                    )
                    for sub_key, sub_value in expected_value.items():
                        assert record[key][sub_key] == sub_value, (
                            f"Nested value '{key}.{sub_key}' not preserved in {env_name}"
                        )
                else:
                    assert record[key] == expected_value, (
                        f"Metadata value '{key}' not preserved in {env_name}"
                    )
        
        # Validate cross-environment consistency
        if len(environment_metadata) > 1:
            reference_env = next(iter(environment_metadata.keys()))
            reference_record = environment_metadata[reference_env]
            
            for env_name, record in environment_metadata.items():
                if env_name == reference_env:
                    continue
                
                # Compare metadata fields
                for key in standard_metadata.keys():
                    assert record[key] == reference_record[key], (
                        f"Metadata inconsistency for '{key}' between {reference_env} and {env_name}"
                    )


# Integration test for complete JSON logging workflow
class TestJSONLoggingIntegration:
    """
    Integration test suite for complete JSON logging workflow validation.
    
    Tests end-to-end JSON logging functionality including configuration loading,
    sink setup, log generation, and validation across multiple scenarios.
    """
    
    def test_complete_logging_workflow_integration(self, tmp_path):
        """Test complete JSON logging workflow from configuration to validation."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Complete workflow testing requires Loguru logging")
        
        # Create comprehensive logging configuration
        logging_config = {
            "sinks": {
                "json": {
                    "enabled": True,
                    "sink": str(tmp_path / "integration_test.json"),
                    "format": "json",
                    "level": "DEBUG",
                    "serialize": True,
                    "enqueue": False
                }
            },
            "correlation": {
                "enabled": True,
                "default_context": {
                    "correlation_id": "integration_test",
                    "module": "integration"
                }
            },
            "performance": {
                "monitoring_enabled": True,
                "thresholds": {
                    "cache_hit_rate_threshold": 0.90
                }
            }
        }
        
        config_file = tmp_path / "integration_config.yaml"
        with open(config_file, 'w') as f:
            yaml.dump(logging_config, f)
        
        # Load and apply configuration
        loaded_config = _load_logging_yaml(config_file)
        assert loaded_config is not None
        
        # Setup logging system
        logger.remove()
        
        default_context = loaded_config["correlation"]["default_context"]
        _setup_yaml_sinks(loaded_config["sinks"], default_context)
        
        # Generate comprehensive log workflow
        scenarios = [
            {
                "operation": "system_initialization",
                "metadata": {"component": "video_plume", "status": "starting"}
            },
            {
                "operation": "cache_warming", 
                "metadata": {"cache_mode": "lru", "preload_size": 1024}
            },
            {
                "operation": "agent_navigation",
                "metadata": {"agent_id": "agent_001", "position": [1.0, 2.0]}
            },
            {
                "operation": "performance_checkpoint",
                "metadata": {"step_count": 100, "fps": 30.5}
            }
        ]
        
        correlation_ids = []
        
        for scenario in scenarios:
            with correlation_context(scenario["operation"]) as ctx:
                correlation_ids.append(ctx.correlation_id)
                
                # Add scenario metadata
                ctx.add_metadata(**scenario["metadata"])
                
                # Update cache metrics for scenarios that involve caching
                if "cache" in scenario["operation"]:
                    update_cache_metrics(
                        ctx,
                        cache_hit_count=85,
                        cache_miss_count=15,
                        cache_memory_usage_mb=512.0
                    )
                
                # Generate log entry
                logger.bind(**ctx.bind_context()).info(
                    f"Integration test: {scenario['operation']}",
                    extra={
                        "metric_type": "integration_test",
                        "scenario": scenario["operation"],
                        "test_phase": "workflow_validation"
                    }
                )
        
        # Validate complete workflow output
        json_output_file = tmp_path / "integration_test.json"
        assert json_output_file.exists()
        
        # Read all log records
        log_records = []
        with open(json_output_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    log_records.append(json.loads(line))
        
        assert len(log_records) == len(scenarios)
        
        # Validate workflow integrity
        for i, record in enumerate(log_records):
            scenario = scenarios[i]
            
            # Validate basic structure
            assert record["correlation_id"] == correlation_ids[i]
            assert record["scenario"] == scenario["operation"]
            assert record["metric_type"] == "integration_test"
            
            # Validate scenario-specific metadata
            for key, value in scenario["metadata"].items():
                assert record[key] == value
            
            # Validate cache statistics for cache scenarios
            if "cache" in scenario["operation"]:
                assert "cache_stats" in record
                cache_stats = record["cache_stats"]
                assert cache_stats["cache_hit_count"] == 85
                assert cache_stats["cache_miss_count"] == 15
        
        # Validate correlation ID uniqueness
        assert len(set(correlation_ids)) == len(correlation_ids)
        
        # Validate timestamp ordering
        timestamps = [record["timestamp"] for record in log_records]
        sorted_timestamps = sorted(timestamps)
        assert timestamps == sorted_timestamps, "Log entries should be chronologically ordered"


# Performance test for JSON logging overhead
@pytest.mark.benchmark
class TestJSONLoggingPerformance:
    """Performance test suite for JSON logging overhead validation."""
    
    def test_json_logging_performance_overhead(self, tmp_path, benchmark):
        """Test JSON logging performance overhead per Section 6.6.8.2."""
        if not LOGGING_AVAILABLE:
            pytest.skip("Performance testing requires Loguru logging")
        
        # Setup high-performance JSON logging
        json_log_file = tmp_path / "performance_test.json"
        
        logger.remove()
        logger.add(
            str(json_log_file),
            format=_create_json_formatter(),
            level="INFO",
            serialize=True,
            enqueue=True,  # Use async for performance
            backtrace=False,
            diagnose=False
        )
        
        # Performance test function
        def json_logging_operation():
            with correlation_context("performance_test") as ctx:
                update_cache_metrics(
                    ctx,
                    cache_hit_count=100,
                    cache_miss_count=10,
                    cache_memory_usage_mb=256.0
                )
                
                logger.bind(**ctx.bind_context()).info(
                    "Performance test log entry",
                    metric_type="performance_benchmark",
                    test_value=42,
                    performance_data={"latency": 5.2, "throughput": 1000}
                )
        
        # Benchmark the logging operation
        result = benchmark.pedantic(
            json_logging_operation,
            rounds=10,
            iterations=100
        )
        
        # Validate performance requirements (should complete within 100ms per Section 6.6.8.2)
        assert benchmark.stats.mean < 0.1, f"JSON logging overhead too high: {benchmark.stats.mean:.3f}s"
        assert benchmark.stats.max < 0.2, f"Maximum JSON logging time too high: {benchmark.stats.max:.3f}s"


if __name__ == "__main__":
    # Run JSON logging validation tests
    pytest.main([
        __file__,
        "-v",
        "--cov=src.odor_plume_nav.utils.logging_setup",
        "--cov-report=html",
        "--cov-report=term-missing",
        "-m", "not benchmark",  # Exclude performance tests from regular runs
        "--tb=short"
    ])