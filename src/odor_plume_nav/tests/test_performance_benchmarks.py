"""
Performance benchmark test suite for odor plume navigation system.

This module provides comprehensive performance validation using pytest-benchmark to ensure
the system meets strict SLA requirements for step latency (≤10ms), cache hit rates (≥90%),
and memory usage compliance (≤2 GiB default limit). These tests serve as quality gates
for performance regression prevention in CI/CD pipelines.

The benchmark suite validates:
- Sub-10ms step execution with frame cache optimization
- Cache hit-rate performance achieving ≥90% target with sequential access
- Memory consumption benchmarks ensuring ≤2 GiB default limit compliance  
- Frame retrieval latency targeting ≤1ms from cache
- No memory leaks detected over 1M+ steps
- Performance SLA validation across reference CPU hardware
- Statistical significance testing for regression detection

Performance Requirements:
- Environment step() execution: ≤10ms on reference CPU (Intel i7-8700K equivalent)
- Frame cache hit rate: ≥90% for sequential access patterns
- Process memory usage: ≤2 GiB under default configuration
- Frame retrieval from cache: ≤1ms average latency
- Training throughput: ≥1M steps/hour sustained performance

Architecture Integration:
- Integrates with pytest-benchmark for microsecond-precision timing
- Provides merge-blocking quality gates for CI/CD pipeline
- Supports reference hardware performance baselines
- Enables automated performance regression detection
- Correlates with structured Loguru logging for observability

Authors: Generated by Blitzy Platform
Version: 2.0.0
License: MIT
"""

import gc
import os
import sys
import time
import warnings
import psutil
import threading
from typing import Dict, List, Tuple, Optional, Any, Generator
from pathlib import Path
from contextlib import contextmanager
from unittest.mock import Mock, patch, MagicMock
from dataclasses import dataclass, field

import pytest
import numpy as np

# Core testing framework imports
try:
    import pytest_benchmark
    from pytest_benchmark.fixture import BenchmarkFixture
    BENCHMARK_AVAILABLE = True
except ImportError:
    BENCHMARK_AVAILABLE = False
    BenchmarkFixture = None

# Statistical analysis for performance regression detection
try:
    from scipy import stats
    import scipy.stats as scipy_stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    warnings.warn("SciPy not available. Statistical significance testing disabled.", ImportWarning)

# Memory profiling capabilities
try:
    import memory_profiler
    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False
    warnings.warn("memory_profiler not available. Advanced memory profiling disabled.", ImportWarning)

# Import components under test with graceful degradation
try:
    from odor_plume_nav.cache.frame_cache import FrameCache
    FRAME_CACHE_AVAILABLE = True
except ImportError:
    FRAME_CACHE_AVAILABLE = False
    FrameCache = None

try:
    from odor_plume_nav.environments.gymnasium_env import GymnasiumEnv
    GYMNASIUM_ENV_AVAILABLE = True
except ImportError:
    GYMNASIUM_ENV_AVAILABLE = False
    GymnasiumEnv = None

# Enhanced logging and performance monitoring
try:
    from odor_plume_nav.utils.logging_setup import (
        get_enhanced_logger, PerformanceMetrics, correlation_context
    )
    logger = get_enhanced_logger(__name__)
    ENHANCED_LOGGING_AVAILABLE = True
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
    ENHANCED_LOGGING_AVAILABLE = False


# ================================================================================================
# PERFORMANCE CONSTANTS AND THRESHOLDS
# ================================================================================================

# Performance SLA Requirements (from Section 6.6.4.1.1)
STEP_LATENCY_THRESHOLD_MS = 10.0        # ≤10ms step execution requirement
FRAME_RETRIEVAL_THRESHOLD_MS = 1.0      # ≤1ms cache retrieval target
CACHE_HIT_RATE_THRESHOLD = 0.90         # ≥90% hit rate requirement
DEFAULT_MEMORY_LIMIT_GIB = 2.0          # ≤2 GiB default memory limit
SIMULATION_FPS_THRESHOLD = 30.0         # ≥30 FPS simulation requirement
TRAINING_THROUGHPUT_THRESHOLD = 1000000 # ≥1M steps/hour requirement

# Reference hardware specifications for consistent benchmarks
REFERENCE_CPU_CORES = 8                 # Intel i7-8700K equivalent
REFERENCE_MEMORY_GB = 16                # Standard test configuration
REFERENCE_DISK_TYPE = "NVMe SSD"        # For deterministic I/O performance

# Statistical analysis parameters
STATISTICAL_SIGNIFICANCE_LEVEL = 0.05   # 95% confidence for regression detection
BENCHMARK_ITERATIONS = 10               # Minimum iterations for statistical validity
BENCHMARK_ROUNDS = 5                    # Number of rounds for variance analysis
MEMORY_LEAK_DETECTION_STEPS = 10000     # Steps for memory leak validation

# Performance regression detection thresholds
PERFORMANCE_REGRESSION_THRESHOLD = 0.15  # 15% performance degradation threshold
MEMORY_GROWTH_THRESHOLD = 0.10          # 10% memory growth tolerance


@dataclass
class PerformanceBenchmarkConfig:
    """Configuration for performance benchmark execution."""
    
    # Benchmark execution parameters
    iterations: int = BENCHMARK_ITERATIONS
    rounds: int = BENCHMARK_ROUNDS
    warmup_iterations: int = 5
    
    # Performance thresholds
    step_latency_ms: float = STEP_LATENCY_THRESHOLD_MS
    frame_retrieval_ms: float = FRAME_RETRIEVAL_THRESHOLD_MS
    cache_hit_rate: float = CACHE_HIT_RATE_THRESHOLD
    memory_limit_gib: float = DEFAULT_MEMORY_LIMIT_GIB
    
    # Statistical analysis
    significance_level: float = STATISTICAL_SIGNIFICANCE_LEVEL
    regression_threshold: float = PERFORMANCE_REGRESSION_THRESHOLD
    
    # Test data configuration
    video_frame_count: int = 1000
    video_width: int = 640
    video_height: int = 480
    access_pattern_length: int = 1000
    
    # Memory leak detection
    memory_leak_steps: int = MEMORY_LEAK_DETECTION_STEPS
    memory_growth_threshold: float = MEMORY_GROWTH_THRESHOLD


@dataclass 
class PerformanceMetricsSnapshot:
    """Snapshot of performance metrics for analysis."""
    
    timestamp: float = field(default_factory=time.time)
    step_latency_ms: float = 0.0
    frame_retrieval_ms: float = 0.0
    cache_hit_rate: float = 0.0
    cache_hits: int = 0
    cache_misses: int = 0
    memory_usage_mb: float = 0.0
    process_memory_mb: float = 0.0
    fps_estimate: float = 0.0
    
    # Statistical properties
    step_count: int = 0
    episode_count: int = 0
    correlation_id: Optional[str] = None


class PerformanceRegressionDetector:
    """Statistical performance regression detection with confidence intervals."""
    
    def __init__(self, significance_level: float = STATISTICAL_SIGNIFICANCE_LEVEL):
        self.significance_level = significance_level
        self.baseline_metrics: List[PerformanceMetricsSnapshot] = []
        self.current_metrics: List[PerformanceMetricsSnapshot] = []
    
    def add_baseline_metric(self, metric: PerformanceMetricsSnapshot) -> None:
        """Add baseline performance metric for comparison."""
        self.baseline_metrics.append(metric)
    
    def add_current_metric(self, metric: PerformanceMetricsSnapshot) -> None:
        """Add current performance metric for regression analysis."""
        self.current_metrics.append(metric)
    
    def detect_regression(self, metric_name: str) -> Tuple[bool, float, str]:
        """
        Detect performance regression using statistical significance testing.
        
        Args:
            metric_name: Name of metric to analyze (e.g., 'step_latency_ms')
            
        Returns:
            Tuple of (has_regression, p_value, analysis_summary)
        """
        if not SCIPY_AVAILABLE:
            logger.warning("SciPy not available, using simple threshold comparison")
            return self._simple_threshold_check(metric_name)
        
        if len(self.baseline_metrics) < 3 or len(self.current_metrics) < 3:
            return False, 1.0, "Insufficient data for statistical analysis"
        
        # Extract metric values for analysis
        baseline_values = [getattr(m, metric_name) for m in self.baseline_metrics]
        current_values = [getattr(m, metric_name) for m in self.current_metrics]
        
        # Perform two-sample t-test for performance comparison
        try:
            statistic, p_value = scipy_stats.ttest_ind(
                baseline_values, current_values, 
                alternative='less'  # Test if current is significantly worse
            )
            
            baseline_mean = np.mean(baseline_values)
            current_mean = np.mean(current_values)
            percent_change = ((current_mean - baseline_mean) / baseline_mean) * 100
            
            has_regression = (
                p_value < self.significance_level and 
                percent_change > PERFORMANCE_REGRESSION_THRESHOLD * 100
            )
            
            analysis_summary = (
                f"Statistical analysis: baseline_mean={baseline_mean:.3f}, "
                f"current_mean={current_mean:.3f}, percent_change={percent_change:.1f}%, "
                f"p_value={p_value:.4f}, significant={has_regression}"
            )
            
            return has_regression, p_value, analysis_summary
            
        except Exception as e:
            logger.error(f"Statistical analysis failed for {metric_name}: {e}")
            return self._simple_threshold_check(metric_name)
    
    def _simple_threshold_check(self, metric_name: str) -> Tuple[bool, float, str]:
        """Fallback threshold-based regression detection."""
        if not self.baseline_metrics or not self.current_metrics:
            return False, 1.0, "No data available for comparison"
        
        baseline_values = [getattr(m, metric_name) for m in self.baseline_metrics]
        current_values = [getattr(m, metric_name) for m in self.current_metrics]
        
        baseline_mean = np.mean(baseline_values)
        current_mean = np.mean(current_values)
        percent_change = ((current_mean - baseline_mean) / baseline_mean) * 100
        
        has_regression = percent_change > PERFORMANCE_REGRESSION_THRESHOLD * 100
        
        analysis_summary = (
            f"Threshold analysis: baseline_mean={baseline_mean:.3f}, "
            f"current_mean={current_mean:.3f}, percent_change={percent_change:.1f}%, "
            f"threshold_exceeded={has_regression}"
        )
        
        return has_regression, 0.5, analysis_summary


# ================================================================================================
# PERFORMANCE TESTING FIXTURES
# ================================================================================================

@pytest.fixture(scope="session")
def performance_config():
    """Session-scoped performance benchmark configuration."""
    config = PerformanceBenchmarkConfig()
    
    # Adjust configuration based on environment
    if os.getenv("CI") == "true":
        # Reduce iterations in CI to balance accuracy with execution time
        config.iterations = max(5, config.iterations // 2)
        config.rounds = max(3, config.rounds - 1)
        config.memory_leak_steps = config.memory_leak_steps // 2
    
    # Override with environment variables if present
    config.step_latency_ms = float(os.getenv("PERF_STEP_LATENCY_MS", config.step_latency_ms))
    config.cache_hit_rate = float(os.getenv("PERF_CACHE_HIT_RATE", config.cache_hit_rate))
    config.memory_limit_gib = float(os.getenv("PERF_MEMORY_LIMIT_GIB", config.memory_limit_gib))
    
    logger.info(f"Performance benchmark configuration: {config}")
    return config


@pytest.fixture
def performance_monitor():
    """Performance monitoring context manager for detailed metrics collection."""
    
    @contextmanager
    def monitor_performance(operation_name: str) -> Generator[PerformanceMetricsSnapshot, None, None]:
        """Context manager for monitoring operation performance."""
        # Get initial process state
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # Create metrics snapshot
        metrics = PerformanceMetricsSnapshot()
        metrics.process_memory_mb = initial_memory
        
        start_time = time.perf_counter()
        
        try:
            yield metrics
        finally:
            # Calculate final metrics
            end_time = time.perf_counter()
            final_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            metrics.step_latency_ms = (end_time - start_time) * 1000
            metrics.process_memory_mb = final_memory
            metrics.fps_estimate = 1.0 / (end_time - start_time) if (end_time - start_time) > 0 else float('inf')
            
            # Log performance metrics if enhanced logging available
            if ENHANCED_LOGGING_AVAILABLE:
                logger.debug(
                    f"Performance monitoring complete for {operation_name}",
                    extra={
                        "operation": operation_name,
                        "step_latency_ms": metrics.step_latency_ms,
                        "memory_mb": metrics.process_memory_mb,
                        "fps_estimate": metrics.fps_estimate,
                        "metric_type": "performance_benchmark"
                    }
                )
    
    return monitor_performance


@pytest.fixture
def mock_frame_cache():
    """Mock frame cache with realistic performance characteristics."""
    if not FRAME_CACHE_AVAILABLE:
        pytest.skip("FrameCache module not available")
    
    # Create actual FrameCache instance for realistic testing
    cache = FrameCache(mode="lru", max_memory_mb=100)  # Small cache for testing
    
    # Pre-populate with some test frames to simulate realistic hit rates
    test_frames = []
    for i in range(50):
        frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        test_frames.append(frame)
        cache.put(i, frame)
    
    yield cache
    
    # Cleanup
    cache.clear()


@pytest.fixture
def mock_gymnasium_env(mock_frame_cache, tmp_path):
    """Mock Gymnasium environment with frame cache integration."""
    if not GYMNASIUM_ENV_AVAILABLE:
        pytest.skip("GymnasiumEnv not available")
    
    # Create minimal test video file
    test_video = tmp_path / "test_video.mp4"
    test_video.write_bytes(b"MOCK_VIDEO_DATA")
    
    # Mock environment with performance optimizations
    with patch('odor_plume_nav.environments.gymnasium_env.VideoPlume') as mock_video_plume:
        # Configure video plume mock
        mock_plume_instance = Mock()
        mock_plume_instance.get_frame.return_value = np.random.randint(
            0, 255, (480, 640, 3), dtype=np.uint8
        )
        mock_plume_instance.get_metadata.return_value = {
            'width': 640, 'height': 480, 'fps': 30.0, 'frame_count': 1000
        }
        mock_video_plume.return_value = mock_plume_instance
        
        # Create environment with frame cache
        env = GymnasiumEnv(
            video_path=str(test_video),
            frame_cache=mock_frame_cache,
            performance_monitoring=True,
            max_episode_steps=100  # Shorter episodes for testing
        )
        
        yield env
        
        # Cleanup
        env.close()


@pytest.fixture
def regression_detector():
    """Performance regression detector for statistical analysis."""
    return PerformanceRegressionDetector()


@pytest.fixture
def memory_monitor():
    """Memory usage monitoring for leak detection."""
    
    @contextmanager
    def monitor_memory(operation_name: str) -> Generator[Dict[str, float], None, None]:
        """Monitor memory usage during operation execution."""
        process = psutil.Process()
        
        # Force garbage collection before measurement
        gc.collect()
        
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        peak_memory = initial_memory
        
        memory_stats = {
            "initial_mb": initial_memory,
            "peak_mb": initial_memory,
            "final_mb": initial_memory,
            "growth_mb": 0.0,
            "growth_percent": 0.0
        }
        
        def update_peak():
            nonlocal peak_memory
            current_memory = process.memory_info().rss / 1024 / 1024
            peak_memory = max(peak_memory, current_memory)
        
        # Start memory monitoring thread for peak detection
        monitor_thread = None
        monitoring = threading.Event()
        
        def memory_monitor_worker():
            while not monitoring.is_set():
                update_peak()
                time.sleep(0.1)  # Sample every 100ms
        
        try:
            monitor_thread = threading.Thread(target=memory_monitor_worker, daemon=True)
            monitor_thread.start()
            
            yield memory_stats
            
        finally:
            # Stop monitoring
            monitoring.set()
            if monitor_thread:
                monitor_thread.join(timeout=1.0)
            
            # Final memory measurement
            gc.collect()
            final_memory = process.memory_info().rss / 1024 / 1024
            
            # Update statistics
            memory_stats.update({
                "final_mb": final_memory,
                "peak_mb": peak_memory,
                "growth_mb": final_memory - initial_memory,
                "growth_percent": ((final_memory - initial_memory) / initial_memory) * 100
            })
            
            logger.debug(
                f"Memory monitoring complete for {operation_name}",
                extra={
                    "operation": operation_name,
                    "memory_stats": memory_stats,
                    "metric_type": "memory_benchmark"
                }
            )
    
    return monitor_memory


# ================================================================================================
# STEP LATENCY PERFORMANCE BENCHMARKS
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.performance
class TestStepLatencyBenchmarks:
    """Comprehensive step execution latency benchmarks with cache optimization."""
    
    def test_step_execution_under_10ms_baseline(self, benchmark: BenchmarkFixture, mock_gymnasium_env, performance_config):
        """Validate step execution maintains <10ms baseline without cache optimization."""
        if not BENCHMARK_AVAILABLE:
            pytest.skip("pytest-benchmark not available")
        
        env = mock_gymnasium_env
        env.reset()
        
        def step_execution():
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            return obs
        
        # Benchmark step execution
        result = benchmark.pedantic(
            step_execution, 
            rounds=performance_config.rounds, 
            iterations=performance_config.iterations
        )
        
        # Validate performance requirement
        step_time_ms = benchmark.stats.mean * 1000
        assert step_time_ms < performance_config.step_latency_ms, (
            f"Step execution took {step_time_ms:.2f}ms, "
            f"should be <{performance_config.step_latency_ms}ms"
        )
        
        # Ensure no extreme outliers
        max_time_ms = benchmark.stats.max * 1000
        assert max_time_ms < performance_config.step_latency_ms * 1.5, (
            f"Maximum step time {max_time_ms:.2f}ms exceeds tolerance"
        )
        
        logger.info(
            f"Step latency benchmark passed: {step_time_ms:.2f}ms mean, "
            f"{max_time_ms:.2f}ms max",
            extra={
                "benchmark_name": "step_execution_baseline",
                "mean_ms": step_time_ms,
                "max_ms": max_time_ms,
                "threshold_ms": performance_config.step_latency_ms,
                "metric_type": "step_latency_benchmark"
            }
        )
    
    def test_step_execution_with_cache_optimization(self, benchmark: BenchmarkFixture, mock_gymnasium_env, performance_config):
        """Validate step execution with frame cache maintains <10ms performance target."""
        if not BENCHMARK_AVAILABLE:
            pytest.skip("pytest-benchmark not available")
        
        env = mock_gymnasium_env
        env.reset()
        
        # Warm up cache with sequential access pattern
        for i in range(20):
            action = env.action_space.sample()
            env.step(action)
        
        def step_execution_cached():
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            return obs, info
        
        # Benchmark cached step execution
        result = benchmark.pedantic(
            step_execution_cached,
            rounds=performance_config.rounds,
            iterations=performance_config.iterations
        )
        
        # Validate performance requirement
        step_time_ms = benchmark.stats.mean * 1000
        assert step_time_ms < performance_config.step_latency_ms, (
            f"Cached step execution took {step_time_ms:.2f}ms, "
            f"should be <{performance_config.step_latency_ms}ms"
        )
        
        # Verify cache efficiency - cached steps should be faster
        cache_overhead_ms = benchmark.stats.stddev * 1000
        assert cache_overhead_ms < 2.0, (
            f"Cache introduces too much variance: {cache_overhead_ms:.2f}ms stddev"
        )
        
        logger.info(
            f"Cached step latency benchmark passed: {step_time_ms:.2f}ms mean, "
            f"{cache_overhead_ms:.2f}ms stddev",
            extra={
                "benchmark_name": "step_execution_cached",
                "mean_ms": step_time_ms,
                "stddev_ms": cache_overhead_ms,
                "threshold_ms": performance_config.step_latency_ms,
                "metric_type": "cached_step_latency_benchmark"
            }
        )
    
    def test_step_latency_statistical_consistency(self, mock_gymnasium_env, regression_detector, performance_config):
        """Validate step latency statistical consistency for regression detection."""
        env = mock_gymnasium_env
        env.reset()
        
        # Collect baseline performance measurements
        baseline_metrics = []
        for i in range(performance_config.iterations * 2):
            start_time = time.perf_counter()
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            end_time = time.perf_counter()
            
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = (end_time - start_time) * 1000
            metric.step_count = i + 1
            baseline_metrics.append(metric)
            
            regression_detector.add_baseline_metric(metric)
        
        # Simulate current measurements
        current_metrics = []
        for i in range(performance_config.iterations):
            start_time = time.perf_counter()
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            end_time = time.perf_counter()
            
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = (end_time - start_time) * 1000
            metric.step_count = len(baseline_metrics) + i + 1
            current_metrics.append(metric)
            
            regression_detector.add_current_metric(metric)
        
        # Analyze statistical consistency
        has_regression, p_value, analysis = regression_detector.detect_regression("step_latency_ms")
        
        # Should not detect regression in normal operation
        assert not has_regression, f"False positive regression detected: {analysis}"
        
        # Validate statistical properties
        baseline_latencies = [m.step_latency_ms for m in baseline_metrics]
        current_latencies = [m.step_latency_ms for m in current_metrics]
        
        baseline_mean = np.mean(baseline_latencies)
        current_mean = np.mean(current_latencies)
        
        assert baseline_mean < performance_config.step_latency_ms, (
            f"Baseline performance violates SLA: {baseline_mean:.2f}ms"
        )
        assert current_mean < performance_config.step_latency_ms, (
            f"Current performance violates SLA: {current_mean:.2f}ms"
        )
        
        logger.info(
            f"Step latency statistical consistency validated: "
            f"baseline={baseline_mean:.2f}ms, current={current_mean:.2f}ms, "
            f"p_value={p_value:.4f}",
            extra={
                "baseline_mean_ms": baseline_mean,
                "current_mean_ms": current_mean,
                "p_value": p_value,
                "has_regression": has_regression,
                "metric_type": "step_latency_consistency"
            }
        )


# ================================================================================================
# CACHE PERFORMANCE BENCHMARKS
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.cache
class TestCachePerformanceBenchmarks:
    """Comprehensive frame cache performance validation and optimization testing."""
    
    def test_cache_hit_rate_sequential_access(self, mock_frame_cache, performance_config):
        """Validate cache achieves ≥90% hit rate with sequential access patterns."""
        if not FRAME_CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        cache = mock_frame_cache
        
        # Generate sequential access pattern with some repetition (realistic for RL)
        access_pattern = []
        for i in range(performance_config.access_pattern_length):
            # 80% sequential, 20% random previous frames for realistic pattern
            if np.random.random() < 0.8:
                frame_id = i % 100  # Sequential with wraparound
            else:
                frame_id = np.random.randint(0, min(i + 1, 100))  # Random previous frame
            access_pattern.append(frame_id)
        
        # Execute access pattern
        hits = 0
        misses = 0
        for frame_id in access_pattern:
            frame = cache.get(frame_id)
            if frame is not None:
                hits += 1
            else:
                misses += 1
                # Simulate cache miss by storing a new frame
                test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                cache.put(frame_id, test_frame)
        
        # Calculate hit rate
        total_accesses = hits + misses
        hit_rate = hits / total_accesses if total_accesses > 0 else 0.0
        
        # Validate hit rate requirement
        assert hit_rate >= performance_config.cache_hit_rate, (
            f"Cache hit rate {hit_rate:.2%} below target "
            f"{performance_config.cache_hit_rate:.2%}"
        )
        
        # Additional cache health checks
        cache_size = getattr(cache, 'size', len(access_pattern))
        assert cache_size > 0, "Cache should contain frames after access pattern"
        
        memory_usage_mb = getattr(cache, 'memory_usage_mb', 0.0)
        assert memory_usage_mb > 0, "Cache should report memory usage"
        
        logger.info(
            f"Cache hit rate benchmark passed: {hit_rate:.2%} "
            f"({hits} hits, {misses} misses, {cache_size} cached frames)",
            extra={
                "hit_rate": hit_rate,
                "hits": hits,
                "misses": misses,
                "cache_size": cache_size,
                "memory_usage_mb": memory_usage_mb,
                "threshold": performance_config.cache_hit_rate,
                "metric_type": "cache_hit_rate_benchmark"
            }
        )
    
    def test_frame_retrieval_latency_from_cache(self, benchmark: BenchmarkFixture, mock_frame_cache, performance_config):
        """Validate frame retrieval from cache achieves ≤1ms target latency."""
        if not BENCHMARK_AVAILABLE or not FRAME_CACHE_AVAILABLE:
            pytest.skip("Required components not available")
        
        cache = mock_frame_cache
        
        # Pre-populate cache with test frames
        test_frame_ids = list(range(50))
        for frame_id in test_frame_ids:
            test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            cache.put(frame_id, test_frame)
        
        def cached_frame_retrieval():
            # Retrieve random cached frame
            frame_id = np.random.choice(test_frame_ids)
            frame = cache.get(frame_id)
            return frame
        
        # Benchmark frame retrieval
        result = benchmark.pedantic(
            cached_frame_retrieval,
            rounds=performance_config.rounds,
            iterations=performance_config.iterations * 10  # More iterations for cache ops
        )
        
        # Validate retrieval latency requirement
        retrieval_time_ms = benchmark.stats.mean * 1000
        assert retrieval_time_ms < performance_config.frame_retrieval_ms, (
            f"Frame retrieval took {retrieval_time_ms:.3f}ms, "
            f"should be <{performance_config.frame_retrieval_ms}ms"
        )
        
        # Ensure consistent performance
        max_time_ms = benchmark.stats.max * 1000
        assert max_time_ms < performance_config.frame_retrieval_ms * 2, (
            f"Maximum retrieval time {max_time_ms:.3f}ms too variable"
        )
        
        logger.info(
            f"Frame retrieval latency benchmark passed: {retrieval_time_ms:.3f}ms mean, "
            f"{max_time_ms:.3f}ms max",
            extra={
                "mean_ms": retrieval_time_ms,
                "max_ms": max_time_ms,
                "threshold_ms": performance_config.frame_retrieval_ms,
                "metric_type": "frame_retrieval_latency_benchmark"
            }
        )
    
    def test_cache_memory_boundary_compliance(self, performance_config):
        """Validate cache respects memory boundaries under variable sizing."""
        if not FRAME_CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Test different cache sizes to validate memory boundary compliance
        test_sizes_mb = [10, 50, 100, 200]  # Various cache sizes to test
        
        for cache_size_mb in test_sizes_mb:
            max_memory_bytes = cache_size_mb * 1024 * 1024
            
            # Create cache with specific memory limit
            cache = FrameCache(mode="lru", max_memory_mb=cache_size_mb)
            
            try:
                # Fill cache beyond capacity to trigger eviction
                frame_size_bytes = 480 * 640 * 3  # RGB frame size
                frames_to_add = (max_memory_bytes // frame_size_bytes) + 50  # Exceed capacity
                
                for i in range(frames_to_add):
                    test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                    cache.put(i, test_frame)
                    
                    # Check memory usage doesn't exceed limit
                    current_memory_mb = getattr(cache, 'memory_usage_mb', 0.0)
                    assert current_memory_mb <= cache_size_mb * 1.1, (  # 10% tolerance for overhead
                        f"Cache memory usage {current_memory_mb:.1f}MB exceeds "
                        f"limit {cache_size_mb}MB"
                    )
                    
                    # Ensure cache remains operational
                    assert hasattr(cache, 'get'), "Cache should remain functional"
                
                # Validate final state
                final_memory_mb = getattr(cache, 'memory_usage_mb', 0.0)
                cache_size = getattr(cache, 'size', 0)
                
                assert final_memory_mb <= cache_size_mb * 1.1, (
                    f"Final memory usage {final_memory_mb:.1f}MB exceeds limit"
                )
                assert cache_size > 0, "Cache should contain frames after filling"
                
                logger.debug(
                    f"Cache memory boundary test passed for {cache_size_mb}MB: "
                    f"final_memory={final_memory_mb:.1f}MB, size={cache_size}",
                    extra={
                        "cache_limit_mb": cache_size_mb,
                        "final_memory_mb": final_memory_mb,
                        "cache_size": cache_size,
                        "metric_type": "cache_memory_boundary"
                    }
                )
                
            finally:
                # Cleanup cache
                cache.clear()
        
        logger.info(
            f"Cache memory boundary compliance validated for sizes: {test_sizes_mb}MB",
            extra={
                "test_sizes_mb": test_sizes_mb,
                "metric_type": "cache_memory_compliance"
            }
        )
    
    def test_cache_eviction_policy_correctness(self, performance_config):
        """Validate LRU cache eviction policy maintains correctness under arbitrary access patterns."""
        if not FRAME_CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        # Create small cache to force evictions
        cache = FrameCache(mode="lru", max_memory_mb=5)  # Small cache for testing
        
        # Track access order for LRU validation
        access_history = []
        frame_data = {}
        
        # Generate access pattern with known LRU behavior
        access_pattern = []
        for i in range(100):
            if i < 20:
                # Initial population phase
                frame_id = i
            elif i < 60:
                # Mix of new frames and recent accesses
                frame_id = np.random.choice([i, i-1, i-2, i-5, i-10])
            else:
                # Focus on recent frames to validate LRU
                frame_id = np.random.choice(list(range(max(0, i-10), i+1)))
            
            access_pattern.append(frame_id)
        
        # Execute access pattern with LRU tracking
        for frame_id in access_pattern:
            # Try to get frame
            frame = cache.get(frame_id)
            
            if frame is None:
                # Cache miss - add new frame
                test_frame = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                cache.put(frame_id, test_frame)
                frame_data[frame_id] = test_frame
            
            # Update access history
            if frame_id in access_history:
                access_history.remove(frame_id)
            access_history.append(frame_id)
        
        # Validate cache contains most recently accessed frames
        cache_size = getattr(cache, 'size', 0)
        recent_frames = access_history[-cache_size:] if cache_size > 0 else []
        
        # Check that cache contains expected frames (allowing for eviction variations)
        for frame_id in recent_frames[-10:]:  # Check last 10 accessed frames
            frame = cache.get(frame_id)
            if frame is not None:
                # Validate frame data integrity
                assert isinstance(frame, np.ndarray), "Retrieved frame should be numpy array"
                assert frame.shape == (480, 640, 3), "Frame should have correct dimensions"
        
        # Validate cache statistics
        total_hits = getattr(cache, 'hits', 0)
        total_misses = getattr(cache, 'misses', 0)
        hit_rate = total_hits / (total_hits + total_misses) if (total_hits + total_misses) > 0 else 0.0
        
        # Hit rate should be reasonable for LRU with this access pattern
        assert hit_rate > 0.3, f"Hit rate {hit_rate:.2%} too low for LRU cache"
        
        logger.info(
            f"LRU eviction policy correctness validated: hit_rate={hit_rate:.2%}, "
            f"cache_size={cache_size}, recent_frames={len(recent_frames)}",
            extra={
                "hit_rate": hit_rate,
                "cache_size": cache_size,
                "total_accesses": len(access_pattern),
                "hits": total_hits,
                "misses": total_misses,
                "metric_type": "lru_eviction_correctness"
            }
        )


# ================================================================================================
# MEMORY USAGE BENCHMARKS
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.memory
class TestMemoryUsageBenchmarks:
    """Comprehensive memory usage validation and leak detection testing."""
    
    def test_process_memory_under_default_limit(self, mock_gymnasium_env, memory_monitor, performance_config):
        """Validate process memory usage stays within ≤2 GiB default limit during operation."""
        env = mock_gymnasium_env
        
        with memory_monitor("sustained_operation") as memory_stats:
            # Reset environment
            env.reset()
            
            # Run sustained operation to check memory usage
            for step in range(performance_config.memory_leak_steps // 10):  # Shorter test
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                
                if terminated or truncated:
                    env.reset()
                
                # Check memory periodically during operation
                if step % 100 == 0:
                    current_memory_gib = memory_stats["peak_mb"] / 1024
                    assert current_memory_gib < performance_config.memory_limit_gib, (
                        f"Memory usage {current_memory_gib:.2f} GiB exceeds "
                        f"limit {performance_config.memory_limit_gib} GiB at step {step}"
                    )
        
        # Validate final memory usage
        final_memory_gib = memory_stats["final_mb"] / 1024
        peak_memory_gib = memory_stats["peak_mb"] / 1024
        
        assert final_memory_gib < performance_config.memory_limit_gib, (
            f"Final memory usage {final_memory_gib:.2f} GiB exceeds limit"
        )
        assert peak_memory_gib < performance_config.memory_limit_gib, (
            f"Peak memory usage {peak_memory_gib:.2f} GiB exceeds limit"
        )
        
        logger.info(
            f"Process memory limit compliance validated: "
            f"final={final_memory_gib:.2f} GiB, peak={peak_memory_gib:.2f} GiB, "
            f"limit={performance_config.memory_limit_gib} GiB",
            extra={
                "final_memory_gib": final_memory_gib,
                "peak_memory_gib": peak_memory_gib,
                "memory_limit_gib": performance_config.memory_limit_gib,
                "memory_stats": memory_stats,
                "metric_type": "memory_limit_compliance"
            }
        )
    
    def test_memory_leak_detection_extended_operation(self, mock_gymnasium_env, memory_monitor, performance_config):
        """Validate no memory leaks detected over extended operation (1M+ steps simulation)."""
        env = mock_gymnasium_env
        
        # Use shorter test in CI/testing, but validate growth rate
        test_steps = min(performance_config.memory_leak_steps, 5000)
        
        with memory_monitor("memory_leak_detection") as memory_stats:
            env.reset()
            
            # Collect memory samples at intervals
            memory_samples = []
            sample_interval = max(1, test_steps // 20)  # 20 samples
            
            for step in range(test_steps):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                
                if terminated or truncated:
                    env.reset()
                
                # Sample memory usage
                if step % sample_interval == 0:
                    current_memory_mb = psutil.Process().memory_info().rss / 1024 / 1024
                    memory_samples.append((step, current_memory_mb))
        
        # Analyze memory growth trend
        if len(memory_samples) >= 3:
            steps = [s[0] for s in memory_samples]
            memory_values = [s[1] for s in memory_samples]
            
            # Linear regression to detect memory growth trend
            if SCIPY_AVAILABLE:
                slope, intercept, r_value, p_value, std_err = scipy_stats.linregress(steps, memory_values)
                
                # Calculate projected memory growth over full simulation
                projected_growth_mb = slope * 1000000  # Growth over 1M steps
                growth_rate_mb_per_1k_steps = slope * 1000
                
                # Validate acceptable memory growth
                acceptable_growth_mb = memory_stats["initial_mb"] * performance_config.memory_growth_threshold
                
                assert abs(projected_growth_mb) < acceptable_growth_mb, (
                    f"Projected memory growth {projected_growth_mb:.1f}MB over 1M steps "
                    f"exceeds acceptable threshold {acceptable_growth_mb:.1f}MB"
                )
                
                logger.info(
                    f"Memory leak detection passed: growth_rate={growth_rate_mb_per_1k_steps:.3f}MB/1K steps, "
                    f"projected_1M_steps={projected_growth_mb:.1f}MB, r²={r_value**2:.3f}",
                    extra={
                        "growth_rate_mb_per_1k_steps": growth_rate_mb_per_1k_steps,
                        "projected_growth_mb": projected_growth_mb,
                        "r_squared": r_value**2,
                        "acceptable_threshold_mb": acceptable_growth_mb,
                        "test_steps": test_steps,
                        "metric_type": "memory_leak_detection"
                    }
                )
            else:
                # Simple growth check without regression analysis
                initial_memory = memory_samples[0][1]
                final_memory = memory_samples[-1][1]
                growth_mb = final_memory - initial_memory
                growth_percent = (growth_mb / initial_memory) * 100
                
                assert growth_percent < performance_config.memory_growth_threshold * 100, (
                    f"Memory growth {growth_percent:.1f}% exceeds threshold "
                    f"{performance_config.memory_growth_threshold * 100:.1f}%"
                )
                
                logger.info(
                    f"Memory leak detection passed (simple): growth={growth_mb:.1f}MB ({growth_percent:.1f}%)",
                    extra={
                        "memory_growth_mb": growth_mb,
                        "memory_growth_percent": growth_percent,
                        "test_steps": test_steps,
                        "metric_type": "simple_memory_leak_detection"
                    }
                )
    
    def test_cache_memory_cleanup_on_environment_close(self, mock_gymnasium_env, memory_monitor):
        """Validate cache memory is properly cleaned up when environment is closed."""
        if not FRAME_CACHE_AVAILABLE:
            pytest.skip("FrameCache not available")
        
        with memory_monitor("cache_cleanup") as memory_stats:
            env = mock_gymnasium_env
            env.reset()
            
            # Populate cache with frames
            for i in range(100):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                if terminated or truncated:
                    env.reset()
            
            # Check cache has data
            cache_stats = env.get_cache_stats()
            initial_cache_size = cache_stats.get("size", 0)
            assert initial_cache_size > 0, "Cache should contain frames before cleanup"
            
            # Close environment (should trigger cache cleanup)
            env.close()
            
            # Force garbage collection
            gc.collect()
        
        # Validate memory cleanup occurred
        memory_freed_mb = memory_stats["peak_mb"] - memory_stats["final_mb"]
        
        # Should have freed some memory from cache cleanup
        assert memory_freed_mb >= 0, "Memory should be freed or remain stable after cleanup"
        
        # Verify cache was actually cleared
        final_cache_stats = env.get_cache_stats()
        final_cache_size = final_cache_stats.get("size", 0)
        
        logger.info(
            f"Cache memory cleanup validated: freed={memory_freed_mb:.1f}MB, "
            f"cache_size: {initial_cache_size} -> {final_cache_size}",
            extra={
                "memory_freed_mb": memory_freed_mb,
                "initial_cache_size": initial_cache_size,
                "final_cache_size": final_cache_size,
                "memory_stats": memory_stats,
                "metric_type": "cache_memory_cleanup"
            }
        )


# ================================================================================================
# INTEGRATED PERFORMANCE BENCHMARKS
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.integration
class TestIntegratedPerformanceBenchmarks:
    """End-to-end performance validation with full system integration."""
    
    def test_complete_training_episode_performance(self, benchmark: BenchmarkFixture, mock_gymnasium_env, performance_config):
        """Validate complete training episode maintains performance targets."""
        if not BENCHMARK_AVAILABLE:
            pytest.skip("pytest-benchmark not available")
        
        env = mock_gymnasium_env
        
        def complete_episode():
            env.reset()
            step_count = 0
            total_reward = 0.0
            
            while step_count < 50:  # Short episode for benchmarking
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                total_reward += reward
                step_count += 1
                
                if terminated or truncated:
                    break
            
            return step_count, total_reward
        
        # Benchmark complete episode
        result = benchmark.pedantic(
            complete_episode,
            rounds=performance_config.rounds,
            iterations=max(1, performance_config.iterations // 5)  # Fewer iterations for full episodes
        )
        
        # Validate episode performance
        episode_time_ms = benchmark.stats.mean * 1000
        steps_per_episode = result[0] if hasattr(result, '__getitem__') else 50
        
        # Calculate step latency from episode timing
        avg_step_latency_ms = episode_time_ms / steps_per_episode if steps_per_episode > 0 else episode_time_ms
        
        assert avg_step_latency_ms < performance_config.step_latency_ms, (
            f"Average step latency {avg_step_latency_ms:.2f}ms in full episode "
            f"exceeds threshold {performance_config.step_latency_ms}ms"
        )
        
        # Validate FPS performance
        fps_estimate = 1000 / avg_step_latency_ms if avg_step_latency_ms > 0 else float('inf')
        assert fps_estimate >= SIMULATION_FPS_THRESHOLD, (
            f"Episode FPS {fps_estimate:.1f} below target {SIMULATION_FPS_THRESHOLD}"
        )
        
        logger.info(
            f"Complete episode performance validated: episode={episode_time_ms:.1f}ms, "
            f"avg_step={avg_step_latency_ms:.2f}ms, fps={fps_estimate:.1f}",
            extra={
                "episode_time_ms": episode_time_ms,
                "avg_step_latency_ms": avg_step_latency_ms,
                "fps_estimate": fps_estimate,
                "steps_per_episode": steps_per_episode,
                "metric_type": "complete_episode_performance"
            }
        )
    
    def test_training_throughput_estimation(self, mock_gymnasium_env, performance_monitor, performance_config):
        """Validate training throughput achieves ≥1M steps/hour target."""
        env = mock_gymnasium_env
        
        with performance_monitor("training_throughput") as metrics:
            env.reset()
            
            # Run sustained training simulation
            test_steps = min(1000, performance_config.memory_leak_steps // 10)
            episode_count = 0
            step_count = 0
            
            start_time = time.perf_counter()
            
            for _ in range(test_steps):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                step_count += 1
                
                if terminated or truncated:
                    env.reset()
                    episode_count += 1
            
            end_time = time.perf_counter()
        
        # Calculate throughput metrics
        total_time_hours = (end_time - start_time) / 3600
        steps_per_hour = step_count / total_time_hours if total_time_hours > 0 else 0
        
        # Project to full-scale throughput
        projected_throughput = steps_per_hour
        
        # Validate throughput requirement (allow for test scaling)
        min_acceptable_throughput = TRAINING_THROUGHPUT_THRESHOLD * 0.5  # 50% of target for test scaling
        assert projected_throughput >= min_acceptable_throughput, (
            f"Projected training throughput {projected_throughput:.0f} steps/hour "
            f"below minimum {min_acceptable_throughput:.0f} steps/hour"
        )
        
        # Calculate additional metrics
        avg_step_time_ms = (end_time - start_time) * 1000 / step_count if step_count > 0 else float('inf')
        fps_estimate = 1000 / avg_step_time_ms if avg_step_time_ms > 0 else float('inf')
        
        logger.info(
            f"Training throughput estimation: {projected_throughput:.0f} steps/hour, "
            f"avg_step={avg_step_time_ms:.2f}ms, fps={fps_estimate:.1f}, "
            f"episodes={episode_count}",
            extra={
                "projected_throughput_steps_per_hour": projected_throughput,
                "avg_step_time_ms": avg_step_time_ms,
                "fps_estimate": fps_estimate,
                "total_steps": step_count,
                "episode_count": episode_count,
                "test_duration_seconds": end_time - start_time,
                "metric_type": "training_throughput_estimation"
            }
        )
    
    def test_performance_consistency_across_episodes(self, mock_gymnasium_env, regression_detector, performance_config):
        """Validate performance consistency across multiple training episodes."""
        env = mock_gymnasium_env
        
        episode_metrics = []
        
        # Run multiple episodes and collect performance metrics
        for episode in range(min(10, performance_config.iterations)):
            env.reset()
            
            episode_start = time.perf_counter()
            step_count = 0
            step_times = []
            
            # Run episode
            while step_count < 50:  # Fixed episode length for consistency
                step_start = time.perf_counter()
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                step_end = time.perf_counter()
                
                step_times.append((step_end - step_start) * 1000)
                step_count += 1
                
                if terminated or truncated:
                    break
            
            episode_end = time.perf_counter()
            
            # Create episode performance metrics
            episode_metric = PerformanceMetricsSnapshot()
            episode_metric.step_latency_ms = np.mean(step_times)
            episode_metric.fps_estimate = 1000 / episode_metric.step_latency_ms if episode_metric.step_latency_ms > 0 else float('inf')
            episode_metric.step_count = step_count
            episode_metric.episode_count = episode + 1
            
            # Get cache stats if available
            cache_stats = env.get_cache_stats()
            episode_metric.cache_hit_rate = cache_stats.get("hit_rate", 0.0)
            
            episode_metrics.append(episode_metric)
            regression_detector.add_baseline_metric(episode_metric)
            
            # Validate individual episode performance
            assert episode_metric.step_latency_ms < performance_config.step_latency_ms, (
                f"Episode {episode + 1} step latency {episode_metric.step_latency_ms:.2f}ms "
                f"exceeds threshold"
            )
        
        # Analyze performance consistency
        step_latencies = [m.step_latency_ms for m in episode_metrics]
        cache_hit_rates = [m.cache_hit_rate for m in episode_metrics]
        
        # Validate statistical consistency
        latency_mean = np.mean(step_latencies)
        latency_std = np.std(step_latencies)
        latency_cv = latency_std / latency_mean if latency_mean > 0 else float('inf')
        
        hit_rate_mean = np.mean(cache_hit_rates)
        
        # Performance should be consistent (low coefficient of variation)
        assert latency_cv < 0.3, f"Step latency too variable: CV={latency_cv:.3f}"
        assert latency_mean < performance_config.step_latency_ms, (
            f"Mean step latency {latency_mean:.2f}ms exceeds threshold"
        )
        
        if hit_rate_mean > 0:  # Only validate if cache is operational
            assert hit_rate_mean >= performance_config.cache_hit_rate * 0.8, (
                f"Mean cache hit rate {hit_rate_mean:.2%} too low"
            )
        
        logger.info(
            f"Performance consistency validated across {len(episode_metrics)} episodes: "
            f"latency={latency_mean:.2f}±{latency_std:.2f}ms (CV={latency_cv:.3f}), "
            f"cache_hit_rate={hit_rate_mean:.2%}",
            extra={
                "latency_mean_ms": latency_mean,
                "latency_std_ms": latency_std,
                "latency_cv": latency_cv,
                "cache_hit_rate_mean": hit_rate_mean,
                "episode_count": len(episode_metrics),
                "metric_type": "performance_consistency"
            }
        )


# ================================================================================================
# STATISTICAL PERFORMANCE REGRESSION TESTS
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.regression
class TestPerformanceRegressionDetection:
    """Statistical performance regression detection with confidence intervals."""
    
    def test_performance_baseline_establishment(self, mock_gymnasium_env, regression_detector, performance_config):
        """Establish performance baseline for regression detection."""
        env = mock_gymnasium_env
        env.reset()
        
        # Collect baseline performance measurements
        baseline_samples = performance_config.iterations * 2
        
        for i in range(baseline_samples):
            start_time = time.perf_counter()
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            end_time = time.perf_counter()
            
            if terminated or truncated:
                env.reset()
            
            # Create performance snapshot
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = (end_time - start_time) * 1000
            metric.step_count = i + 1
            
            # Extract performance stats from info if available
            if "perf_stats" in info:
                perf_stats = info["perf_stats"]
                metric.cache_hit_rate = perf_stats.get("cache_hit_rate", 0.0)
                metric.fps_estimate = perf_stats.get("fps_estimate", 0.0)
            
            regression_detector.add_baseline_metric(metric)
        
        # Validate baseline establishment
        assert len(regression_detector.baseline_metrics) == baseline_samples, (
            "Baseline metrics collection incomplete"
        )
        
        # Analyze baseline performance characteristics
        baseline_latencies = [m.step_latency_ms for m in regression_detector.baseline_metrics]
        baseline_mean = np.mean(baseline_latencies)
        baseline_std = np.std(baseline_latencies)
        
        assert baseline_mean < performance_config.step_latency_ms, (
            f"Baseline performance {baseline_mean:.2f}ms violates SLA"
        )
        
        logger.info(
            f"Performance baseline established: {baseline_samples} samples, "
            f"mean={baseline_mean:.2f}ms, std={baseline_std:.2f}ms",
            extra={
                "baseline_samples": baseline_samples,
                "baseline_mean_ms": baseline_mean,
                "baseline_std_ms": baseline_std,
                "baseline_min_ms": np.min(baseline_latencies),
                "baseline_max_ms": np.max(baseline_latencies),
                "metric_type": "performance_baseline"
            }
        )
    
    def test_regression_detection_with_simulated_degradation(self, regression_detector, performance_config):
        """Test regression detection with artificially degraded performance."""
        
        # First establish a good baseline
        baseline_latency = 8.0  # Good performance
        for i in range(performance_config.iterations):
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = baseline_latency + np.random.normal(0, 0.5)  # Small variance
            metric.step_count = i + 1
            regression_detector.add_baseline_metric(metric)
        
        # Simulate performance degradation
        degraded_latency = baseline_latency * 1.25  # 25% degradation
        for i in range(performance_config.iterations):
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = degraded_latency + np.random.normal(0, 0.5)
            metric.step_count = len(regression_detector.baseline_metrics) + i + 1
            regression_detector.add_current_metric(metric)
        
        # Test regression detection
        has_regression, p_value, analysis = regression_detector.detect_regression("step_latency_ms")
        
        # Should detect the simulated regression
        assert has_regression, f"Failed to detect simulated regression: {analysis}"
        
        if SCIPY_AVAILABLE:
            assert p_value < performance_config.significance_level, (
                f"P-value {p_value:.4f} not significant at {performance_config.significance_level}"
            )
        
        logger.info(
            f"Regression detection test passed: detected degradation from "
            f"{baseline_latency:.1f}ms to {degraded_latency:.1f}ms, p_value={p_value:.4f}",
            extra={
                "baseline_latency_ms": baseline_latency,
                "degraded_latency_ms": degraded_latency,
                "has_regression": has_regression,
                "p_value": p_value,
                "analysis": analysis,
                "metric_type": "regression_detection_test"
            }
        )
    
    def test_false_positive_regression_prevention(self, regression_detector, performance_config):
        """Validate regression detection doesn't generate false positives with normal variance."""
        
        # Create two similar performance datasets (no real regression)
        baseline_latency = 7.5  # Good performance
        
        # Baseline dataset
        for i in range(performance_config.iterations):
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = baseline_latency + np.random.normal(0, 0.8)  # Natural variance
            metric.step_count = i + 1
            regression_detector.add_baseline_metric(metric)
        
        # Current dataset (similar performance with natural variance)
        for i in range(performance_config.iterations):
            metric = PerformanceMetricsSnapshot()
            metric.step_latency_ms = baseline_latency + np.random.normal(0, 0.8)  # Same distribution
            metric.step_count = len(regression_detector.baseline_metrics) + i + 1
            regression_detector.add_current_metric(metric)
        
        # Test for false positive regression
        has_regression, p_value, analysis = regression_detector.detect_regression("step_latency_ms")
        
        # Should NOT detect regression in similar distributions
        assert not has_regression, f"False positive regression detected: {analysis}"
        
        if SCIPY_AVAILABLE:
            assert p_value >= performance_config.significance_level, (
                f"P-value {p_value:.4f} indicates false positive at {performance_config.significance_level}"
            )
        
        logger.info(
            f"False positive prevention test passed: no regression detected in "
            f"similar distributions, p_value={p_value:.4f}",
            extra={
                "baseline_latency_ms": baseline_latency,
                "has_regression": has_regression,
                "p_value": p_value,
                "analysis": analysis,
                "metric_type": "false_positive_prevention"
            }
        )


# ================================================================================================
# PERFORMANCE QUALITY GATES
# ================================================================================================

@pytest.mark.benchmark
@pytest.mark.quality_gate
class TestPerformanceQualityGates:
    """Performance quality gates for CI/CD merge blocking."""
    
    def test_step_latency_quality_gate(self, mock_gymnasium_env, performance_config):
        """Critical quality gate: Step latency must be ≤10ms for merge approval."""
        env = mock_gymnasium_env
        env.reset()
        
        # Collect performance samples for quality gate validation
        latency_samples = []
        violation_count = 0
        
        for i in range(performance_config.iterations * 2):
            start_time = time.perf_counter()
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            end_time = time.perf_counter()
            
            if terminated or truncated:
                env.reset()
            
            step_latency_ms = (end_time - start_time) * 1000
            latency_samples.append(step_latency_ms)
            
            # Count SLA violations
            if step_latency_ms > performance_config.step_latency_ms:
                violation_count += 1
        
        # Quality gate criteria
        mean_latency = np.mean(latency_samples)
        max_latency = np.max(latency_samples)
        violation_rate = violation_count / len(latency_samples)
        
        # CRITICAL: Mean latency must meet SLA
        assert mean_latency <= performance_config.step_latency_ms, (
            f"QUALITY GATE FAILURE: Mean step latency {mean_latency:.2f}ms "
            f"exceeds SLA {performance_config.step_latency_ms}ms"
        )
        
        # CRITICAL: Violation rate must be minimal
        max_violation_rate = 0.05  # 5% tolerance
        assert violation_rate <= max_violation_rate, (
            f"QUALITY GATE FAILURE: {violation_rate:.1%} of steps exceed SLA "
            f"(max allowed: {max_violation_rate:.1%})"
        )
        
        # CRITICAL: No extreme outliers
        outlier_threshold = performance_config.step_latency_ms * 2
        assert max_latency <= outlier_threshold, (
            f"QUALITY GATE FAILURE: Maximum latency {max_latency:.2f}ms "
            f"indicates performance instability"
        )
        
        logger.info(
            f"✅ QUALITY GATE PASSED: Step latency - mean={mean_latency:.2f}ms, "
            f"max={max_latency:.2f}ms, violations={violation_rate:.1%}",
            extra={
                "quality_gate": "step_latency",
                "status": "PASSED",
                "mean_latency_ms": mean_latency,
                "max_latency_ms": max_latency,
                "violation_rate": violation_rate,
                "threshold_ms": performance_config.step_latency_ms,
                "metric_type": "quality_gate_step_latency"
            }
        )
    
    def test_cache_performance_quality_gate(self, mock_gymnasium_env, performance_config):
        """Critical quality gate: Cache hit rate must be ≥90% for merge approval."""
        if not FRAME_CACHE_AVAILABLE:
            pytest.skip("FrameCache not available - cache quality gate skipped")
        
        env = mock_gymnasium_env
        env.reset()
        
        # Run simulation to populate cache and measure hit rate
        total_steps = performance_config.access_pattern_length
        
        for step in range(total_steps):
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            
            if terminated or truncated:
                env.reset()
        
        # Get final cache statistics
        cache_stats = env.get_cache_stats()
        hit_rate = cache_stats.get("hit_rate", 0.0)
        hits = cache_stats.get("hits", 0)
        misses = cache_stats.get("misses", 0)
        
        # CRITICAL: Cache hit rate must meet SLA
        assert hit_rate >= performance_config.cache_hit_rate, (
            f"QUALITY GATE FAILURE: Cache hit rate {hit_rate:.2%} "
            f"below SLA {performance_config.cache_hit_rate:.2%}"
        )
        
        # CRITICAL: Cache must be operational
        total_accesses = hits + misses
        assert total_accesses > 0, (
            "QUALITY GATE FAILURE: Cache not operational - no recorded accesses"
        )
        
        # CRITICAL: Minimum cache utilization
        min_cache_hits = total_steps * 0.5  # At least 50% should be cache hits
        assert hits >= min_cache_hits, (
            f"QUALITY GATE FAILURE: Insufficient cache utilization - "
            f"{hits} hits vs minimum {min_cache_hits}"
        )
        
        logger.info(
            f"✅ QUALITY GATE PASSED: Cache performance - hit_rate={hit_rate:.2%}, "
            f"hits={hits}, misses={misses}",
            extra={
                "quality_gate": "cache_performance",
                "status": "PASSED",
                "hit_rate": hit_rate,
                "hits": hits,
                "misses": misses,
                "threshold": performance_config.cache_hit_rate,
                "metric_type": "quality_gate_cache_performance"
            }
        )
    
    def test_memory_usage_quality_gate(self, mock_gymnasium_env, memory_monitor, performance_config):
        """Critical quality gate: Memory usage must be ≤2 GiB for merge approval."""
        
        with memory_monitor("memory_quality_gate") as memory_stats:
            env = mock_gymnasium_env
            env.reset()
            
            # Run sustained operation to check memory compliance
            test_steps = min(1000, performance_config.memory_leak_steps // 5)
            
            for step in range(test_steps):
                action = env.action_space.sample()
                obs, reward, terminated, truncated, info = env.step(action)
                
                if terminated or truncated:
                    env.reset()
                
                # Check memory periodically
                if step % 100 == 0:
                    current_memory_gib = memory_stats["peak_mb"] / 1024
                    
                    # Immediate failure if memory limit exceeded
                    if current_memory_gib > performance_config.memory_limit_gib:
                        pytest.fail(
                            f"QUALITY GATE FAILURE: Memory limit exceeded during operation - "
                            f"{current_memory_gib:.2f} GiB > {performance_config.memory_limit_gib} GiB"
                        )
        
        # CRITICAL: Final memory check
        final_memory_gib = memory_stats["final_mb"] / 1024
        peak_memory_gib = memory_stats["peak_mb"] / 1024
        
        assert final_memory_gib <= performance_config.memory_limit_gib, (
            f"QUALITY GATE FAILURE: Final memory {final_memory_gib:.2f} GiB "
            f"exceeds limit {performance_config.memory_limit_gib} GiB"
        )
        
        assert peak_memory_gib <= performance_config.memory_limit_gib, (
            f"QUALITY GATE FAILURE: Peak memory {peak_memory_gib:.2f} GiB "
            f"exceeds limit {performance_config.memory_limit_gib} GiB"
        )
        
        # CRITICAL: Memory growth validation
        memory_growth_gib = (memory_stats["final_mb"] - memory_stats["initial_mb"]) / 1024
        max_acceptable_growth_gib = performance_config.memory_limit_gib * 0.1  # 10% of limit
        
        assert memory_growth_gib <= max_acceptable_growth_gib, (
            f"QUALITY GATE FAILURE: Excessive memory growth {memory_growth_gib:.2f} GiB "
            f"exceeds acceptable limit {max_acceptable_growth_gib:.2f} GiB"
        )
        
        logger.info(
            f"✅ QUALITY GATE PASSED: Memory usage - final={final_memory_gib:.2f} GiB, "
            f"peak={peak_memory_gib:.2f} GiB, growth={memory_growth_gib:.2f} GiB",
            extra={
                "quality_gate": "memory_usage",
                "status": "PASSED",
                "final_memory_gib": final_memory_gib,
                "peak_memory_gib": peak_memory_gib,
                "memory_growth_gib": memory_growth_gib,
                "limit_gib": performance_config.memory_limit_gib,
                "metric_type": "quality_gate_memory_usage"
            }
        )
    
    def test_overall_performance_sla_compliance(self, mock_gymnasium_env, performance_config):
        """Master quality gate: Overall system performance SLA compliance."""
        env = mock_gymnasium_env
        env.reset()
        
        # Collect comprehensive performance metrics
        sla_results = {
            "step_latency": {"status": "UNKNOWN", "value": 0.0, "threshold": performance_config.step_latency_ms},
            "cache_hit_rate": {"status": "UNKNOWN", "value": 0.0, "threshold": performance_config.cache_hit_rate},
            "memory_usage": {"status": "UNKNOWN", "value": 0.0, "threshold": performance_config.memory_limit_gib},
            "fps_estimate": {"status": "UNKNOWN", "value": 0.0, "threshold": SIMULATION_FPS_THRESHOLD}
        }
        
        # Measure step latency
        latency_samples = []
        for i in range(performance_config.iterations):
            start_time = time.perf_counter()
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            end_time = time.perf_counter()
            
            if terminated or truncated:
                env.reset()
            
            latency_samples.append((end_time - start_time) * 1000)
        
        mean_latency = np.mean(latency_samples)
        sla_results["step_latency"]["value"] = mean_latency
        sla_results["step_latency"]["status"] = "PASS" if mean_latency <= performance_config.step_latency_ms else "FAIL"
        
        # Check cache performance
        if FRAME_CACHE_AVAILABLE:
            cache_stats = env.get_cache_stats()
            hit_rate = cache_stats.get("hit_rate", 0.0)
            sla_results["cache_hit_rate"]["value"] = hit_rate
            sla_results["cache_hit_rate"]["status"] = "PASS" if hit_rate >= performance_config.cache_hit_rate else "FAIL"
        else:
            sla_results["cache_hit_rate"]["status"] = "SKIP"
        
        # Check memory usage
        current_memory_gib = psutil.Process().memory_info().rss / 1024 / 1024 / 1024
        sla_results["memory_usage"]["value"] = current_memory_gib
        sla_results["memory_usage"]["status"] = "PASS" if current_memory_gib <= performance_config.memory_limit_gib else "FAIL"
        
        # Check FPS estimate
        fps_estimate = 1000 / mean_latency if mean_latency > 0 else float('inf')
        sla_results["fps_estimate"]["value"] = fps_estimate
        sla_results["fps_estimate"]["status"] = "PASS" if fps_estimate >= SIMULATION_FPS_THRESHOLD else "FAIL"
        
        # Validate overall SLA compliance
        failed_slas = [name for name, result in sla_results.items() if result["status"] == "FAIL"]
        
        assert len(failed_slas) == 0, (
            f"OVERALL QUALITY GATE FAILURE: Failed SLAs: {failed_slas}\n"
            f"Detailed results: {sla_results}"
        )
        
        logger.info(
            f"✅ OVERALL QUALITY GATE PASSED: All performance SLAs met",
            extra={
                "quality_gate": "overall_sla_compliance",
                "status": "PASSED",
                "sla_results": sla_results,
                "metric_type": "quality_gate_overall_sla"
            }
        )


# ================================================================================================
# MODULE EXPORTS AND PYTEST MARKERS
# ================================================================================================

if __name__ == "__main__":
    # Run performance benchmarks with appropriate configuration
    pytest.main([
        __file__,
        "-v",
        "--benchmark-only",
        "--benchmark-sort=mean",
        "--benchmark-columns=min,max,mean,stddev,rounds,iterations",
        "-m", "benchmark",
        "--tb=short"
    ])

# Export test classes for external test discovery
__all__ = [
    "TestStepLatencyBenchmarks",
    "TestCachePerformanceBenchmarks", 
    "TestMemoryUsageBenchmarks",
    "TestIntegratedPerformanceBenchmarks",
    "TestPerformanceRegressionDetection",
    "TestPerformanceQualityGates",
    "PerformanceBenchmarkConfig",
    "PerformanceMetricsSnapshot",
    "PerformanceRegressionDetector"
]