"""
Comprehensive pytest test suite for CLI interface module validation.

This module provides thorough testing of the command-line interface functionality
including Click command execution, Hydra configuration integration, parameter
validation, error handling, and performance requirements compliance. The test
suite uses click.testing.CliRunner for command-line interface validation and
ensures robust CLI behavior across all supported scenarios.

Test Coverage:
- CLI command execution validation with CliRunner per Section 6.6.1.1
- Parameter override testing through command-line flags per F-013-RQ-003
- Command initialization timing validation (<2s) per Section 2.2.9.3
- Help system and usage example validation per F-013-RQ-002
- Batch processing and automation workflow testing per Section 7.4.4.1
- Error handling and parameter validation testing per Section 6.6.7.2
- Hydra configuration integration testing per Feature F-013

Test Architecture:
- CliRunner-based testing for command execution per Section 6.6.1.1
- Mock integration for dependencies to ensure test isolation
- Parameterized testing for comprehensive parameter combination coverage
- Performance testing for CLI initialization timing requirements
- Security testing for parameter injection prevention per Section 6.6.7.2

Author: Generated by Blitzy Template Engine
Version: 2.0.0
"""

import os
import sys
import time
import json
import tempfile
import pytest
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock, call
from typing import Dict, List, Any, Optional, Tuple

import numpy as np
from click.testing import CliRunner
from omegaconf import DictConfig, OmegaConf

# Import the CLI module under test
from plume_nav_sim.cli.main import (
    cli, run, config, validate, export, visualize, main,
    CLIError, handle_cli_exception, validate_configuration,
    initialize_system, cleanup_system, get_cli_config, set_cli_config
)

# Import dependencies for mocking
from plume_nav_sim.api.navigation import (
    create_navigator, create_video_plume, run_plume_simulation,
    ConfigurationError, SimulationError
)
from plume_nav_sim.config.schemas import (
    NavigatorConfig, VideoPlumeConfig, SimulationConfig
)


class TestCLIRunner:
    """
    Test suite for Click CLI runner functionality and basic command structure.
    
    Validates CLI command registration, help text generation, and basic
    command execution patterns using click.testing.CliRunner per Section 6.6.1.1
    enhanced testing standards.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for CLI testing."""
        return CliRunner()
    
    @pytest.fixture
    def mock_hydra_config(self):
        """Provide mock Hydra configuration for CLI testing."""
        config = OmegaConf.create({
            "navigator": {
                "position": [0.0, 0.0],
                "orientation": 0.0,
                "speed": 1.0,
                "max_speed": 2.0,
                "angular_velocity": 0.0
            },
            "video_plume": {
                "video_path": "test_video.mp4",
                "flip": False,
                "kernel_size": 5,
                "kernel_sigma": 1.0
            },
            "simulation": {
                "num_steps": 100,
                "dt": 0.1,
                "sensor_distance": 5.0,
                "sensor_angle": 45.0
            },
            "visualization": {
                "animation": {"enabled": True},
                "plotting": {"show_trails": True}
            },
            "hydra": {
                "run": {"dir": "outputs"}
            }
        })
        return config
    
    def test_cli_command_registration(self, cli_runner):
        """Test that CLI commands are properly registered with Click framework."""
        result = cli_runner.invoke(cli, ['--help'])
        
        assert result.exit_code == 0
        assert 'run' in result.output
        assert 'config' in result.output
        assert 'visualize' in result.output
        
        # Verify command descriptions are present
        assert 'Execute odor plume navigation simulation' in result.output
        assert 'Configuration management and validation' in result.output
        assert 'Generate visualizations from simulation results' in result.output
    
    def test_cli_help_text_generation(self, cli_runner):
        """
        Test help system and usage example validation per F-013-RQ-002.
        
        Validates that comprehensive help messages and usage examples are
        displayed via --help flag meeting CLI documentation requirements.
        """
        # Test main CLI help
        result = cli_runner.invoke(cli, ['--help'])
        assert result.exit_code == 0
        assert 'Odor Plume Navigation System' in result.output
        assert 'Examples:' in result.output
        assert 'plume_nav_sim run' in result.output
        
        # Test run command help
        result = cli_runner.invoke(cli, ['run', '--help'])
        assert result.exit_code == 0
        assert 'Execute odor plume navigation simulation' in result.output
        assert 'Performance Requirements:' in result.output
        assert 'Examples:' in result.output
        
        # Test config command help
        result = cli_runner.invoke(cli, ['config', '--help'])
        assert result.exit_code == 0
        assert 'Configuration management and validation' in result.output
        
        # Test visualize command help
        result = cli_runner.invoke(cli, ['visualize', '--help'])
        assert result.exit_code == 0
        assert 'Generate visualizations from simulation results' in result.output
        assert 'Examples:' in result.output
    
    def test_cli_global_options(self, cli_runner):
        """Test global CLI options like verbose and quiet flags."""
        # Test verbose flag
        result = cli_runner.invoke(cli, ['--verbose', '--help'])
        assert result.exit_code == 0
        
        # Test quiet flag
        result = cli_runner.invoke(cli, ['--quiet', '--help'])
        assert result.exit_code == 0
        
        # Test config-dir option
        with tempfile.TemporaryDirectory() as temp_dir:
            result = cli_runner.invoke(cli, ['--config-dir', temp_dir, '--help'])
            assert result.exit_code == 0
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    def test_cli_command_initialization_timing(self, mock_get_config, cli_runner):
        """
        Test command initialization timing validation (<2s) per Section 2.2.9.3.
        
        Validates that CLI command initialization meets performance requirements
        for interactive command-line responsiveness.
        """
        # Mock configuration to prevent actual Hydra loading
        mock_config = OmegaConf.create({"test": "value"})
        mock_get_config.return_value = mock_config
        
        start_time = time.time()
        result = cli_runner.invoke(cli, ['--help'])
        end_time = time.time()
        
        initialization_time = end_time - start_time
        
        assert result.exit_code == 0
        assert initialization_time < 2.0, f"CLI initialization took {initialization_time:.2f}s, exceeding 2s requirement"
    
    def test_cli_error_handling_basic(self, cli_runner):
        """Test basic CLI error handling for invalid commands and options."""
        # Test invalid command
        result = cli_runner.invoke(cli, ['invalid-command'])
        assert result.exit_code != 0
        assert 'No such command' in result.output or 'Usage:' in result.output
        
        # Test invalid option
        result = cli_runner.invoke(cli, ['--invalid-option'])
        assert result.exit_code != 0


class TestCLIParameterValidation:
    """
    Test suite for CLI parameter validation and type checking.
    
    Validates Click framework integration per F-013-RQ-003 parameter override
    requirements and ensures robust parameter handling and validation.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for parameter testing."""
        return CliRunner()
    
    @pytest.fixture
    def mock_dependencies(self):
        """Mock all CLI dependencies for isolated parameter testing."""
        with patch.multiple(
            'plume_nav_sim.cli.main',
            create_navigator=Mock(),
            create_video_plume=Mock(),
            run_plume_simulation=Mock(return_value=(np.array([]), np.array([]), np.array([]))),
            validate_configuration=Mock(return_value=True),
            initialize_system=Mock(return_value={"test": "system"}),
            cleanup_system=Mock(),
            get_cli_config=Mock(return_value=OmegaConf.create({"test": "config"}))
        ) as mocks:
            yield mocks
    
    def test_run_command_parameter_validation(self, cli_runner, mock_dependencies):
        """
        Test run command parameter validation and type checking.
        
        Validates Click parameter type enforcement and range validation
        ensuring command-line arguments cannot bypass security controls.
        """
        # Test valid parameters
        result = cli_runner.invoke(cli, [
            'run', 
            '--dry-run',
            '--max-duration', '10.0',
            '--num-agents', '5'
        ])
        
        # Should not fail due to parameter validation
        # (may fail due to missing config, but not parameter issues)
        assert '--max-duration' not in str(result.exception) if result.exception else True
        assert '--num-agents' not in str(result.exception) if result.exception else True
    
    def test_run_command_invalid_parameters(self, cli_runner):
        """Test run command rejects invalid parameter types and values."""
        # Test invalid max-duration (non-numeric)
        result = cli_runner.invoke(cli, ['run', '--max-duration', 'invalid'])
        assert result.exit_code != 0
        
        # Test invalid num-agents (non-integer)
        result = cli_runner.invoke(cli, ['run', '--num-agents', 'invalid'])
        assert result.exit_code != 0
        
        # Test negative num-agents
        result = cli_runner.invoke(cli, ['run', '--num-agents', '-1'])
        assert result.exit_code != 0
    
    def test_visualize_command_parameter_validation(self, cli_runner):
        """Test visualize command parameter validation."""
        # Test missing required input-file
        result = cli_runner.invoke(cli, ['visualize'])
        assert result.exit_code != 0
        assert 'input-file' in result.output or 'required' in result.output.lower()
        
        # Test invalid format option
        result = cli_runner.invoke(cli, [
            'visualize', 
            '--input-file', 'nonexistent.npz',
            '--format', 'invalid_format'
        ])
        assert result.exit_code != 0
        
        # Test invalid quality option
        result = cli_runner.invoke(cli, [
            'visualize',
            '--input-file', 'nonexistent.npz',
            '--quality', 'invalid_quality'
        ])
        assert result.exit_code != 0
    
    def test_config_export_parameter_validation(self, cli_runner):
        """Test config export command parameter validation."""
        # Test invalid format option
        result = cli_runner.invoke(cli, [
            'config', 'export',
            '--format', 'invalid_format'
        ])
        assert result.exit_code != 0


class TestCLIHydraIntegration:
    """
    Test suite for Hydra configuration integration with CLI commands.
    
    Validates Hydra configuration override mechanisms through CLI parameters
    per Section 7.2.3.2 and Feature F-013 requirements.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for Hydra integration testing."""
        return CliRunner()
    
    @pytest.fixture
    def mock_hydra_config(self):
        """Provide comprehensive mock Hydra configuration."""
        return OmegaConf.create({
            "navigator": {
                "position": [0.0, 0.0],
                "orientation": 0.0,
                "speed": 1.0,
                "max_speed": 2.0,
                "angular_velocity": 0.0
            },
            "video_plume": {
                "video_path": "test_video.mp4",
                "flip": False,
                "kernel_size": 5,
                "kernel_sigma": 1.0
            },
            "simulation": {
                "num_steps": 100,
                "dt": 0.1,
                "sensor_distance": 5.0,
                "sensor_angle": 45.0
            },
            "hydra": {
                "run": {"dir": "outputs"}
            },
            "reproducibility": {
                "global_seed": 42,
                "auto_seed": True
            },
            "logging": {"level": "INFO"},
            "database": {"enabled": False}
        })
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    @patch('plume_nav_sim.cli.main.validate_configuration')
    @patch('plume_nav_sim.cli.main.initialize_system')
    @patch('plume_nav_sim.cli.main.cleanup_system')
    def test_hydra_configuration_loading(
        self, 
        mock_cleanup,
        mock_init_system,
        mock_validate,
        mock_get_config,
        cli_runner,
        mock_hydra_config
    ):
        """
        Test Hydra configuration loading and validation in CLI context.
        
        Validates that CLI commands properly integrate with Hydra configuration
        system and handle configuration validation.
        """
        # Setup mocks
        mock_get_config.return_value = mock_hydra_config
        mock_validate.return_value = True
        mock_init_system.return_value = {"initialized": True}
        
        # Test that configuration is loaded for commands that need it
        result = cli_runner.invoke(cli, ['run', '--dry-run'])
        
        # Verify configuration-related functions were called
        mock_get_config.assert_called()
        mock_validate.assert_called_with(mock_hydra_config)
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    @patch('plume_nav_sim.cli.main.OmegaConf')
    def test_cli_parameter_override_integration(
        self,
        mock_omega_conf,
        mock_get_config,
        cli_runner,
        mock_hydra_config
    ):
        """
        Test CLI parameter override mechanisms per F-013-RQ-003.
        
        Validates that command-line parameters properly override configuration
        values through Hydra integration and OmegaConf.set functionality.
        """
        mock_get_config.return_value = mock_hydra_config
        mock_omega_conf.set = Mock()
        
        # Test parameter overrides
        with patch('plume_nav_sim.cli.main.validate_configuration'), \
             patch('plume_nav_sim.cli.main.initialize_system'), \
             patch('plume_nav_sim.cli.main.cleanup_system'):
            
            result = cli_runner.invoke(cli, [
                'run',
                '--dry-run',
                '--max-duration', '15.0',
                '--num-agents', '3',
                '--output-dir', '/tmp/test'
            ])
            
            # Verify OmegaConf.set was called for parameter overrides
            expected_calls = [
                call(mock_hydra_config, 'simulation.max_duration', 15.0),
                call(mock_hydra_config, 'navigator.num_agents', 3),
                call(mock_hydra_config, 'hydra.run.dir', '/tmp/test')
            ]
            
            mock_omega_conf.set.assert_has_calls(expected_calls, any_order=True)
    
    def test_hydra_configuration_validation_errors(self, cli_runner):
        """
        Test CLI handling of Hydra configuration validation errors.
        
        Validates proper error handling and reporting for configuration
        validation failures with clear error messages.
        """
        # Mock configuration validation failure
        with patch('plume_nav_sim.cli.main.get_cli_config') as mock_get_config, \
             patch('plume_nav_sim.cli.main.validate_configuration') as mock_validate:
            
            # Setup mock to return config but validation fails
            mock_get_config.return_value = OmegaConf.create({"test": "config"})
            mock_validate.side_effect = CLIError("Configuration validation failed", exit_code=2)
            
            result = cli_runner.invoke(cli, ['run', '--dry-run'])
            
            # Should exit with error code from validation failure
            assert result.exit_code == 2
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    def test_configuration_not_available_error(self, mock_get_config, cli_runner):
        """Test CLI behavior when Hydra configuration is not available."""
        mock_get_config.return_value = None
        
        result = cli_runner.invoke(cli, ['run', '--dry-run'])
        
        # Should handle missing configuration gracefully
        assert result.exit_code == 4  # Expected exit code for missing config


class TestCLICommandExecution:
    """
    Test suite for CLI command execution and workflow integration.
    
    Validates simulation launching commands with exit code verification per
    F-013-RQ-001 and batch processing capabilities per Section 7.4.4.1.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for command execution testing."""
        return CliRunner()
    
    @pytest.fixture
    def mock_complete_system(self):
        """Comprehensive mock system for CLI execution testing."""
        mocks = {
            'get_cli_config': Mock(),
            'validate_configuration': Mock(return_value=True),
            'initialize_system': Mock(return_value={'initialized': True}),
            'cleanup_system': Mock(),
            'create_navigator': Mock(),
            'create_video_plume': Mock(),
            'run_plume_simulation': Mock(return_value=(
                np.array([[[0, 0], [1, 1]]]),  # positions
                np.array([[0, 45]]),           # orientations  
                np.array([[0.5, 0.8]])         # readings
            )),
            'visualize_simulation_results': Mock(),
            'export_animation': Mock(),
            'setup_logging': Mock(),
            'set_global_seed': Mock(),
            'get_current_seed': Mock(return_value=42)
        }
        
        # Configure mock Hydra config
        mock_config = OmegaConf.create({
            "navigator": {"position": [0, 0], "max_speed": 2.0},
            "video_plume": {"video_path": "test.mp4", "flip": False},
            "simulation": {"num_steps": 10, "dt": 0.1},
            "hydra": {"run": {"dir": "outputs"}},
            "visualization": {"animation": {"enabled": True}},
            "reproducibility": {"global_seed": 42}
        })
        mocks['get_cli_config'].return_value = mock_config
        
        return mocks
    
    @patch.multiple('plume_nav_sim.cli.main', **{
        'get_cli_config': Mock(),
        'validate_configuration': Mock(return_value=True),
        'initialize_system': Mock(return_value={'initialized': True}),
        'cleanup_system': Mock(),
        'create_navigator': Mock(),
        'create_video_plume': Mock(),
        'run_plume_simulation': Mock(return_value=(np.array([]), np.array([]), np.array([]))),
    })
    def test_run_command_dry_run_execution(self, cli_runner):
        """
        Test dry run command execution per F-013-RQ-001.
        
        Validates that simulation launching commands execute successfully
        with proper exit code verification in dry-run mode.
        """
        # Setup mock configuration
        mock_config = OmegaConf.create({
            "navigator": {"position": [0, 0], "max_speed": 2.0},
            "video_plume": {"video_path": "test.mp4"},
            "simulation": {"num_steps": 10}
        })
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config):
            result = cli_runner.invoke(cli, ['run', '--dry-run'])
            
            # Should succeed in dry-run mode
            assert result.exit_code == 0
            assert 'Dry run mode' in result.output or result.exit_code == 0
    
    @patch.multiple('plume_nav_sim.cli.main', **{
        'get_cli_config': Mock(),
        'validate_configuration': Mock(return_value=True),
        'initialize_system': Mock(return_value={'initialized': True}),
        'cleanup_system': Mock(),
        'Path': Mock()
    })
    def test_run_command_batch_processing(self, cli_runner):
        """
        Test batch processing and automation workflow per Section 7.4.4.1.
        
        Validates CLI batch processing capabilities for automated experiment
        execution and headless processing mode.
        """
        # Setup mock configuration with batch mode
        mock_config = OmegaConf.create({
            "navigator": {"position": [0, 0]},
            "video_plume": {"video_path": "test.mp4"},
            "simulation": {"num_steps": 10},
            "visualization": {"animation": {"enabled": False}},
            "environment": {"debug_mode": False}
        })
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config), \
             patch('plume_nav_sim.cli.main.OmegaConf') as mock_omega_conf:
            
            result = cli_runner.invoke(cli, ['run', '--batch', '--dry-run'])
            
            # Verify batch mode configuration changes
            expected_calls = [
                call.set(mock_config, 'visualization.animation.enabled', False),
                call.set(mock_config, 'environment.debug_mode', False)
            ]
            mock_omega_conf.set.assert_has_calls(expected_calls, any_order=True)
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    @patch('plume_nav_sim.cli.main.OmegaConf')
    def test_config_validate_command_execution(
        self,
        mock_omega_conf,
        mock_get_config,
        cli_runner
    ):
        """Test config validate command execution and output formatting."""
        mock_config = OmegaConf.create({"test": "config"})
        mock_get_config.return_value = mock_config
        mock_omega_conf.to_yaml.return_value = "test: config\n"
        
        with patch('plume_nav_sim.cli.main.validate_configuration', return_value=True):
            # Test pretty format (default)
            result = cli_runner.invoke(cli, ['config', 'validate'])
            assert result.exit_code == 0
            assert '✅' in result.output or 'Configuration validation passed' in result.output
            
            # Test YAML format
            result = cli_runner.invoke(cli, ['config', 'validate', '--format', 'yaml'])
            assert result.exit_code == 0
            
            # Test strict validation
            result = cli_runner.invoke(cli, ['config', 'validate', '--strict'])
            assert result.exit_code == 0
    
    def test_config_export_command_execution(self, cli_runner):
        """Test config export command execution with different formats."""
        mock_config = OmegaConf.create({"test": "config", "nested": {"value": 42}})
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config), \
             patch('plume_nav_sim.cli.main.OmegaConf') as mock_omega_conf:
            
            mock_omega_conf.to_yaml.return_value = "test: config\nnested:\n  value: 42\n"
            mock_omega_conf.to_container.return_value = {"test": "config", "nested": {"value": 42}}
            
            # Test YAML export to console
            result = cli_runner.invoke(cli, ['config', 'export'])
            assert result.exit_code == 0
            
            # Test JSON export to console
            result = cli_runner.invoke(cli, ['config', 'export', '--format', 'json'])
            assert result.exit_code == 0
            
            # Test export with resolved interpolations
            result = cli_runner.invoke(cli, ['config', 'export', '--resolved'])
            assert result.exit_code == 0
    
    def test_visualize_command_execution(self, cli_runner):
        """Test visualize command execution with mock input file."""
        with tempfile.NamedTemporaryFile(suffix='.npz', delete=False) as temp_file:
            # Create mock NPZ file
            np.savez_compressed(
                temp_file.name,
                positions=np.array([[[0, 0], [1, 1]]]),
                orientations=np.array([[0, 45]]),
                config=b'test: config'
            )
            
            with patch('plume_nav_sim.cli.main.visualize_simulation_results') as mock_viz, \
                 patch('plume_nav_sim.cli.main.export_animation') as mock_export, \
                 patch('plume_nav_sim.cli.main.Path') as mock_path:
                
                # Mock Path operations
                mock_path.return_value.parent.mkdir = Mock()
                mock_path.return_value.parent.exists = Mock(return_value=True)
                
                # Test MP4 visualization
                result = cli_runner.invoke(cli, [
                    'visualize',
                    '--input-file', temp_file.name,
                    '--format', 'mp4'
                ])
                
                if result.exit_code != 0:
                    print(f"Visualize command output: {result.output}")
                    print(f"Exception: {result.exception}")
                
                # Should attempt to call export_animation for MP4 format
                mock_export.assert_called()
        
        # Clean up
        os.unlink(temp_file.name)


class TestCLIErrorHandling:
    """
    Test suite for CLI error handling and recovery strategies.
    
    Validates error handling and recovery strategies for invalid CLI inputs
    per Section 4.1.3.2 and CLI security per Section 6.6.7.2.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for error handling testing."""
        return CliRunner()
    
    def test_cli_configuration_errors(self, cli_runner):
        """Test CLI handling of configuration-related errors."""
        # Test missing configuration
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=None):
            result = cli_runner.invoke(cli, ['run'])
            assert result.exit_code == 4  # Configuration not available error
    
    def test_cli_validation_errors(self, cli_runner):
        """Test CLI handling of validation errors."""
        mock_config = OmegaConf.create({"test": "config"})
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config), \
             patch('plume_nav_sim.cli.main.validate_configuration') as mock_validate:
            
            # Test configuration validation failure
            mock_validate.side_effect = CLIError("Validation failed", exit_code=2)
            result = cli_runner.invoke(cli, ['run'])
            assert result.exit_code == 2
    
    def test_cli_system_initialization_errors(self, cli_runner):
        """Test CLI handling of system initialization errors."""
        mock_config = OmegaConf.create({"test": "config"})
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config), \
             patch('plume_nav_sim.cli.main.validate_configuration', return_value=True), \
             patch('plume_nav_sim.cli.main.initialize_system') as mock_init:
            
            # Test system initialization failure
            mock_init.side_effect = CLIError("System init failed", exit_code=3)
            result = cli_runner.invoke(cli, ['run'])
            assert result.exit_code == 3
    
    def test_cli_simulation_errors(self, cli_runner):
        """Test CLI handling of simulation execution errors."""
        mock_config = OmegaConf.create({
            "navigator": {"position": [0, 0]},
            "video_plume": {"video_path": "nonexistent.mp4"},
            "simulation": {"num_steps": 10}
        })
        
        with patch('plume_nav_sim.cli.main.get_cli_config', return_value=mock_config), \
             patch('plume_nav_sim.cli.main.validate_configuration', return_value=True), \
             patch('plume_nav_sim.cli.main.initialize_system', return_value={}), \
             patch('plume_nav_sim.cli.main.cleanup_system'), \
             patch('plume_nav_sim.cli.main.create_navigator') as mock_nav, \
             patch('plume_nav_sim.cli.main.create_video_plume') as mock_plume:
            
            # Test navigator creation failure
            mock_nav.side_effect = ConfigurationError("Navigator creation failed")
            result = cli_runner.invoke(cli, ['run', '--dry-run'])
            assert result.exit_code == 5  # Dry run validation failed
            
            # Test video plume creation failure
            mock_nav.side_effect = None
            mock_nav.return_value = Mock()
            mock_plume.side_effect = FileNotFoundError("Video file not found")
            result = cli_runner.invoke(cli, ['run', '--dry-run'])
            assert result.exit_code == 5  # Dry run validation failed
    
    def test_cli_keyboard_interrupt_handling(self, cli_runner):
        """Test CLI handling of keyboard interrupts (Ctrl+C)."""
        with patch('plume_nav_sim.cli.main.get_cli_config') as mock_config:
            mock_config.side_effect = KeyboardInterrupt()
            
            result = cli_runner.invoke(cli, ['run'])
            # CliRunner doesn't propagate KeyboardInterrupt the same way,
            # but we can test the handler function directly
            assert True  # Test passes if no exception is raised
    
    def test_cli_security_parameter_injection_prevention(self, cli_runner):
        """
        Test CLI parameter injection prevention per Section 6.6.7.2.
        
        Validates that CLI parameter parsing rejects injection attempts
        and maintains security controls through Click validation.
        """
        # Test shell injection attempts
        malicious_args = [
            ['--output-dir', '; rm -rf /'],
            ['--max-duration', '$(whoami)'],
            ['--config-dir', '../../../etc'],
        ]
        
        for args in malicious_args:
            result = cli_runner.invoke(cli, ['run'] + args)
            # Should either reject the argument or handle it safely
            assert result.exit_code != 0 or '$(whoami)' not in str(result.output)
    
    def test_cli_file_path_validation(self, cli_runner):
        """Test CLI file path validation and security."""
        # Test path traversal attempts
        result = cli_runner.invoke(cli, [
            'visualize',
            '--input-file', '../../../etc/passwd',
            '--format', 'mp4'
        ])
        # Should fail due to file not existing or being rejected
        assert result.exit_code != 0


class TestCLIPerformanceRequirements:
    """
    Test suite for CLI performance requirements validation.
    
    Validates CLI command initialization performance (<2s) per Section 2.2.9.3
    and other performance criteria for interactive command-line responsiveness.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for performance testing."""
        return CliRunner()
    
    def test_cli_initialization_performance_requirement(self, cli_runner):
        """
        Test CLI command initialization timing (<2s) per Section 2.2.9.3.
        
        Validates that CLI initialization meets performance requirements
        for interactive command-line responsiveness under various scenarios.
        """
        # Test help command performance
        start_time = time.time()
        result = cli_runner.invoke(cli, ['--help'])
        end_time = time.time()
        
        initialization_time = end_time - start_time
        assert initialization_time < 2.0, f"Help command took {initialization_time:.2f}s, exceeding 2s requirement"
        assert result.exit_code == 0
        
        # Test subcommand help performance
        start_time = time.time()
        result = cli_runner.invoke(cli, ['run', '--help'])
        end_time = time.time()
        
        initialization_time = end_time - start_time
        assert initialization_time < 2.0, f"Run help took {initialization_time:.2f}s, exceeding 2s requirement"
        assert result.exit_code == 0
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    def test_cli_command_response_time(self, mock_get_config, cli_runner):
        """Test CLI command response time for various operations."""
        # Mock quick configuration loading
        mock_config = OmegaConf.create({"test": "config"})
        mock_get_config.return_value = mock_config
        
        with patch('plume_nav_sim.cli.main.validate_configuration', return_value=True):
            # Test config validate command performance
            start_time = time.time()
            result = cli_runner.invoke(cli, ['config', 'validate'])
            end_time = time.time()
            
            response_time = end_time - start_time
            assert response_time < 2.0, f"Config validate took {response_time:.2f}s, exceeding 2s requirement"
    
    def test_cli_memory_usage_basic(self, cli_runner):
        """Test basic CLI memory usage patterns."""
        # This is a basic test - in production, you might use memory profiling tools
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss
        
        # Run a simple CLI command
        result = cli_runner.invoke(cli, ['--help'])
        
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # Basic check that memory usage isn't excessive (100MB limit per requirements)
        assert memory_increase < 100 * 1024 * 1024, f"Memory usage increased by {memory_increase / 1024 / 1024:.2f}MB"
        assert result.exit_code == 0


class TestCLIInteractionAndUX:
    """
    Test suite for CLI user experience and interaction patterns.
    
    Validates interactive prompt and confirmation dialog testing for CLI
    user experience and comprehensive CLI workflow validation.
    """
    
    @pytest.fixture
    def cli_runner(self):
        """Provide isolated CliRunner instance for UX testing."""
        return CliRunner()
    
    def test_cli_output_formatting(self, cli_runner):
        """Test CLI output formatting and user-friendly messages."""
        # Test help output formatting
        result = cli_runner.invoke(cli, ['--help'])
        assert result.exit_code == 0
        
        # Check for proper formatting elements
        assert 'Usage:' in result.output or 'usage:' in result.output.lower()
        assert 'Examples:' in result.output or 'Example:' in result.output
        
        # Test error message formatting
        result = cli_runner.invoke(cli, ['invalid-command'])
        assert result.exit_code != 0
        assert 'Usage:' in result.output or 'No such command' in result.output
    
    def test_cli_verbose_output_mode(self, cli_runner):
        """Test CLI verbose output mode functionality."""
        # Test verbose flag effect
        result = cli_runner.invoke(cli, ['--verbose', '--help'])
        assert result.exit_code == 0
        
        # In a real implementation, verbose mode might show additional debug info
        # For now, we just verify the flag is accepted
    
    def test_cli_quiet_output_mode(self, cli_runner):
        """Test CLI quiet output mode functionality."""
        # Test quiet flag effect
        result = cli_runner.invoke(cli, ['--quiet', '--help'])
        assert result.exit_code == 0
        
        # In a real implementation, quiet mode might suppress non-error output
        # For now, we just verify the flag is accepted
    
    @patch('plume_nav_sim.cli.main.get_cli_config')
    def test_cli_progress_indication(self, mock_get_config, cli_runner):
        """Test CLI progress indication for long-running operations."""
        mock_config = OmegaConf.create({"test": "config"})
        mock_get_config.return_value = mock_config
        
        with patch('plume_nav_sim.cli.main.validate_configuration', return_value=True):
            # Test config validation progress
            result = cli_runner.invoke(cli, ['config', 'validate', '--format', 'pretty'])
            
            # Should provide user-friendly output
            if result.exit_code == 0:
                # Look for progress or completion indicators
                assert '✅' in result.output or 'passed' in result.output.lower() or 'success' in result.output.lower()


class TestCLIExceptionHandlers:
    """
    Test suite for CLI exception handling decorators and error management.
    
    Validates the @handle_cli_exception decorator and comprehensive error
    recovery strategies across different failure scenarios.
    """
    
    def test_handle_cli_exception_decorator_success(self):
        """Test @handle_cli_exception decorator for successful function execution."""
        @handle_cli_exception
        def test_function():
            return "success"
        
        # Should execute normally without raising exceptions
        result = test_function()
        assert result == "success"
    
    def test_handle_cli_exception_decorator_cli_error(self):
        """Test @handle_cli_exception decorator handling of CLIError exceptions."""
        @handle_cli_exception
        def test_function():
            raise CLIError("Test CLI error", exit_code=42)
        
        # Should catch CLIError and call sys.exit
        with patch('plume_nav_sim.cli.main.sys.exit') as mock_exit:
            test_function()
            mock_exit.assert_called_once_with(42)
    
    def test_handle_cli_exception_decorator_keyboard_interrupt(self):
        """Test @handle_cli_exception decorator handling of KeyboardInterrupt."""
        @handle_cli_exception
        def test_function():
            raise KeyboardInterrupt()
        
        # Should catch KeyboardInterrupt and call sys.exit with 130
        with patch('plume_nav_sim.cli.main.sys.exit') as mock_exit:
            test_function()
            mock_exit.assert_called_once_with(130)
    
    def test_handle_cli_exception_decorator_general_exception(self):
        """Test @handle_cli_exception decorator handling of general exceptions."""
        @handle_cli_exception
        def test_function():
            raise ValueError("Test error")
        
        # Should catch general exceptions and call sys.exit with 1
        with patch('plume_nav_sim.cli.main.sys.exit') as mock_exit:
            test_function()
            mock_exit.assert_called_once_with(1)
    
    def test_cli_error_class_functionality(self):
        """Test CLIError class functionality and attributes."""
        error = CLIError("Test message", exit_code=5, details={"key": "value"})
        
        assert str(error) == "Test message"
        assert error.exit_code == 5
        assert error.details == {"key": "value"}
        assert hasattr(error, 'timestamp')
        assert isinstance(error.timestamp, float)
    
    def test_cli_configuration_validation_function(self):
        """Test validate_configuration function with various scenarios."""
        # Test valid configuration
        valid_config = OmegaConf.create({
            "navigator": {
                "position": [0.0, 0.0],
                "orientation": 0.0,
                "speed": 1.0,
                "max_speed": 2.0
            },
            "video_plume": {
                "video_path": "test.mp4",
                "flip": False,
                "kernel_size": 5,
                "kernel_sigma": 1.0
            },
            "simulation": {
                "num_steps": 100,
                "dt": 0.1
            }
        })
        
        with patch('plume_nav_sim.cli.main.NavigatorConfig'), \
             patch('plume_nav_sim.cli.main.VideoPlumeConfig'), \
             patch('plume_nav_sim.cli.main.SimulationConfig'), \
             patch('plume_nav_sim.cli.main.Path') as mock_path:
            
            # Mock Path.exists() to return True for video file
            mock_path.return_value.exists.return_value = True
            
            result = validate_configuration(valid_config)
            assert result is True
    
    def test_cli_system_initialization_function(self):
        """Test initialize_system function functionality."""
        test_config = OmegaConf.create({
            "logging": {"level": "INFO"},
            "reproducibility": {"global_seed": 42},
            "database": {"enabled": False},
            "performance": {"numpy": {"thread_count": 4}}
        })
        
        with patch('plume_nav_sim.cli.main.setup_logging'), \
             patch('plume_nav_sim.cli.main.set_global_seed'), \
             patch('plume_nav_sim.cli.main.get_current_seed', return_value=42), \
             patch('plume_nav_sim.cli.main.os.environ') as mock_env:
            
            result = initialize_system(test_config)
            
            assert isinstance(result, dict)
            assert 'config' in result
            assert 'seed' in result
            assert 'initialized_at' in result
            assert result['seed'] == 42
    
    def test_cli_system_cleanup_function(self):
        """Test cleanup_system function functionality."""
        system_info = {
            'session': Mock(),
            'initialized_at': time.time() - 1.0  # 1 second ago
        }
        
        with patch('plume_nav_sim.cli.main.close_session') as mock_close:
            cleanup_system(system_info)
            mock_close.assert_called_once_with(system_info['session'])


if __name__ == "__main__":
    # Enable direct execution of test file for debugging
    pytest.main([__file__, "-v", "--tb=short"])